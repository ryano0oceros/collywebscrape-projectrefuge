{"url":"https://en.wikipedia.org/wiki/Robotics","text":"Robotics is the interdisciplinary study and practice of the design, construction, operation, and use of robots.[1]\nWithin mechanical engineering, robotics is the design and construction of the physical structures of robots, while in computer science, robotics focuses on robotic automation algorithms. Other disciplines contributing to robotics include electrical, control, software, information, electronic, telecommunication, computer, mechatronic, and materials engineering. \nThe goal of most robotics is to design machines that can help and assist humans. Many robots are built to do jobs that are hazardous to people, such as finding survivors in unstable ruins, and exploring space, mines and shipwrecks. Others replace people in jobs that are boring, repetitive, or unpleasant, such as cleaning, monitoring, transporting, and assembling. Today, robotics is a rapidly growing field, as technological advances continue; researching, designing, and building new robots serve various practical purposes.\nThere are many types of robots; they are used in many different environments and for many different uses. Although diverse in application and form, they all share three basic aspects when it comes to their design and construction:\nAs more and more robots are designed for specific tasks, this method of classification becomes more relevant. For example, many robots are designed for assembly work, which may not be readily adaptable for other applications. They are termed \"assembly robots\". For seam welding, some suppliers provide complete welding systems with the robot i.e. the welding equipment along with other material handling facilities like turntables, etc. as an integrated unit. Such an integrated robotic system is called a \"welding robot\" even though its discrete manipulator unit could be adapted to a variety of tasks. Some robots are specifically designed for heavy load manipulation, and are labeled as \"heavy-duty robots\".[3]\nCurrent and potential applications include:\nAt present, mostly (lead–acid) batteries are used as a power source. Many different types of batteries can be used as a power source for robots. They range from lead–acid batteries, which are safe and have relatively long shelf lives but are rather heavy compared to silver–cadmium batteries which are much smaller in volume and are currently much more expensive. Designing a battery-powered robot needs to take into account factors such as safety, cycle lifetime, and weight. Generators, often some type of internal combustion engine, can also be used. However, such designs are often mechanically complex and need fuel, require heat dissipation, and are relatively heavy. A tether connecting the robot to a power supply would remove the power supply from the robot entirely. This has the advantage of saving weight and space by moving all power generation and storage components elsewhere. However, this design does come with the drawback of constantly having a cable connected to the robot, which can be difficult to manage.[15] \nPotential power sources could be:\nActuators are the \"muscles\" of a robot, the parts which convert stored energy into movement.[16] By far the most popular actuators are electric motors that rotate a wheel or gear, and linear actuators that control industrial robots in factories. There are some recent advances in alternative types of actuators, powered by electricity, chemicals, or compressed air.\nThe vast majority of robots use electric motors, often brushed and brushless DC motors in portable robots or AC motors in industrial robots and CNC machines. These motors are often preferred in systems with lighter loads, and where the predominant form of motion is rotational.\nVarious types of linear actuators move in and out instead of by spinning, and often have quicker direction changes, particularly when very large forces are needed such as with industrial robotics. They are typically powered by compressed and oxidized air (pneumatic actuator) or an oil (hydraulic actuator) Linear actuators can also be powered by electricity which usually consists of a motor and a leadscrew. Another common type is a mechanical linear actuator such as a rack and pinion on a car.\nSeries elastic actuation (SEA) relies on the idea of introducing intentional elasticity between the motor actuator and the load for robust force control. Due to the resultant lower reflected inertia, series elastic actuation improves safety when a robot interacts with the environment (e.g., humans or workpieces) or during collisions.[17] Furthermore, it also provides energy efficiency and shock absorption (mechanical filtering) while reducing excessive wear on the transmission and other mechanical components. This approach has successfully been employed in various robots, particularly advanced manufacturing robots[18] and walking humanoid robots.[19][20]\nThe controller design of a series elastic actuator is most often performed within the passivity framework as it ensures the safety of interaction with unstructured environments.[21] Despite its remarkable stability and robustness, this framework suffers from the stringent limitations imposed on the controller which may trade-off performance. The reader is referred to the following survey which summarizes the common controller architectures for SEA along with the corresponding sufficient passivity conditions.[22] One recent study has derived the necessary and sufficient passivity conditions for one of the most common impedance control architectures, namely velocity-sourced SEA.[23] This work is of particular importance as it drives the non-conservative passivity bounds in an SEA scheme for the first time which allows a larger selection of control gains.\nPneumatic artificial muscles also known as air muscles, are special tubes that expand (typically up to 42%) when air is forced inside them. They are used in some robot applications.[24][25][26]\nMuscle wire, also known as shape memory alloy, Nitinol® or Flexinol® wire, is a material that contracts (under 5%) when electricity is applied. They have been used for some small robot applications.[27][28]\nEAPs or EPAMs are a plastic material that can contract substantially (up to 380% activation strain) from electricity, and have been used in facial muscles and arms of humanoid robots,[29] and to enable new robots to float,[30] fly, swim or walk.[31]\nRecent alternatives to DC motors are piezo motors or ultrasonic motors. These work on a fundamentally different principle, whereby tiny piezoceramic elements, vibrating many thousands of times per second, cause linear or rotary motion. There are different mechanisms of operation; one type uses the vibration of the piezo elements to step the motor in a circle or a straight line.[32] Another type uses the piezo elements to cause a nut to vibrate or to drive a screw. The advantages of these motors are nanometer resolution, speed, and available force for their size.[33] These motors are already available commercially and being used on some robots.[34][35]\nElastic nanotubes are a promising artificial muscle technology in early-stage experimental development. The absence of defects in carbon nanotubes enables these filaments to deform elastically by several percent, with energy storage levels of perhaps 10 J/cm3 for metal nanotubes. Human biceps could be replaced with an 8 mm diameter wire of this material. Such compact \"muscle\" might allow future robots to outrun and outjump humans.[36]\nSensors allow robots to receive information about a certain measurement of the environment, or internal components. This is essential for robots to perform their tasks, and act upon any changes in the environment to calculate the appropriate response. They are used for various forms of measurements, to give the robots warnings about safety or malfunctions, and to provide real-time information about the task it is performing.\nCurrent robotic and prosthetic hands receive far less tactile information than the human hand. Recent research has developed a tactile sensor array that mimics the mechanical properties and touch receptors of human fingertips.[37][38] The sensor array is constructed as a rigid core surrounded by conductive fluid contained by an elastomeric skin. Electrodes are mounted on the surface of the rigid core and are connected to an impedance-measuring device within the core. When the artificial skin touches an object the fluid path around the electrodes is deformed, producing impedance changes that map the forces received from the object. The researchers expect that an important function of such artificial fingertips will be adjusting the robotic grip on held objects.\nScientists from several European countries and Israel developed a prosthetic hand in 2009, called SmartHand, which functions like a real one —allowing patients to write with it, type on a keyboard, play piano, and perform other fine movements. The prosthesis has sensors which enable the patient to sense real feelings in its fingertips.[39]\nOther common forms of sensing in robotics use lidar, radar, and sonar.[40] Lidar measures the distance to a target by illuminating the target with laser light and measuring the reflected light with a sensor. Radar uses radio waves to determine the range, angle, or velocity of objects. Sonar uses sound propagation to navigate, communicate with or detect objects on or under the surface of the water.\nA definition of robotic manipulation has been provided by Matt Mason as: \"manipulation refers to an agent's control of its environment through selective contact\".[41]\nRobots need to manipulate objects; pick up, modify, destroy, move or otherwise have an effect. Thus the functional end of a robot arm intended to make the effect (whether a hand, or tool) are often referred to as end effectors,[42] while the \"arm\" is referred to as a manipulator.[43] Most robot arms have replaceable end-effectors, each allowing them to perform some small range of tasks. Some have a fixed manipulator that cannot be replaced, while a few have one very general-purpose manipulator, for example, a humanoid hand.[44]\nOne of the most common types of end-effectors are \"grippers\". In its simplest manifestation, it consists of just two fingers that can open and close to pick up and let go of a range of small objects. Fingers can, for example, be made of a chain with a metal wire running through it.[45] Hands that resemble and work more like a human hand include the Shadow Hand and the Robonaut hand.[46] Hands that are of a mid-level complexity include the Delft hand.[47][48] Mechanical grippers can come in various types, including friction and encompassing jaws. Friction jaws use all the force of the gripper to hold the object in place using friction. Encompassing jaws cradle the object in place, using less friction.\nSuction end-effectors, powered by vacuum generators, are very simple astrictive[49] devices that can hold very large loads provided the prehension surface is smooth enough to ensure suction.\nPick and place robots for electronic components and for large objects like car windscreens, often use very simple vacuum end-effectors.\nSuction is a highly used type of end-effector in industry, in part because the natural compliance of soft suction end-effectors can enable a robot to be more robust in the presence of imperfect robotic perception. As an example: consider the case of a robot vision system that estimates the position of a water bottle but has 1 centimeter of error. While this may cause a rigid mechanical gripper to puncture the water bottle, the soft suction end-effector may just bend slightly and conform to the shape of the water bottle surface.\nSome advanced robots are beginning to use fully humanoid hands, like the Shadow Hand, MANUS,[50] and the Schunk hand.[51] They have powerful robot dexterity intelligence (RDI), with as many as 20 degrees of freedom and hundreds of tactile sensors.[52]\nFor simplicity, most mobile robots have four wheels or a number of continuous tracks. Some researchers have tried to create more complex wheeled robots with only one or two wheels. These can have certain advantages such as greater efficiency and reduced parts, as well as allowing a robot to navigate in confined places that a four-wheeled robot would not be able to.\nBalancing robots generally use a gyroscope to detect how much a robot is falling and then drive the wheels proportionally in the same direction, to counterbalance the fall at hundreds of times per second, based on the dynamics of an inverted pendulum.[53] Many different balancing robots have been designed.[54] While the Segway is not commonly thought of as a robot, it can be thought of as a component of a robot, when used as such Segway refer to them as RMP (Robotic Mobility Platform). An example of this use has been as NASA's Robonaut that has been mounted on a Segway.[55]\nA one-wheeled balancing robot is an extension of a two-wheeled balancing robot so that it can move in any 2D direction using a round ball as its only wheel. Several one-wheeled balancing robots have been designed recently, such as Carnegie Mellon University's \"Ballbot\" which is the approximate height and width of a person, and Tohoku Gakuin University's \"BallIP\".[56] Because of the long, thin shape and ability to maneuver in tight spaces, they have the potential to function better than other robots in environments with people.[57]\nSeveral attempts have been made in robots that are completely inside a spherical ball, either by spinning a weight inside the ball,[58][59] or by rotating the outer shells of the sphere.[60][61] These have also been referred to as an orb bot[62] or a ball bot.[63][64]\nUsing six wheels instead of four wheels can give better traction or grip in outdoor terrain such as on rocky dirt or grass.\nTank tracks provide even more traction than a six-wheeled robot. Tracked wheels behave as if they were made of hundreds of wheels, therefore are very common for outdoor and military robots, where the robot must drive on very rough terrain. However, they are difficult to use indoors such as on carpets and smooth floors. Examples include NASA's Urban Robot \"Urbie\".[65]\nWalking is a difficult and dynamic problem to solve. Several robots have been made which can walk reliably on two legs, however, none have yet been made which are as robust as a human. There has been much study on human-inspired walking, such as AMBER lab which was established in 2008 by the Mechanical Engineering Department at Texas A\u0026M University.[66] Many other robots have been built that walk on more than two legs, due to these robots being significantly easier to construct.[67][68] Walking robots can be used for uneven terrains, which would provide better mobility and energy efficiency than other locomotion methods. Typically, robots on two legs can walk well on flat floors and can occasionally walk up stairs. None can walk over rocky, uneven terrain. Some of the methods which have been tried are:\nThe zero moment point (ZMP) is the algorithm used by robots such as Honda's ASIMO. The robot's onboard computer tries to keep the total inertial forces (the combination of Earth's gravity and the acceleration and deceleration of walking), exactly opposed by the floor reaction force (the force of the floor pushing back on the robot's foot). In this way, the two forces cancel out, leaving no moment (force causing the robot to rotate and fall over).[69] However, this is not exactly how a human walks, and the difference is obvious to human observers, some of whom have pointed out that ASIMO walks as if it needs the lavatory.[70][71][72] ASIMO's walking algorithm is not static, and some dynamic balancing is used (see below). However, it still requires a smooth surface to walk on.\nSeveral robots, built in the 1980s by Marc Raibert at the MIT Leg Laboratory, successfully demonstrated very dynamic walking. Initially, a robot with only one leg, and a very small foot could stay upright simply by hopping. The movement is the same as that of a person on a pogo stick. As the robot falls to one side, it would jump slightly in that direction, in order to catch itself.[73] Soon, the algorithm was generalised to two and four legs. A bipedal robot was demonstrated running and even performing somersaults.[74] A quadruped was also demonstrated which could trot, run, pace, and bound.[75] For a full list of these robots, see the MIT Leg Lab Robots page.[76]\nA more advanced way for a robot to walk is by using a dynamic balancing algorithm, which is potentially more robust than the Zero Moment Point technique, as it constantly monitors the robot's motion, and places the feet in order to maintain stability.[77] This technique was recently demonstrated by Anybots' Dexter Robot,[78] which is so stable, it can even jump.[79] Another example is the TU Delft Flame.\nPerhaps the most promising approach uses passive dynamics where the momentum of swinging limbs is used for greater efficiency. It has been shown that totally unpowered humanoid mechanisms can walk down a gentle slope, using only gravity to propel themselves. Using this technique, a robot need only supply a small amount of motor power to walk along a flat surface or a little more to walk up a hill. This technique promises to make walking robots at least ten times more efficient than ZMP walkers, like ASIMO.[80][81]\nA modern passenger airliner is essentially a flying robot, with two humans to manage it. The autopilot can control the plane for each stage of the journey, including takeoff, normal flight, and even landing.[82] Other flying robots are uninhabited and are known as unmanned aerial vehicles (UAVs). They can be smaller and lighter without a human pilot on board, and fly into dangerous territory for military surveillance missions. Some can even fire on targets under command. UAVs are also being developed which can fire on targets automatically, without the need for a command from a human. Other flying robots include cruise missiles, the Entomopter, and the Epson micro helicopter robot. Robots such as the Air Penguin, Air Ray, and Air Jelly have lighter-than-air bodies, are propelled by paddles, and are guided by sonar.\nBFRs take inspiration from flying mammals, birds, or insects. BFRs can have flapping wings, which generate the lift and thrust, or they can be propeller actuated. BFRs with flapping wings have increased stroke efficiencies, increased maneuverability, and reduced energy consumption in comparison to propeller actuated BFRs.[83] Mammal and bird inspired BFRs share similar flight characteristics and design considerations. For instance, both mammal and bird inspired BFRs minimize edge fluttering and pressure-induced wingtip curl by increasing the rigidity of the wing edge and wingtips. Mammal and insect inspired BFRs can be impact resistant, making them useful in cluttered environments.\nMammal inspired BFRs typically take inspiration from bats, but the flying squirrel has also inspired a prototype.[84] Examples of bat inspired BFRs include Bat Bot[85] and the DALER.[86] Mammal inspired BFRs can be designed to be multi-modal; therefore, they're capable of both flight and terrestrial movement. To reduce the impact of landing, shock absorbers can be implemented along the wings.[86] Alternatively, the BFR can pitch up and increase the amount of drag it experiences.[84] By increasing the drag force, the BFR will decelerate and minimize the impact upon grounding. Different land gait patterns can also be implemented.[84]\nBird inspired BFRs can take inspiration from raptors, gulls, and everything in-between. Bird inspired BFRs can be feathered to increase the angle of attack range over which the prototype can operate before stalling.[87] The wings of bird inspired BFRs allow for in-plane deformation, and the in-plane wing deformation can be adjusted to maximize flight efficiency depending on the flight gait.[87] An example of a raptor inspired BFR is the prototype by Savastano et al.[88] The prototype has fully deformable flapping wings and is capable of carrying a payload of up to 0.8 kg while performing a parabolic climb, steep descent, and rapid recovery. The gull inspired prototype by Grant et al. accurately mimics the elbow and wrist rotation of gulls, and they find that lift generation is maximized when the elbow and wrist deformations are opposite but equal.[89]\nInsect inspired BFRs typically take inspiration from beetles or dragonflies. An example of a beetle inspired BFR is the prototype by Phan and Park,[90] and a dragonfly inspired BFR is the prototype by Hu et al.[91] The flapping frequency of insect inspired BFRs are much higher than those of other BFRs; this is because of the aerodynamics of insect flight.[92] Insect inspired BFRs are much smaller than those inspired by mammals or birds, so they are more suitable for dense environments.\nA class of robots that are biologically inspired, but which do not attempt to mimic biology, are creations such as the Entomopter. Funded by DARPA, NASA, the United States Air Force, and the Georgia Tech Research Institute and patented by Prof. Robert C. Michelson for covert terrestrial missions as well as flight in the lower Mars atmosphere, the Entomopter flight propulsion system uses low Reynolds number wings similar to those of the hawk moth (Manduca sexta), but flaps them in a non-traditional \"opposed x-wing fashion\" while \"blowing\" the surface to enhance lift based on the Coandă effect as well as to control vehicle attitude and direction. Waste gas from the propulsion system not only facilitates the blown wing aerodynamics, but also serves to create ultrasonic emissions like that of a Bat for obstacle avoidance. The Entomopter and other biologically-inspired robots leverage features of biological systems, but do not attempt to create mechanical analogs.\nSeveral snake robots have been successfully developed. Mimicking the way real snakes move, these robots can navigate very confined spaces, meaning they may one day be used to search for people trapped in collapsed buildings.[93] The Japanese ACM-R5 snake robot[94] can even navigate both on land and in water.[95]\nA small number of skating robots have been developed, one of which is a multi-mode walking and skating device. It has four legs, with unpowered wheels, which can either step or roll.[96] Another robot, Plen, can use a miniature skateboard or roller-skates, and skate across a desktop.[97]\nSeveral different approaches have been used to develop robots that have the ability to climb vertical surfaces. One approach mimics the movements of a human climber on a wall with protrusions; adjusting the center of mass and moving each limb in turn to gain leverage. An example of this is Capuchin,[98] built by Ruixiang Zhang at Stanford University, California. Another approach uses the specialized toe pad method of wall-climbing geckoes, which can run on smooth surfaces such as vertical glass. Examples of this approach include Wallbot[99] and Stickybot.[100]\nChina's Technology Daily reported on 15 November 2008, that Li Hiu Yeung and his research group of New Concept Aircraft (Zhuhai) Co., Ltd. had successfully developed a bionic gecko robot named \"Speedy Freelander\". According to Yeung, the gecko robot could rapidly climb up and down a variety of building walls, navigate through ground and wall fissures, and walk upside-down on the ceiling. It was also able to adapt to the surfaces of smooth glass, rough, sticky or dusty walls as well as various types of metallic materials. It could also identify and circumvent obstacles automatically. Its flexibility and speed were comparable to a natural gecko. A third approach is to mimic the motion of a snake climbing a pole.[40]\nIt is calculated that when swimming some fish can achieve a propulsive efficiency greater than 90%.[101] Furthermore, they can accelerate and maneuver far better than any man-made boat or submarine, and produce less noise and water disturbance. Therefore, many researchers studying underwater robots would like to copy this type of locomotion.[102] Notable examples are the Essex University Computer Science Robotic Fish G9,[103] and the Robot Tuna built by the Institute of Field Robotics, to analyze and mathematically model thunniform motion.[104] The Aqua Penguin,[105] designed and built by Festo of Germany, copies the streamlined shape and propulsion by front \"flippers\" of penguins. Festo have also built the Aqua Ray and Aqua Jelly, which emulate the locomotion of manta ray, and jellyfish, respectively.\nIn 2014, iSplash-II was developed by PhD student Richard James Clapham and Prof. Huosheng Hu at Essex University. It was the first robotic fish capable of outperforming real carangiform fish in terms of average maximum velocity (measured in body lengths/ second) and endurance, the duration that top speed is maintained.[106] This build attained swimming speeds of 11.6BL/s (i.e. 3.7 m/s).[107] The first build, iSplash-I (2014) was the first robotic platform to apply a full-body length carangiform swimming motion which was found to increase swimming speed by 27% over the traditional approach of a posterior confined waveform.[108]\nSailboat robots have also been developed in order to make measurements at the surface of the ocean. A typical sailboat robot is Vaimos[109] built by IFREMER and ENSTA-Bretagne. Since the propulsion of sailboat robots uses the wind, the energy of the batteries is only used for the computer, for the communication and for the actuators (to tune the rudder and the sail). If the robot is equipped with solar panels, the robot could theoretically navigate forever. The two main competitions of sailboat robots are WRSC, which takes place every year in Europe, and Sailbot.\nThe mechanical structure of a robot must be controlled to perform tasks.[110] The control of a robot involves three distinct phases – perception, processing, and action (robotic paradigms).[111] Sensors give information about the environment or the robot itself (e.g. the position of its joints or its end effector). This information is then processed to be stored or transmitted and to calculate the appropriate signals to the actuators (motors), which move the mechanical structure to achieve the required co-ordinated motion or force actions.\nThe processing phase can range in complexity. At a reactive level, it may translate raw sensor information directly into actuator commands (e.g. firing motor power electronic gates based directly upon encoder feedback signals to achieve the required torque/velocity of the shaft). Sensor fusion and internal models may first be used to estimate parameters of interest (e.g. the position of the robot's gripper) from noisy sensor data. An immediate task (such as moving the gripper in a certain direction until an object is detected with a proximity sensor) is sometimes inferred from these estimates. Techniques from control theory are generally used to convert the higher-level tasks into individual commands that drive the actuators, most often using kinematic and dynamic models of the mechanical structure.[110][111][112]\nAt longer time scales or with more sophisticated tasks, the robot may need to build and reason with a \"cognitive\" model. Cognitive models try to represent the robot, the world, and how the two interact. Pattern recognition and computer vision can be used to track objects.[110] Mapping techniques can be used to build maps of the world. Finally, motion planning and other artificial intelligence techniques may be used to figure out how to act. For example, a planner may figure out how to achieve a task without hitting obstacles, falling over, etc.\nModern commercial robotic control systems are highly complex, integrate multiple sensors and effectors, have many interacting degrees-of-freedom (DOF) and require operator interfaces, programming tools and real-time capabilities.[111] They are oftentimes interconnected to wider communication networks and in many cases are now both IoT-enabled and mobile.[113] Progress towards open architecture, layered, user-friendly and 'intelligent' sensor-based interconnected robots has emerged from earlier concepts related to Flexible Manufacturing Systems (FMS), and several 'open or 'hybrid' reference architectures exist which assist developers of robot control software and hardware to move beyond traditional, earlier notions of 'closed' robot control systems have been proposed.[112] Open architecture controllers are said to be better able to meet the growing requirements of a wide range of robot users, including system developers, end users and research scientists, and are better positioned to deliver the advanced robotic concepts related to Industry 4.0.[112] In addition to utilizing many established features of robot controllers, such as position, velocity and force control of end effectors, they also enable IoT interconnection and the implementation of more advanced sensor fusion and control techniques, including adaptive control, Fuzzy control and Artificial Neural Network (ANN)-based control.[112] When implemented in real-time, such techniques can potentially improve the stability and performance of robots operating in unknown or uncertain environments by enabling the control systems to learn and adapt to environmental changes.[114] There are several examples of reference architectures for robot controllers, and also examples of successful implementations of actual robot controllers developed from them. One example of a generic reference architecture and associated interconnected, open-architecture robot and controller implementation was developed by Michael Short and colleagues at the University of Sunderland in the UK in 2000 (pictured right).[112] The robot was used in a number of research and development studies, including prototype implementation of novel advanced and intelligent control and environment mapping methods in real-time.[114][115]\nControl systems may also have varying levels of autonomy.\nAnother classification takes into account the interaction between human control and the machine motions.\n\nComputer vision is the science and technology of machines that see. As a scientific discipline, computer vision is concerned with the theory behind artificial systems that extract information from images. The image data can take many forms, such as video sequences and views from cameras.\nIn most practical computer vision applications, the computers are pre-programmed to solve a particular task, but methods based on learning are now becoming increasingly common.\nComputer vision systems rely on image sensors that detect electromagnetic radiation which is typically in the form of either visible light or infra-red light. The sensors are designed using solid-state physics. The process by which light propagates and reflects off surfaces is explained using optics. Sophisticated image sensors even require quantum mechanics to provide a complete understanding of the image formation process. Robots can also be equipped with multiple vision sensors to be better able to compute the sense of depth in the environment. Like human eyes, robots' \"eyes\" must also be able to focus on a particular area of interest, and also adjust to variations in light intensities.\nThere is a subfield within computer vision where artificial systems are designed to mimic the processing and behavior of biological system, at different levels of complexity. Also, some of the learning-based methods developed within computer vision have a background in biology.\nThough a significant percentage of robots in commission today are either human controlled or operate in a static environment, there is an increasing interest in robots that can operate autonomously in a dynamic environment. These robots require some combination of navigation hardware and software in order to traverse their environment. In particular, unforeseen events (e.g. people and other obstacles that are not stationary) can cause problems or collisions. Some highly advanced robots such as ASIMO and Meinü robot have particularly good robot navigation hardware and software. Also, self-controlled cars, Ernst Dickmanns' driverless car, and the entries in the DARPA Grand Challenge, are capable of sensing the environment well and subsequently making navigational decisions based on this information, including by a swarm of autonomous robots.[118] Most of these robots employ a GPS navigation device with waypoints, along with radar, sometimes combined with other sensory data such as lidar, video cameras, and inertial guidance systems for better navigation between waypoints.\nThe state of the art in sensory intelligence for robots will have to progress through several orders of magnitude if we want the robots working in our homes to go beyond vacuum-cleaning the floors. If robots are to work effectively in homes and other non-industrial environments, the way they are instructed to perform their jobs, and especially how they will be told to stop will be of critical importance. The people who interact with them may have little or no training in robotics, and so any interface will need to be extremely intuitive. Science fiction authors also typically assume that robots will eventually be capable of communicating with humans through speech, gestures, and facial expressions, rather than a command-line interface. Although speech would be the most natural way for the human to communicate, it is unnatural for the robot. It will probably be a long time before robots interact as naturally as the fictional C-3PO, or Data of Star Trek, Next Generation. Even though the current state of robotics cannot meet the standards of these robots from science-fiction, robotic media characters (e.g., Wall-E, R2-D2) can elicit audience sympathies that increase people's willingness to accept actual robots in the future.[119] Acceptance of social robots is also likely to increase if people can meet a social robot under appropriate conditions. Studies have shown that interacting with a robot by looking at, touching, or even imagining interacting with the robot can reduce negative feelings that some people have about robots before interacting with them.[120] However, if pre-existing negative sentiments are especially strong, interacting with a robot can increase those negative feelings towards robots.[120]\nInterpreting the continuous flow of sounds coming from a human, in real time, is a difficult task for a computer, mostly because of the great variability of speech.[121] The same word, spoken by the same person may sound different depending on local acoustics, volume, the previous word, whether or not the speaker has a cold, etc.. It becomes even harder when the speaker has a different accent.[122] Nevertheless, great strides have been made in the field since Davis, Biddulph, and Balashek designed the first \"voice input system\" which recognized \"ten digits spoken by a single user with 100% accuracy\" in 1952.[123] Currently, the best systems can recognize continuous, natural speech, up to 160 words per minute, with an accuracy of 95%.[124] With the help of artificial intelligence, machines nowadays can use people's voice to identify their emotions such as satisfied or angry.[125]\nOther hurdles exist when allowing the robot to use voice for interacting with humans. For social reasons, synthetic voice proves suboptimal as a communication medium,[126] making it necessary to develop the emotional component of robotic voice through various techniques.[127][128] An advantage of diphonic branching is the emotion that the robot is programmed to project, can be carried on the voice tape, or phoneme, already pre-programmed onto the voice media. One of the earliest examples is a teaching robot named Leachim developed in 1974 by Michael J. Freeman.[129][130] Leachim was able to convert digital memory to rudimentary verbal speech on pre-recorded computer discs.[131] It was programmed to teach students in The Bronx, New York.[131]\nOne can imagine, in the future, explaining to a robot chef how to make a pastry, or asking directions from a robot police officer. In both of these cases, making hand gestures would aid the verbal descriptions. In the first case, the robot would be recognizing gestures made by the human, and perhaps repeating them for confirmation. In the second case, the robot police officer would gesture to indicate \"down the road, then turn right\". It is likely that gestures will make up a part of the interaction between humans and robots.[132] A great many systems have been developed to recognize human hand gestures.[133]\nFacial expressions can provide rapid feedback on the progress of a dialog between two humans, and soon may be able to do the same for humans and robots. Robotic faces have been constructed by Hanson Robotics using their elastic polymer called Frubber, allowing a large number of facial expressions due to the elasticity of the rubber facial coating and embedded subsurface motors (servos).[134] The coating and servos are built on a metal skull. A robot should know how to approach a human, judging by their facial expression and body language. Whether the person is happy, frightened, or crazy-looking affects the type of interaction expected of the robot. Likewise, robots like Kismet and the more recent addition, Nexi[135] can produce a range of facial expressions, allowing it to have meaningful social exchanges with humans.[136]\nArtificial emotions can also be generated, composed of a sequence of facial expressions or gestures. As can be seen from the movie Final Fantasy: The Spirits Within, the programming of these artificial emotions is complex and requires a large amount of human observation. To simplify this programming in the movie, presets were created together with a special software program. This decreased the amount of time needed to make the film. These presets could possibly be transferred for use in real-life robots. An example of a robot with artificial emotions is Robin the Robot developed by an Armenian IT company Expper Technologies, which uses AI-based peer-to-peer interaction. Its main task is achieving emotional well-being, i.e. overcome stress and anxiety. Robin was trained to analyze facial expressions and use his face to display his emotions given the context. The robot has been tested by kids in US clinics, and observations show that Robin increased the appetite and cheerfulness of children after meeting and talking.[137]\nMany of the robots of science fiction have a personality, something which may or may not be desirable in the commercial robots of the future.[138] Nevertheless, researchers are trying to create robots which appear to have a personality:[139][140] i.e. they use sounds, facial expressions, and body language to try to convey an internal state, which may be joy, sadness, or fear. One commercial example is Pleo, a toy robot dinosaur, which can exhibit several apparent emotions.[141]\nProxemics is the study of personal space, and HRI systems may try to model and work with its concepts for human interactions.\n\nMuch of the research in robotics focuses not on specific industrial tasks, but on investigations into new types of robots, alternative ways to think about or design robots, and new ways to manufacture them. Other investigations, such as MIT's cyberflora project, are almost wholly academic.\nTo describe the level of advancement of a robot, the term \"Generation Robots\" can be used. This term is coined by Professor Hans Moravec, Principal Research Scientist at the Carnegie Mellon University Robotics Institute in describing the near future evolution of robot technology. First-generation robots, Moravec predicted in 1997, should have an intellectual capacity comparable to perhaps a lizard and should become available by 2010. Because the first generation robot would be incapable of learning, however, Moravec predicts that the second generation robot would be an improvement over the first and become available by 2020, with the intelligence maybe comparable to that of a mouse. The third generation robot should have intelligence comparable to that of a monkey. Though fourth generation robots, robots with human intelligence, professor Moravec predicts, would become possible, he does not predict this happening before around 2040 or 2050.[142]\nThe study of motion can be divided into kinematics and dynamics.[143] Direct kinematics or forward kinematics refers to the calculation of end effector position, orientation, velocity, and acceleration when the corresponding joint values are known. Inverse kinematics refers to the opposite case in which required joint values are calculated for given end effector values, as done in path planning. Some special aspects of kinematics include handling of redundancy (different possibilities of performing the same movement), collision avoidance, and singularity avoidance. Once all relevant positions, velocities, and accelerations have been calculated using kinematics, methods from the field of dynamics are used to study the effect of forces upon these movements. Direct dynamics refers to the calculation of accelerations in the robot once the applied forces are known. Direct dynamics is used in computer simulations of the robot. Inverse dynamics refers to the calculation of the actuator forces necessary to create a prescribed end-effector acceleration. This information can be used to improve the control algorithms of a robot.\nIn each area mentioned above, researchers strive to develop new concepts and strategies, improve existing ones, and improve the interaction between these areas. To do this, criteria for \"optimal\" performance and ways to optimize design, structure, and control of robots must be developed and implemented.\nOpen source robotics research seeks standards for defining, and methods for designing and building, robots so that they can easily be reproduced by anyone. Research includes legal and technical definitions; seeking out alternative tools and materials to reduce costs and simplify builds; and creating interfaces and standards for designs to work together. Human usability research also investigates how to best document builds through visual, text or video instructions.\nEvolutionary robots is a methodology that uses evolutionary computation to help design robots, especially the body form, or motion and behavior controllers. In a similar way to natural evolution, a large population of robots is allowed to compete in some way, or their ability to perform a task is measured using a fitness function. Those that perform worst are removed from the population and replaced by a new set, which have new behaviors based on those of the winners. Over time the population improves, and eventually a satisfactory robot may appear. This happens without any direct programming of the robots by the researchers. Researchers use this method both to create better robots,[144] and to explore the nature of evolution.[145] Because the process often requires many generations of robots to be simulated,[146] this technique may be run entirely or mostly in simulation, using a robot simulator software package, then tested on real robots once the evolved algorithms are good enough.[147] Currently, there are about 10 million industrial robots toiling around the world, and Japan is the top country having high density of utilizing robots in its manufacturing industry.[citation needed]\nBionics and biomimetics apply the physiology and methods of locomotion of animals to the design of robots. For example, the design of BionicKangaroo was based on the way kangaroos jump.\nSwarm robotics is an approach to the coordination of multiple robots as a system which consist of large numbers of mostly simple physical robots. ″In a robot swarm, the collective behavior of the robots results from local interactions between the robots and between the robots and the environment in which they act.″* [118]\nThere has been some research into whether robotics algorithms can be run more quickly on quantum computers than they can be run on digital computers. This area has been referred to as quantum robotics.[148]\nThe main venues for robotics research are the international conferences ICRA and IROS.\nRobotics engineers design robots, maintain them, develop new applications for them, and conduct research to expand the potential of robotics.[151] Robots have become a popular educational tool in some middle and high schools, particularly in parts of the USA,[152] as well as in numerous youth summer camps, raising interest in programming, artificial intelligence, and robotics among students.\nRobotics is an essential component in many modern manufacturing environments. As factories increase their use of robots, the number of robotics–related jobs grow and have been observed to be steadily rising.[153] The employment of robots in industries has increased productivity and efficiency savings and is typically seen as a long-term investment for benefactors. A study found that 47 percent of US jobs are at risk to automation \"over some unspecified number of years\".[154] These claims have been criticized on the ground that social policy, not AI, causes unemployment.[155] In a 2016 article in The Guardian, Stephen Hawking stated \"The automation of factories has already decimated jobs in traditional manufacturing, and the rise of artificial intelligence is likely to extend this job destruction deep into the middle classes, with only the most caring, creative or supervisory roles remaining\".[156]\nAccording to a GlobalData September 2021 report, the robotics industry was worth $45bn in 2020, and by 2030, it will have grown at a compound annual growth rate (CAGR) of 29% to $568bn, driving jobs in robotics and related industries.[157]\nA discussion paper drawn up by EU-OSHA highlights how the spread of robotics presents both opportunities and challenges for occupational safety and health (OSH).[158]\nThe greatest OSH benefits stemming from the wider use of robotics should be substitution for people working in unhealthy or dangerous environments. In space, defense, security, or the nuclear industry, but also in logistics, maintenance, and inspection, autonomous robots are particularly useful in replacing human workers performing dirty, dull or unsafe tasks, thus avoiding workers' exposures to hazardous agents and conditions and reducing physical, ergonomic and psychosocial risks. For example, robots are already used to perform repetitive and monotonous tasks, to handle radioactive material or to work in explosive atmospheres. In the future, many other highly repetitive, risky or unpleasant tasks will be performed by robots in a variety of sectors like agriculture, construction, transport, healthcare, firefighting or cleaning services.[159]\nMoreover, there are certain skills to which humans will be better suited than machines for some time to come and the question is how to achieve the best combination of human and robot skills. The advantages of robotics include heavy-duty jobs with precision and repeatability, whereas the advantages of humans include creativity, decision-making, flexibility, and adaptability. This need to combine optimal skills has resulted in collaborative robots and humans sharing a common workspace more closely and led to the development of new approaches and standards to guarantee the safety of the \"man-robot merger\". Some European countries are including robotics in their national programs and trying to promote a safe and flexible cooperation between robots and operators to achieve better productivity. For example, the German Federal Institute for Occupational Safety and Health (BAuA) organises annual workshops on the topic \"human-robot collaboration\".\nIn the future, cooperation between robots and humans will be diversified, with robots increasing their autonomy and human-robot collaboration reaching completely new forms. Current approaches and technical standards[160][161] aiming to protect employees from the risk of working with collaborative robots will have to be revised.\nGreat user experience predicts the needs, experiences, behaviors, language and cognitive abilities, and other factors of each user group. It then uses these insights to produce a product or solution that is ultimately useful and usable. For robots, user experience begins with an understanding of the robot's intended task and environment, while considering any possible social impact the robot may have on human operations and interactions with it.[162]\nIt defines that communication as the transmission of information through signals, which are elements perceived through touch, sound, smell and sight.[163] The author states that the signal connects the sender to the receiver and consists of three parts: the signal itself, what it refers to, and the interpreter. Body postures and gestures, facial expressions, hand and head movements are all part of nonverbal behavior and communication. Robots are no exception when it comes to human-robot interaction. Therefore, humans use their verbal and nonverbal behaviors to communicate their defining characteristics. Similarly, social robots need this coordination to perform human-like behaviors.\nRobotics is an interdisciplinary field, combining primarily mechanical engineering and computer science but also drawing on electronic engineering and other subjects. The usual way to build a career in robotics is to complete an undergraduate degree in one of these established subjects, followed by a graduate (masters') degree in Robotics. Graduate degrees are typically joined by students coming from all of the contributing disciplines, and include familiarization of relevant undergraduate level subject matter from each of them, followed by specialist study in pure robotics topics which build upon them. As an interdisciplinary subject, robotics graduate programmes tend to be especially reliant on students working and learning together and sharing their knowledge and skills from their home discipline first degrees.    \nRobotics industry careers then follow the same pattern, with most roboticists working as part of interdisciplinary teams of specialists from these home disciplines followed by the robotics graduate degrees which enable them to work together. Workers typically continue to identify as members of their home disciplines who work in robotics, rather than as 'roboticists'. This structure is reinforced by the nature of some engineering professions, which grant chartered engineer status to members of home disciplines rather than to robotics as a whole.\nRobotics careers are widely predicted to grow during in the 21st century, as robots replace more manual and intellectual human work. Workers who lose their jobs to robotics may be well-placed to retrain to build and maintain these robots, using their domain-specific knowledge and skills.\nIn 1948, Norbert Wiener formulated the principles of cybernetics, the basis of practical robotics.\nFully autonomous robots only appeared in the second half of the 20th century. The first digitally operated and programmable robot, the Unimate, was installed in 1961 to lift hot pieces of metal from a die casting machine and stack them. Commercial and industrial robots are widespread today and used to perform jobs more cheaply, more accurately, and more reliably than humans. They are also employed in some jobs that are too dirty, dangerous, or dull to be suitable for humans. Robots are widely used in manufacturing, assembly, packing and packaging, mining, transport, earth and space exploration, surgery,[164] weaponry, laboratory research, safety, and the mass production of consumer and industrial goods.[165]"}
{"url":"https://en.wikipedia.org/wiki/Robot","text":"A robot is a machine—especially one programmable by a computer—capable of carrying out a complex series of actions automatically.[2] A robot can be guided by an external control device, or the control may be embedded within. Robots may be constructed to evoke human form, but most robots are task-performing machines, designed with an emphasis on stark functionality, rather than expressive aesthetics.\nRobots can be autonomous or semi-autonomous and range from humanoids such as Honda's Advanced Step in Innovative Mobility (ASIMO) and TOSY's TOSY Ping Pong Playing Robot (TOPIO) to industrial robots, medical operating robots, patient assist robots, dog therapy robots, collectively programmed swarm robots, UAV drones such as General Atomics MQ-1 Predator, and even microscopic nano robots. By mimicking a lifelike appearance or automating movements, a robot may convey a sense of intelligence or thought of its own. Autonomous things are expected to proliferate in the future, with home robotics and the autonomous car as some of the main drivers.[3]\nThe branch of technology that deals with the design, construction, operation, and application of robots,[4] as well as computer systems for their control, sensory feedback, and information processing is robotics. These technologies deal with automated machines that can take the place of humans in dangerous environments or manufacturing processes, or resemble humans in appearance, behavior, or cognition. Many of today's robots are inspired by nature contributing to the field of bio-inspired robotics. These robots have also created a newer branch of robotics: soft robotics.\nFrom the time of ancient civilization, there have been many accounts of user-configurable automated devices and even automata resembling humans and other animals, such as animatronics, designed primarily as entertainment. As mechanical techniques developed through the Industrial age, there appeared more practical applications such as automated machines, remote-control and wireless remote-control.\nThe term comes from a Slavic root, robot-, with meanings associated with labor. The word 'robot' was first used to denote a fictional humanoid in a 1920 Czech-language play R.U.R. (Rossumovi Univerzální Roboti – Rossum's Universal Robots) by Karel Čapek, though it was Karel's brother Josef Čapek who was the word's true inventor.[5][6][7] Electronics evolved into the driving force of development with the advent of the first electronic autonomous robots created by William Grey Walter in Bristol, England in 1948, as well as Computer Numerical Control (CNC) machine tools in the late 1940s by John T. Parsons and Frank L. Stulen.\nThe first commercial, digital and programmable robot was built by George Devol in 1954 and was named the Unimate. It was sold to General Motors in 1961 where it was used to lift pieces of hot metal from die casting machines at the Inland Fisher Guide Plant in the West Trenton section of Ewing Township, New Jersey.[8]\nRobots have replaced humans[9] in performing repetitive and dangerous tasks which humans prefer not to do, or are unable to do because of size limitations, or which take place in extreme environments such as outer space or the bottom of the sea. There are concerns about the increasing use of robots and their role in society. Robots are blamed for rising technological unemployment as they replace workers in increasing numbers of functions.[10] The use of robots in military combat raises ethical concerns. The possibilities of robot autonomy and potential repercussions have been addressed in fiction and may be a realistic concern in the future.\nThe word robot can refer to both physical robots and virtual software agents, but the latter are usually referred to as bots.[11] There is no consensus on which machines qualify as robots but there is general agreement among experts, and the public, that robots tend to possess some or all of the following abilities and functions: accept electronic programming, process data or physical perceptions electronically, operate autonomously to some degree, move around, operate physical parts of itself or physical processes, sense and manipulate their environment, and exhibit intelligent behavior, especially behavior which mimics humans or other animals.[12][13] Related to the concept of a robot is the field of synthetic biology, which studies entities whose nature is more comparable to living things than to machines.\nThe idea of automata originates in the mythologies of many cultures around the world. Engineers and inventors from ancient civilizations, including Ancient China,[14] Ancient Greece, and Ptolemaic Egypt,[15] attempted to build self-operating machines, some resembling animals and humans. Early descriptions of automata include the artificial doves of Archytas,[16] the artificial birds of Mozi and Lu Ban,[17] a \"speaking\" automaton by Hero of Alexandria, a washstand automaton by Philo of Byzantium, and a human automaton described in the Lie Zi.[14]\nMany ancient mythologies, and most modern religions include artificial people, such as the mechanical servants built by the Greek god Hephaestus[18] (Vulcan to the Romans), the clay golems of Jewish legend and clay giants of Norse legend, and Galatea, the mythical statue of Pygmalion that came to life. Since circa 400 BC, myths of Crete include Talos, a man of bronze who guarded the island from pirates.\nIn ancient Greece, the Greek engineer Ctesibius (c. 270 BC) \"applied a knowledge of pneumatics and hydraulics to produce the first organ and water clocks with moving figures.\"[19]: 2 [20] In the 4th century BC, the Greek mathematician Archytas of Tarentum postulated a mechanical steam-operated bird he called \"The Pigeon\". Hero of Alexandria (10–70 AD), a Greek mathematician and inventor, created numerous user-configurable automated devices, and described machines powered by air pressure, steam and water.[21]\nThe 11th century Lokapannatti tells of how the Buddha's relics were protected by mechanical robots (bhuta vahana yanta), from the kingdom of Roma visaya (Rome); until they were disarmed by King Ashoka.[22]\nIn ancient China, the 3rd-century text of the Lie Zi describes an account of humanoid automata, involving a much earlier encounter between Chinese emperor King Mu of Zhou and a mechanical engineer known as Yan Shi, an 'artificer'. Yan Shi proudly presented the king with a life-size, human-shaped figure of his mechanical 'handiwork' made of leather, wood, and artificial organs.[14] There are also accounts of flying automata in the Han Fei Zi and other texts, which attributes the 5th century BC Mohist philosopher Mozi and his contemporary Lu Ban with the invention of artificial wooden birds (ma yuan) that could successfully fly.[17]\n In 1066, the Chinese inventor Su Song built a water clock in the form of a tower which featured mechanical figurines which chimed the hours.[23][24][25] His mechanism had a programmable drum machine with pegs (cams) that bumped into little levers that operated percussion instruments. The drummer could be made to play different rhythms and different drum patterns by moving the pegs to different locations.[25]\nSamarangana Sutradhara, a Sanskrit treatise by Bhoja (11th century), includes a chapter about the construction of mechanical contrivances (automata), including mechanical bees and birds, fountains shaped like humans and animals, and male and female dolls that refilled oil lamps, danced, played instruments, and re-enacted scenes from Hindu mythology.[26][27][28]\n13th century Muslim scientist Ismail al-Jazari created several automated devices. He built automated moving peacocks driven by hydropower.[29] He also invented the earliest known automatic gates, which were driven by hydropower,[30] created automatic doors as part of one of his elaborate water clocks.[31] One of al-Jazari's humanoid automata was a waitress that could serve water, tea or drinks. The drink was stored in a tank with a reservoir from where the drink drips into a bucket and, after seven minutes, into a cup, after which the waitress appears out of an automatic door serving the drink.[32] Al-Jazari invented a hand washing automaton incorporating a flush mechanism now used in modern flush toilets. It features a female humanoid automaton standing by a basin filled with water. When the user pulls the lever, the water drains and the female automaton refills the basin.[19]\n\nMark E. Rosheim summarizes the advances in robotics made by Muslim engineers, especially al-Jazari, as follows:Unlike the Greek designs, these Arab examples reveal an interest, not only in dramatic illusion, but in manipulating the environment for human comfort. Thus, the greatest contribution the Arabs made, besides preserving, disseminating and building on the work of the Greeks, was the concept of practical application. This was the key element that was missing in Greek robotic science.[19]: 9  In the 14th century, the coronation of Richard II of England featured an automata angel.[34]\nIn Renaissance Italy, Leonardo da Vinci (1452–1519) sketched plans for a humanoid robot around 1495. Da Vinci's notebooks, rediscovered in the 1950s, contained detailed drawings of a mechanical knight now known as Leonardo's robot, able to sit up, wave its arms and move its head and jaw.[35] The design was probably based on anatomical research recorded in his Vitruvian Man. It is not known whether he attempted to build it. According to Encyclopædia Britannica, Leonardo da Vinci may have been influenced by the classic automata of al-Jazari.[29]\nIn Japan, complex animal and human automata were built between the 17th to 19th centuries, with many described in the 18th century Karakuri zui (Illustrated Machinery, 1796). One such automaton was the karakuri ningyō, a mechanized puppet.[36] Different variations of the karakuri existed: the Butai karakuri, which were used in theatre, the Zashiki karakuri, which were small and used in homes, and the Dashi karakuri which were used in religious festivals, where the puppets were used to perform reenactments of traditional myths and legends.\nIn France, between 1738 and 1739, Jacques de Vaucanson exhibited several life-sized automatons: a flute player, a pipe player and a duck. The mechanical duck could flap its wings, crane its neck, and swallow food from the exhibitor's hand, and it gave the illusion of digesting its food by excreting matter stored in a hidden compartment.[37] About 30 years later in Switzerland the clockmaker Pierre Jaquet-Droz made several complex mechanical figures that could write and play music. Several of these devices still exist and work.[38]\nRemotely operated vehicles were demonstrated in the late 19th century in the form of several types of remotely controlled torpedoes. The early 1870s saw remotely controlled torpedoes by John Ericsson (pneumatic), John Louis Lay (electric wire guided), and Victor von Scheliha (electric wire guided).[39]\nThe Brennan torpedo, invented by Louis Brennan in 1877, was powered by two contra-rotating propellers that were spun by rapidly pulling out wires from drums wound inside the torpedo. Differential speed on the wires connected to the shore station allowed the torpedo to be guided to its target, making it \"the world's first practical guided missile\".[40] In 1897 the British inventor Ernest Wilson was granted a patent for a torpedo remotely controlled by \"Hertzian\" (radio) waves[41][42] and in 1898 Nikola Tesla publicly demonstrated a wireless-controlled torpedo that he hoped to sell to the US Navy.[43][44]\nIn 1903, the Spanish engineer Leonardo Torres Quevedo demonstrated a radio control system called \"Telekino\" at the Paris Academy of Sciences,[45] which he wanted to use to control an airship of his own design. He obtained some patents in other countries.[46] Unlike the previous mechanisms, which carried out actions of the 'on/off' type, Torres developed a system for controlling any mechanical or electrical device with different states of operation.[47] \nThe transmitter was capable of sending a family of different codewords by means of a binary telegraph signal to the receiver, which was able to set up a different state of operation in the device being used, depending on the codeword. Specifically, it was able to do up to 19 different actions.[48][49]\nArchibald Low, known as the \"father of radio guidance systems\" for his pioneering work on guided rockets and planes during the First World War. In 1917, he demonstrated a remote controlled aircraft to the Royal Flying Corps and in the same year built the first wire-guided rocket.\nIn 1928, one of the first humanoid robots, Eric, was exhibited at the annual exhibition of the Model Engineers Society in London, where it delivered a speech. Invented by W. H. Richards, the robot's frame consisted of an aluminium body of armour with eleven electromagnets and one motor powered by a twelve-volt power source. The robot could move its hands and head and could be controlled through remote control or voice control.[50] Both Eric and his \"brother\" George toured the world.[51]\nWestinghouse Electric Corporation built Televox in 1926; it was a cardboard cutout connected to various devices which users could turn on and off. In 1939, the humanoid robot known as Elektro was debuted at the 1939 New York World's Fair.[52][53] Seven feet tall (2.1 m) and weighing 265 pounds (120.2 kg), it could walk by voice command, speak about 700 words (using a 78-rpm record player), smoke cigarettes, blow up balloons, and move its head and arms. The body consisted of a steel gear, cam and motor skeleton covered by an aluminum skin. In 1928, Japan's first robot, Gakutensoku, was designed and constructed by biologist Makoto Nishimura.\nThe German V-1 flying bomb was equipped with systems for automatic guidance and range control, flying on a predetermined course (which could include a 90-degree turn) and entering a terminal dive after a predetermined distance. It was reported as being a 'robot' in contemporary descriptions [54]\nThe first electronic autonomous robots with complex behaviour were created by William Grey Walter of the Burden Neurological Institute at Bristol, England in 1948 and 1949. He wanted to prove that rich connections between a small number of brain cells could give rise to very complex behaviors – essentially that the secret of how the brain worked lay in how it was wired up. His first robots, named Elmer and Elsie, were constructed between 1948 and 1949 and were often described as tortoises due to their shape and slow rate of movement. The three-wheeled tortoise robots were capable of phototaxis, by which they could find their way to a recharging station when they ran low on battery power.\nWalter stressed the importance of using purely analogue electronics to simulate brain processes at a time when his contemporaries such as Alan Turing and John von Neumann were all turning towards a view of mental processes in terms of digital computation. His work inspired subsequent generations of robotics researchers such as Rodney Brooks, Hans Moravec and Mark Tilden. Modern incarnations of Walter's turtles may be found in the form of BEAM robotics.[55]\nThe first digitally operated and programmable robot was invented by George Devol in 1954 and was ultimately called the Unimate. This ultimately laid the foundations of the modern robotics industry.[56] Devol sold the first Unimate to General Motors in 1960, and it was installed in 1961 in a plant in Trenton, New Jersey to lift hot pieces of metal from a die casting machine and stack them.[57]\nThe first palletizing robot was introduced in 1963 by the Fuji Yusoki Kogyo Company.[58] In 1973, a robot with six electromechanically driven axes was patented[59][60][61] by KUKA robotics in Germany, and the programmable universal manipulation arm was invented by Victor Scheinman in 1976, and the design was sold to Unimation.\nCommercial and industrial robots are now in widespread use performing jobs more cheaply or with greater accuracy and reliability than humans. They are also employed for jobs which are too dirty, dangerous or dull to be suitable for humans. Robots are widely used in manufacturing, assembly and packing, transport, earth and space exploration, surgery, weaponry, laboratory research, and mass production of consumer and industrial goods.[62]\nVarious techniques have emerged to develop the science of robotics and robots. One method is evolutionary robotics, in which a number of differing robots are submitted to tests. Those which perform best are used as a model to create a subsequent \"generation\" of robots. Another method is developmental robotics, which tracks changes and development within a single robot in the areas of problem-solving and other functions. Another new type of robot is just recently introduced which acts both as a smartphone and robot and is named RoboHon.[63]\nAs robots become more advanced, eventually there may be a standard computer operating system designed mainly for robots. Robot Operating System (ROS) is an open-source software set of programs being developed at Stanford University, the Massachusetts Institute of Technology, and the Technical University of Munich, Germany, among others. ROS provides ways to program a robot's navigation and limbs regardless of the specific hardware involved. It also provides high-level commands for items like image recognition and even opening doors. When ROS boots up on a robot's computer, it would obtain data on attributes such as the length and movement of robots' limbs. It would relay this data to higher-level algorithms. Microsoft is also developing a \"Windows for robots\" system with its Robotics Developer Studio, which has been available since 2007.[64]\nJapan hopes to have full-scale commercialization of service robots by 2025. Much technological research in Japan is led by Japanese government agencies, particularly the Trade Ministry.[65]\nMany future applications of robotics seem obvious to people, even though they are well beyond the capabilities of robots available at the time of the prediction.[66][67] As early as 1982 people were confident that someday robots would:[68] 1. Clean parts by removing molding flash 2. Spray paint automobiles with absolutely no human presence 3. Pack things in boxes—for example, orient and nest chocolate candies in candy boxes 4. Make electrical cable harness 5. Load trucks with boxes—a packing problem 6. Handle soft goods, such as garments and shoes 7. Shear sheep 8. Be used asprostheses 9. Cook fast food and work in other service industries 10. Work as a household robot.\nGenerally such predictions are overly optimistic in timescale.\nIn 2008, Caterpillar Inc. developed a dump truck which can drive itself without any human operator.[69] Many analysts believe that self-driving trucks may eventually revolutionize logistics.[70] By 2014, Caterpillar had a self-driving dump truck which is expected to greatly change the process of mining. In 2015, these Caterpillar trucks were actively used in mining operations in Australia by the mining company Rio Tinto Coal Australia.[71][72][73][74] Some analysts believe that within the next few decades, most trucks will be self-driving.[75]\nA literate or 'reading robot' named Marge has intelligence that comes from software. She can read newspapers, find and correct misspelled words, learn about banks like Barclays, and understand that some restaurants are better places to eat than others.[76]\nBaxter is a new robot introduced in 2012 which learns by guidance. A worker could teach Baxter how to perform a task by moving its hands in the desired motion and having Baxter memorize them. Extra dials, buttons, and controls are available on Baxter's arm for more precision and features. Any regular worker could program Baxter and it only takes a matter of minutes, unlike usual industrial robots that take extensive programs and coding to be used. This means Baxter needs no programming to operate. No software engineers are needed. This also means Baxter can be taught to perform multiple, more complicated tasks. Sawyer was added in 2015 for smaller, more precise tasks.[77]\nPrototype cooking robots have been developed and could be programmed for autonomous, dynamic and adjustable preparation of discrete meals.[78][79]\nThe word robot was introduced to the public by the Czech interwar writer Karel Čapek in his play R.U.R. (Rossum's Universal Robots), published in 1920.[6] The play begins in a factory that uses a chemical substitute for protoplasm to manufacture living, simplified people called robots. The play does not focus in detail on the technology behind the creation of these living creatures, but in their appearance they prefigure modern ideas of androids, creatures who can be mistaken for humans. These mass-produced workers are depicted as efficient but emotionless, incapable of original thinking and indifferent to self-preservation. At issue is whether the robots are being exploited and the consequences of human dependence upon commodified labor (especially after a number of specially-formulated robots achieve self-awareness and incite robots all around the world to rise up against the humans).\nKarel Čapek himself did not coin the word. He wrote a short letter in reference to an etymology in the Oxford English Dictionary in which he named his brother, the painter and writer Josef Čapek, as its actual originator.[6]\nIn an article in the Czech journal Lidové noviny in 1933, he explained that he had originally wanted to call the creatures laboři ('workers', from Latin labor). However, he did not like the word, and sought advice from his brother Josef, who suggested roboti. The word robota means literally 'corvée, serf labor', and figuratively 'drudgery, hard work' in Czech and also (more general) 'work, labor' in many Slavic languages (e.g.: Bulgarian, Russian, Serbian, Slovak, Polish, Macedonian, Ukrainian, archaic Czech, as well as robot in Hungarian). Traditionally the robota (Hungarian robot) was the work period a serf (corvée) had to give for his lord, typically six months of the year. The origin of the word is the Old Church Slavonic rabota 'servitude' ('work' in contemporary Bulgarian, Macedonian and Russian), which in turn comes from the Proto-Indo-European root *orbh-. Robot is cognate with the German Arbeit 'work'.[80][81]\nEnglish pronunciation of the word has evolved relatively quickly since its introduction. In the U.S. during the late 1930s to early 1940s it was pronounced /ˈroʊboʊt/.[82][better source needed] By the late 1950s to early 1960s, some were pronouncing it /ˈroʊbət/, while others used /ˈroʊbɒt/[83] By the 1970s, its current pronunciation /ˈroʊbɒt/ had become predominant.\nThe word robotics, used to describe this field of study,[4] was coined by the science fiction writer Isaac Asimov. Asimov created the Three Laws of Robotics which are a recurring theme in his books. These have since been used by many others to define laws used in fiction. (The three laws are pure fiction, and no technology yet created has the ability to understand or follow them, and in fact most robots serve military purposes, which run quite contrary to the first law and often the third law. \"People think about Asimov's laws, but they were set up to point out how a simple ethical system doesn't work. If you read the short stories, every single one is about a failure, and they are totally impractical,\" said Dr. Joanna Bryson of the University of Bath.[84])\nMobile robots[85] have the capability to move around in their environment and are not fixed to one physical location. An example of a mobile robot that is in common use today is the automated guided vehicle or automatic guided vehicle (AGV). An AGV is a mobile robot that follows markers or wires in the floor, or uses vision or lasers.[86] AGVs are discussed later in this article.\nMobile robots are also found in industry, military and security environments.[87] They also appear as consumer products, for entertainment or to perform certain tasks like vacuum cleaning. Mobile robots are the focus of a great deal of current research and almost every major university has one or more labs that focus on mobile robot research.[88]\nMobile robots are usually used in tightly controlled environments such as on assembly lines because they have difficulty responding to unexpected interference. Because of this most humans rarely encounter robots. However domestic robots for cleaning and maintenance are increasingly common in and around homes in developed countries. Robots can also be found in military applications.[89]\nIndustrial robots usually consist of a jointed arm (multi-linked manipulator) and an end effector that is attached to a fixed surface. One of the most common type of end effector is a gripper assembly.\nThe International Organization for Standardization gives a definition of a  manipulating industrial robot in ISO 8373:\n\"an automatically controlled, reprogrammable, multipurpose, manipulator programmable in three or more axes, which may be either fixed in place or mobile for use in industrial automation applications.\"[90]\nThis definition is used by the International Federation of Robotics, the European Robotics Research Network (EURON) and many national standards committees.[91]\nMost commonly industrial robots are fixed robotic arms and manipulators used primarily for production and distribution of goods. The term \"service robot\" is less well-defined. The International Federation of Robotics has proposed a tentative definition, \"A service robot is a robot which operates semi- or fully autonomously to perform services useful to the well-being of humans and equipment, excluding manufacturing operations.\"[92]\nRobots are used as educational assistants to teachers. From the 1980s, robots such as turtles were used in schools and programmed using the Logo language.[93][94]\nThere are robot kits like Lego Mindstorms, BIOLOID, OLLO from ROBOTIS, or BotBrain Educational Robots can help children to learn about mathematics, physics, programming, and electronics. Robotics have also been introduced into the lives of elementary and high school students in the form of robot competitions with the company FIRST (For Inspiration and Recognition of Science and Technology). The organization is the foundation for the FIRST Robotics Competition, FIRST Tech Challenge, FIRST Lego League Challenge and FIRST Lego League Explore competitions.\nThere have also been robots such as the teaching computer, Leachim (1974).[95] Leachim was an early example of speech synthesis using the using the Diphone synthesis method. 2-XL (1976) was a robot shaped game / teaching toy based on branching between audible tracks on an 8-track tape player, both invented by Michael J. Freeman.[96] Later, the 8-track was upgraded to tape cassettes and then to digital.\nModular robots are a new breed of robots that are designed to increase the use of robots by modularizing their architecture.[97] The functionality and effectiveness of a modular robot is easier to increase compared to conventional robots. These robots are composed of a single type of identical, several different identical module types, or similarly shaped modules, which vary in size. Their architectural structure allows hyper-redundancy for modular robots, as they can be designed with more than 8 degrees of freedom (DOF). Creating the programming, inverse kinematics and dynamics for modular robots is more complex than with traditional robots. Modular robots may be composed of L-shaped modules, cubic modules, and U and H-shaped modules. ANAT technology, an early modular robotic technology patented by Robotics Design Inc., allows the creation of modular robots from U- and H-shaped modules that connect in a chain, and are used to form heterogeneous and homogenous modular robot systems. These \"ANAT robots\" can be designed with \"n\" DOF as each module is a complete motorized robotic system that folds relatively to the modules connected before and after it in its chain, and therefore a single module allows one degree of freedom. The more modules that are connected to one another, the more degrees of freedom it will have. L-shaped modules can also be designed in a chain, and must become increasingly smaller as the size of the chain increases, as payloads attached to the end of the chain place a greater strain on modules that are further from the base. ANAT H-shaped modules do not suffer from this problem, as their design allows a modular robot to distribute pressure and impacts evenly amongst other attached modules, and therefore payload-carrying capacity does not decrease as the length of the arm increases. Modular robots can be manually or self-reconfigured to form a different robot, that may perform different applications. Because modular robots of the same architecture type are composed of modules that compose different modular robots, a snake-arm robot can combine with another to form a dual or quadra-arm robot, or can split into several mobile robots, and mobile robots can split into multiple smaller ones, or combine with others into a larger or different one. This allows a single modular robot the ability to be fully specialized in a single task, as well as the capacity to be specialized to perform multiple different tasks.\nModular robotic technology is currently being applied in hybrid transportation,[98] industrial automation,[99] duct cleaning[100] and handling. Many research centres and universities have also studied this technology, and have developed prototypes.\nA collaborative robot or cobot is a robot that can safely and effectively interact with human workers while performing simple industrial tasks. However, end-effectors and other environmental conditions may create hazards, and as such risk assessments should be done before using any industrial motion-control application.[101]\nThe collaborative robots most widely used in industries today are manufactured by Universal Robots in Denmark.[102]\nRethink Robotics—founded by Rodney Brooks, previously with iRobot—introduced Baxter in September 2012; as an industrial robot designed to safely interact with neighboring human workers, and be programmable for performing simple tasks.[103] Baxters stop if they detect a human in the way of their robotic arms and have prominent off switches. Intended for sale to small businesses, they are promoted as the robotic analogue of the personal computer.[104] As of May 2014[update], 190 companies in the US have bought Baxters and they are being used commercially in the UK.[10]\nRoughly half of all the robots in the world are in Asia, 32% in Europe, and 16% in North America, 1% in Australasia and 1% in Africa.[107] 40% of all the robots in the world are in Japan,[108] making Japan the country with the highest number of robots.\nAs robots have become more advanced and sophisticated, experts and academics have increasingly explored the questions of what ethics might govern robots' behavior,[110][111] and whether robots might be able to claim any kind of social, cultural, ethical or legal rights.[112] One scientific team has said that it was possible that a robot brain would exist by 2019.[113] Others predict robot intelligence breakthroughs by 2050.[114] Recent advances have made robotic behavior more sophisticated.[115] The social impact of intelligent robots is subject of a 2010 documentary film called Plug \u0026 Pray.[116]\nVernor Vinge has suggested that a moment may come when computers and robots are smarter than humans. He calls this \"the Singularity\".[117] He suggests that it may be somewhat or possibly very dangerous for humans.[118] This is discussed by a philosophy called Singularitarianism.\nIn 2009, experts attended a conference hosted by the Association for the Advancement of Artificial Intelligence (AAAI) to discuss whether computers and robots might be able to acquire any autonomy, and how much these abilities might pose a threat or hazard. They noted that some robots have acquired various forms of semi-autonomy, including being able to find power sources on their own and being able to independently choose targets to attack with weapons. They also noted that some computer viruses can evade elimination and have achieved \"cockroach intelligence.\" They noted that self-awareness as depicted in science-fiction is probably unlikely, but that there were other potential hazards and pitfalls.[117] Various media sources and scientific groups have noted separate trends in differing areas which might together result in greater robotic functionalities and autonomy, and which pose some inherent concerns.[119][120][121]\nSome experts and academics have questioned the use of robots for military combat, especially when such robots are given some degree of autonomous functions.[122] There are also concerns about technology which might allow some armed robots to be controlled mainly by other robots.[123] The US Navy has funded a report which indicates that, as military robots become more complex, there should be greater attention to implications of their ability to make autonomous decisions.[124][125] One researcher states that autonomous robots might be more humane, as they could make decisions more effectively. However, other experts question this.[126]\nOne robot in particular, the EATR, has generated public concerns[127] over its fuel source, as it can continually refuel itself using organic substances.[128] Although the engine for the EATR is designed to run on biomass and vegetation[129] specifically selected by its sensors, which it can find on battlefields or other local environments, the project has stated that chicken fat can also be used.[130]\nManuel De Landa has noted that \"smart missiles\" and autonomous bombs equipped with artificial perception can be considered robots, as they make some of their decisions autonomously. He believes this represents an important and dangerous trend in which humans are handing over important decisions to machines.[131]\nFor centuries, people have predicted that machines would make workers obsolete and increase unemployment, although the causes of unemployment are usually thought to be due to social policy.[132][133][134]\nA recent example of human replacement involves Taiwanese technology company Foxconn who, in July 2011, announced a three-year plan to replace workers with more robots. At present the company uses ten thousand robots but will increase them to a million robots over a three-year period.[135]\nLawyers have speculated that an increased prevalence of robots in the workplace could lead to the need to improve redundancy laws.[136]\nKevin J. Delaney said \"Robots are taking human jobs. But Bill Gates believes that governments should tax companies' use of them, as a way to at least temporarily slow the spread of automation and to fund other types of employment.\"[137] The robot tax would also help pay a guaranteed living wage to the displaced workers.\nThe World Bank's World Development Report 2019 puts forth evidence showing that while automation displaces workers, technological innovation creates more new industries and jobs on balance.[138]\nAt present, there are two main types of robots, based on their use: general-purpose autonomous robots and dedicated robots.\nRobots can be classified by their specificity of purpose. A robot might be designed to perform one particular task extremely well, or a range of tasks less well. All robots by their nature can be re-programmed to behave differently, but some are limited by their physical form. For example, a factory robot arm can perform jobs such as cutting, welding, gluing, or acting as a fairground ride, while a pick-and-place robot can only populate printed circuit boards.\nGeneral-purpose autonomous robots can perform a variety of functions independently. General-purpose autonomous robots typically can navigate independently in known spaces, handle their own re-charging needs, interface with electronic doors and elevators and perform other basic tasks. Like computers, general-purpose robots can link with networks, software and accessories that increase their usefulness. They may recognize people or objects, talk, provide companionship, monitor environmental quality, respond to alarms, pick up supplies and perform other useful tasks. General-purpose robots may perform a variety of functions simultaneously or they may take on different roles at different times of day. Some such robots try to mimic human beings and may even resemble people in appearance; this type of robot is called a humanoid robot. Humanoid robots are still in a very limited stage, as no humanoid robot can, as of yet, actually navigate around a room that it has never been in.[139] Thus, humanoid robots are really quite limited, despite their intelligent behaviors in their well-known environments.\nOver the last three decades, automobile factories have become dominated by robots. A typical factory contains hundreds of industrial robots working on fully automated production lines, with one robot for every ten human workers. On an automated production line, a vehicle chassis on a conveyor is welded, glued, painted and finally assembled at a sequence of robot stations.\nIndustrial robots are also used extensively for palletizing and packaging of manufactured goods, for example for rapidly taking drink cartons from the end of a conveyor belt and placing them into boxes, or for loading and unloading machining centers.\nMass-produced printed circuit boards (PCBs) are almost exclusively manufactured by pick-and-place robots, typically with SCARA manipulators, which remove tiny electronic components from strips or trays, and place them on to PCBs with great accuracy.[140] Such robots can place hundreds of thousands of components per hour, far out-performing a human in speed, accuracy, and reliability.[141]\nMobile robots, following markers or wires in the floor, or using vision[86] or lasers, are used to transport goods around large facilities, such as warehouses, container ports, or hospitals.[142]\nLimited to tasks that could be accurately defined and had to be performed the same way every time. Very little feedback or intelligence was required, and the robots needed only the most basic exteroceptors (sensors). The limitations of these AGVs are that their paths are not easily altered and they cannot alter their paths if obstacles block them. If one AGV breaks down, it may stop the entire operation.\nDeveloped to deploy triangulation from beacons or bar code grids for scanning on the floor or ceiling. In most factories, triangulation systems tend to require moderate to high maintenance, such as daily cleaning of all beacons or bar codes. Also, if a tall pallet or large vehicle blocks beacons or a bar code is marred, AGVs may become lost. Often such AGVs are designed to be used in human-free environments.\nSuch as SmartLoader,[143] SpeciMinder,[144] ADAM,[145] Tug[146] Eskorta,[147] and MT 400 with Motivity[148] are designed for people-friendly workspaces. They navigate by recognizing natural features. 3D scanners or other means of sensing the environment in two or three dimensions help to eliminate cumulative errors in dead-reckoning calculations of the AGV's current position. Some AGVs can create maps of their environment using scanning lasers with simultaneous localization and mapping (SLAM) and use those maps to navigate in real time with other path planning and obstacle avoidance algorithms. They are able to operate in complex environments and perform non-repetitive and non-sequential tasks such as transporting photomasks in a semiconductor lab, specimens in hospitals and goods in warehouses. For dynamic areas, such as warehouses full of pallets, AGVs require additional strategies using three-dimensional sensors such as time-of-flight or stereovision cameras.\nThere are many jobs that humans would rather leave to robots. The job may be boring, such as domestic cleaning or sports field line marking, or dangerous, such as exploring inside a volcano.[149] Other jobs are physically inaccessible, such as exploring another planet,[150] cleaning the inside of a long pipe, or performing laparoscopic surgery.[151]\nAlmost every unmanned space probe ever launched was a robot.[152][153] Some were launched in the 1960s with very limited abilities, but their ability to fly and land (in the case of Luna 9) is an indication of their status as a robot. This includes the Voyager probes and the Galileo probes, among others.\nTeleoperated robots, or telerobots, are devices remotely operated from a distance by a human operator rather than following a predetermined sequence of movements, but which has semi-autonomous behaviour. They are used when a human cannot be present on site to perform a job because it is dangerous, far away, or inaccessible. The robot may be in another room or another country, or may be on a very different scale to the operator. For instance, a laparoscopic surgery robot allows the surgeon to work inside a human patient on a relatively small scale compared to open surgery, significantly shortening recovery time.[151] They can also be used to avoid exposing workers to the hazardous and tight spaces such as in duct cleaning. When disabling a bomb, the operator sends a small robot to disable it. Several authors have been using a device called the Longpen to sign books remotely.[154] Teleoperated robot aircraft, like the Predator Unmanned Aerial Vehicle, are increasingly being used by the military. These pilotless drones can search terrain and fire on targets.[155][156] Hundreds of robots such as iRobot's Packbot and the Foster-Miller TALON are being used in Iraq and Afghanistan by the U.S. military to defuse roadside bombs or improvised explosive devices (IEDs) in an activity known as explosive ordnance disposal (EOD).[157]\nRobots are used to automate picking fruit on orchards at a cost lower than that of human pickers.\nDomestic robots are simple robots dedicated to a single task work in home use. They are used in simple but often disliked jobs, such as vacuum cleaning, floor washing, and lawn mowing. An example of a domestic robot is a Roomba.\nMilitary robots include the SWORDS robot which is currently used in ground-based combat. It can use a variety of weapons and there is some discussion of giving it some degree of autonomy in battleground situations.[158][159][160]\nUnmanned combat air vehicles (UCAVs), which are an upgraded form of UAVs, can do a wide variety of missions, including combat. UCAVs are being designed such as the BAE Systems Mantis which would have the ability to fly themselves, to pick their own course and target, and to make most decisions on their own.[161] The BAE Taranis is a UCAV built by Great Britain which can fly across continents without a pilot and has new means to avoid detection.[162] Flight trials are expected to begin in 2011.[163]\nThe AAAI has studied this topic in depth[110] and its president has commissioned a study to look at this issue.[164]\nSome have suggested a need to build \"Friendly AI\", meaning that the advances which are already occurring with AI should also include an effort to make AI intrinsically friendly and humane.[165] Several such measures reportedly already exist, with robot-heavy countries such as Japan and South Korea[166] having begun to pass regulations requiring robots to be equipped with safety systems, and possibly sets of 'laws' akin to Asimov's Three Laws of Robotics.[167][168] An official report was issued in 2009 by the Japanese government's Robot Industry Policy Committee.[169] Chinese officials and researchers have issued a report suggesting a set of ethical rules, and a set of new legal guidelines referred to as \"Robot Legal Studies.\"[170] Some concern has been expressed over a possible occurrence of robots telling apparent falsehoods.[171]\nMining robots are designed to solve a number of problems currently facing the mining industry, including skills shortages, improving productivity from declining ore grades, and achieving environmental targets. Due to the hazardous nature of mining, in particular underground mining, the prevalence of autonomous, semi-autonomous, and tele-operated robots has greatly increased in recent times. A number of vehicle manufacturers provide autonomous trains, trucks and loaders that will load material, transport it on the mine site to its destination, and unload without requiring human intervention. One of the world's largest mining corporations, Rio Tinto, has recently expanded its autonomous truck fleet to the world's largest, consisting of 150 autonomous Komatsu trucks, operating in Western Australia.[172] Similarly, BHP has announced the expansion of its autonomous drill fleet to the world's largest, 21 autonomous Atlas Copco drills.[173]\nDrilling, longwall and rockbreaking machines are now also available as autonomous robots.[174] The Atlas Copco Rig Control System can autonomously execute a drilling plan on a drilling rig, moving the rig into position using GPS, set up the drill rig and drill down to specified depths.[175] Similarly, the Transmin Rocklogic system can automatically plan a path to position a rockbreaker at a selected destination.[176] These systems greatly enhance the safety and efficiency of mining operations.\nRobots in healthcare have two main functions. Those which assist an individual, such as a sufferer of a disease like Multiple Sclerosis, and those which aid in the overall systems such as pharmacies and hospitals.\nRobots used in home automation have developed over time from simple basic robotic assistants, such as the Handy 1,[177] through to semi-autonomous robots, such as FRIEND which can assist the elderly and disabled with common tasks.\nThe population is aging in many countries, especially Japan, meaning that there are increasing numbers of elderly people to care for, but relatively fewer young people to care for them.[178][179] Humans make the best carers, but where they are unavailable, robots are gradually being introduced.[180]\nFRIEND is a semi-autonomous robot designed to support disabled and elderly people in their daily life activities, like preparing and serving a meal. FRIEND make it possible for patients who are paraplegic, have muscle diseases or serious paralysis (due to strokes etc.), to perform tasks without help from other people like therapists or nursing staff.\nScript Pro manufactures a robot designed to help pharmacies fill prescriptions that consist of oral solids or medications in pill form.[181][better source needed] The pharmacist or pharmacy technician enters the prescription information into its information system. The system, upon determining whether or not the drug is in the robot, will send the information to the robot for filling. The robot has 3 different size vials to fill determined by the size of the pill. The robot technician, user, or pharmacist determines the needed size of the vial based on the tablet when the robot is stocked. Once the vial is filled it is brought up to a conveyor belt that delivers it to a holder that spins the vial and attaches the patient label. Afterwards it is set on another conveyor that delivers the patient's medication vial to a slot labeled with the patient's name on an LED read out. The pharmacist or technician then checks the contents of the vial to ensure it's the correct drug for the correct patient and then seals the vials and sends it out front to be picked up.\nMcKesson's Robot RX is another healthcare robotics product that helps pharmacies dispense thousands of medications daily with little or no errors.[182] The robot can be ten feet wide and thirty feet long and can hold hundreds of different kinds of medications and thousands of doses. The pharmacy saves many resources like staff members that are otherwise unavailable in a resource scarce industry. It uses an electromechanical head coupled with a pneumatic system to capture each dose and deliver it to either its stocked or dispensed location. The head moves along a single axis while it rotates 180 degrees to pull the medications. During this process it uses barcode technology to verify it's pulling the correct drug. It then delivers the drug to a patient specific bin on a conveyor belt. Once the bin is filled with all of the drugs that a particular patient needs and that the robot stocks, the bin is then released and returned out on the conveyor belt to a technician waiting to load it into a cart for delivery to the floor.\nWhile most robots today are installed in factories or homes, performing labour or life saving jobs, many new types of robot are being developed in laboratories around the world. Much of the research in robotics focuses not on specific industrial tasks, but on investigations into new types of robot, alternative ways to think about or design robots, and new ways to manufacture them. It is expected that these new types of robot will be able to solve real world problems when they are finally realized.[citation needed]\nOne approach to designing robots is to base them on animals. BionicKangaroo was designed and engineered by studying and applying the physiology and methods of locomotion of a kangaroo.\nNanorobotics is the emerging technology field of creating machines or robots whose components are at or close to the microscopic scale of a nanometer (10−9 meters). Also known as \"nanobots\" or \"nanites\", they would be constructed from molecular machines. So far, researchers have mostly produced only parts of these complex systems, such as bearings, sensors, and synthetic molecular motors, but functioning robots have also been made such as the entrants to the Nanobot Robocup contest.[183] Researchers also hope to be able to create entire robots as small as viruses or bacteria, which could perform tasks on a tiny scale. Possible applications include micro surgery (on the level of individual cells), utility fog,[184] manufacturing, weaponry and cleaning.[185] Some people have suggested that if there were nanobots which could reproduce, the earth would turn into \"grey goo\", while others argue that this hypothetical outcome is nonsense.[186][187]\nA few researchers have investigated the possibility of creating robots which can alter their physical form to suit a particular task,[188] like the fictional T-1000. Real robots are nowhere near that sophisticated however, and mostly consist of a small number of cube shaped units, which can move relative to their neighbours. Algorithms have been designed in case any such robots become a reality.[189]\nIn July 2020 scientists reported the development of a mobile robot chemist and demonstrate that it can assist in experimental searches. According to the scientists their strategy was automating the researcher rather than the instruments – freeing up time for the human researchers to think creatively – and could identify photocatalyst mixtures for hydrogen production from water that were six times more active than initial formulations. The modular robot can operate laboratory instruments, work nearly around the clock, and autonomously make decisions on his next actions depending on experimental results.[190][191]\nRobots with silicone bodies and flexible actuators (air muscles, electroactive polymers, and ferrofluids) look and feel different from robots with rigid skeletons, and can have different behaviors.[192] Soft, flexible (and sometimes even squishy) robots are often designed to mimic the biomechanics of animals and other things found in nature, which is leading to new applications in medicine, care giving, search and rescue, food handling and manufacturing, and scientific exploration.[193][194]\nInspired by colonies of insects such as ants and bees, researchers are modeling the behavior of swarms of thousands of tiny robots which together perform a useful task, such as finding something hidden, cleaning, or spying. Each robot is quite simple, but the emergent behavior of the swarm is more complex. The whole set of robots can be considered as one single distributed system, in the same way an ant colony can be considered a superorganism, exhibiting swarm intelligence. The largest swarms so far created include the iRobot swarm, the SRI/MobileRobots CentiBots project[195] and the Open-source Micro-robotic Project swarm, which are being used to research collective behaviors.[196][197] Swarms are also more resistant to failure. Whereas one large robot may fail and ruin a mission, a swarm can continue even if several robots fail. This could make them attractive for space exploration missions, where failure is normally extremely costly.[198]\nRobotics also has application in the design of virtual reality interfaces. Specialized robots are in widespread use in the haptic research community. These robots, called \"haptic interfaces\", allow touch-enabled user interaction with real and virtual environments. Robotic forces allow simulating the mechanical properties of \"virtual\" objects, which users can experience through their sense of touch.[199]\nRobots are used by contemporary artists to create works that include mechanical automation. There are many branches of robotic art, one of which is robotic installation art, a type of installation art that is programmed to respond to viewer interactions, by means of computers, sensors and actuators. The future behavior of such installations can therefore be altered by input from either the artist or the participant, which differentiates these artworks from other types of kinetic art.\nLe Grand Palais in Paris organized an exhibition \"Artists \u0026 Robots\", featuring artworks created by more than forty artists with the help of robots in 2018.[200]\nRobotic characters, androids (artificial men/women) or gynoids (artificial women), and cyborgs (also \"bionic men/women\", or humans with significant mechanical enhancements) have become a staple of science fiction.\nThe first reference in Western literature to mechanical servants appears in Homer's Iliad. In Book XVIII, Hephaestus, god of fire, creates new armor for the hero Achilles, assisted by robots.[201] According to the Rieu translation, \"Golden maidservants hastened to help their master. They looked like real women and could not only speak and use their limbs but were endowed with intelligence and trained in handwork by the immortal gods.\" The words \"robot\" or \"android\" are not used to describe them, but they are nevertheless mechanical devices human in appearance. \"The first use of the word Robot was in Karel Čapek's play R.U.R. (Rossum's Universal Robots) (written in 1920)\". Writer Karel Čapek was born in Czechoslovakia (Czech Republic).\nPossibly the most prolific author of the twentieth century was Isaac Asimov (1920–1992)[202] who published over five-hundred books.[203] Asimov is probably best remembered for his science-fiction stories and especially those about robots, where he placed robots and their interaction with society at the center of many of his works.[204][205] Asimov carefully considered the problem of the ideal set of instructions robots might be given to lower the risk to humans, and arrived at his Three Laws of Robotics: a robot may not injure a human being or, through inaction, allow a human being to come to harm; a robot must obey orders given it by human beings, except where such orders would conflict with the First Law; and a robot must protect its own existence as long as such protection does not conflict with the First or Second Law.[206] These were introduced in his 1942 short story \"Runaround\", although foreshadowed in a few earlier stories. Later, Asimov added the Zeroth Law: \"A robot may not harm humanity, or, by inaction, allow humanity to come to harm\"; the rest of the laws are modified sequentially to acknowledge this.\nAccording to the Oxford English Dictionary, the first passage in Asimov's short story \"Liar!\" (1941) that mentions the First Law is the earliest recorded use of the word robotics. Asimov was not initially aware of this; he assumed the word already existed by analogy with mechanics, hydraulics, and other similar terms denoting branches of applied knowledge.[207]\nRobots are used in a number of competitive events. Robot combat competitions have been popularized by television shows such as Robot Wars and BattleBots, featuring mostly remotely controlled 'robots' that compete against each other directly using various weaponry, there are also amateur robot combat leagues active globally outside of the televised events. Micromouse events, in which autonomous robots compete to solve mazes or other obstacle courses are also held internationally.\nRobot competitions are also often used within educational settings to introduce the concept of robotics to children such as the FIRST Robotics Competition in the US.\nRobots appear in many films. Most of the robots in cinema are fictional. Two of the most famous are R2-D2 and C-3PO from the Star Wars franchise.\nThe concept of humanoid sex robots has drawn public attention and elicited debate regarding their supposed benefits and potential effects on society. Opponents argue that the introduction of such devices would be socially harmful, and demeaning to women and children,[208] while proponents cite their potential therapeutical benefits, particularly in aiding people with dementia or depression.[209]\nFears and concerns about robots have been repeatedly expressed in a wide range of books and films. A common theme is the development of a master race of conscious and highly intelligent robots, motivated to take over or destroy the human race. Frankenstein (1818), often called the first science fiction novel, has become synonymous with the theme of a robot or android advancing beyond its creator.\nOther works with similar themes include The Mechanical Man, The Terminator, Runaway, RoboCop, the Replicators in Stargate, the Cylons in Battlestar Galactica, the Cybermen and Daleks in Doctor Who, The Matrix, Enthiran and I, Robot. Some fictional robots are programmed to kill and destroy; others gain superhuman intelligence and abilities by upgrading their own software and hardware. Examples of popular media where the robot becomes evil are 2001: A Space Odyssey, Red Planet and Enthiran.\nThe 2017 game Horizon Zero Dawn explores themes of robotics in warfare, robot ethics, and the AI control problem, as well as the positive or negative impact such technologies could have on the environment.\nAnother common theme is the reaction, sometimes called the \"uncanny valley\", of unease and even revulsion at the sight of robots that mimic humans too closely.[109]\nMore recently, fictional representations of artificially intelligent robots in films such as A.I. Artificial Intelligence and Ex Machina and the 2016 TV adaptation of Westworld have engaged audience sympathy for the robots themselves."}
{"url":"https://en.wikipedia.org/wiki/Reinforcement_learning","text":"Reinforcement learning (RL) is an interdisciplinary area of machine learning and optimal control concerned with how an intelligent agent ought to take actions in a dynamic environment in order to maximize the cumulative reward. Reinforcement learning is one of three basic machine learning paradigms, alongside supervised learning and unsupervised learning.\nReinforcement learning differs from supervised learning in not needing labelled input/output pairs to be presented, and in not needing sub-optimal actions to be explicitly corrected. Instead the focus is on finding a balance between exploration (of uncharted territory) and exploitation (of current knowledge) with the goal of maximizing the long term reward, whose feedback might be incomplete or delayed.[1]\n\nThe environment is typically stated in the form of a Markov decision process (MDP), because many reinforcement learning algorithms for this context use dynamic programming techniques.[2] The main difference between the classical dynamic programming methods and reinforcement learning algorithms is that the latter do not assume knowledge of an exact mathematical model of the Markov decision process and they target large Markov decision processes where exact methods become infeasible.[3] .mw-parser-output .toclimit-2 .toclevel-1 ul,.mw-parser-output .toclimit-3 .toclevel-2 ul,.mw-parser-output .toclimit-4 .toclevel-3 ul,.mw-parser-output .toclimit-5 .toclevel-4 ul,.mw-parser-output .toclimit-6 .toclevel-5 ul,.mw-parser-output .toclimit-7 .toclevel-6 ul{display:none}Due to its generality, reinforcement learning is studied in many disciplines, such as game theory, control theory, operations research, information theory, simulation-based optimization, multi-agent systems, swarm intelligence, and statistics. In the operations research and control literature, reinforcement learning is called approximate dynamic programming, or neuro-dynamic programming. The problems of interest in reinforcement learning have also been studied in the theory of optimal control, which is concerned mostly with the existence and characterization of optimal solutions, and algorithms for their exact computation, and less with learning or approximation, particularly in the absence of a mathematical model of the environment.\nBasic reinforcement learning is modeled as a Markov decision process:\nThe purpose of reinforcement learning is for the agent to learn an optimal, or nearly-optimal, policy that maximizes the \"reward function\" or other user-provided reinforcement signal that accumulates from the immediate rewards. This is similar to processes that appear to occur in animal psychology. For example, biological brains are hardwired to interpret signals such as pain and hunger as negative reinforcements, and interpret pleasure and food intake as positive reinforcements. In some circumstances, animals can learn to engage in behaviors that optimize these rewards. This suggests that animals are capable of reinforcement learning.[4][5]\nA basic reinforcement learning agent AI interacts with its environment in discrete time steps. At each time t, the agent receives the current state \n  \n    \n      \n        \n          S\n          \n            t\n          \n        \n      \n    \n    {\\displaystyle S_{t}}\n  \n and reward \n  \n    \n      \n        \n          R\n          \n            t\n          \n        \n      \n    \n    {\\displaystyle R_{t}}\n  \n. It then chooses an action \n  \n    \n      \n        \n          A\n          \n            t\n          \n        \n      \n    \n    {\\displaystyle A_{t}}\n  \n from the set of available actions, which is subsequently sent to the environment. The environment moves to a new state \n  \n    \n      \n        \n          S\n          \n            t\n            +\n            1\n          \n        \n      \n    \n    {\\displaystyle S_{t+1}}\n  \n and the reward \n  \n    \n      \n        \n          R\n          \n            t\n            +\n            1\n          \n        \n      \n    \n    {\\displaystyle R_{t+1}}\n  \n associated with the transition \n  \n    \n      \n        (\n        \n          S\n          \n            t\n          \n        \n        ,\n        \n          A\n          \n            t\n          \n        \n        ,\n        \n          S\n          \n            t\n            +\n            1\n          \n        \n        )\n      \n    \n    {\\displaystyle (S_{t},A_{t},S_{t+1})}\n  \n is determined. The goal of a reinforcement learning agent is to learn a policy: \n  \n    \n      \n        π\n        :\n        \n          \n            S\n          \n        \n        ×\n        \n          \n            A\n          \n        \n        →\n        [\n        0\n        ,\n        1\n        ]\n      \n    \n    {\\displaystyle \\pi :{\\mathcal {S}}\\times {\\mathcal {A}}\\rightarrow [0,1]}\n  \n, \n  \n    \n      \n        π\n        (\n        s\n        ,\n        a\n        )\n        =\n        Pr\n        (\n        \n          A\n          \n            t\n          \n        \n        =\n        a\n        ∣\n        \n          S\n          \n            t\n          \n        \n        =\n        s\n        )\n      \n    \n    {\\displaystyle \\pi (s,a)=\\Pr(A_{t}=a\\mid S_{t}=s)}\n  \n that maximizes the expected cumulative reward.\nFormulating the problem as an Markov decision process assumes the agent directly observes the current environmental state; in this case the problem is said to have full observability. If the agent only has access to a subset of states, or if the observed states are corrupted by noise, the agent is said to have partial observability, and formally the problem must be formulated as a Partially observable Markov decision process. In both cases, the set of actions available to the agent can be restricted. For example, the state of an account balance could be restricted to be positive; if the current value of the state is 3 and the state transition attempts to reduce the value by 4, the transition will not be allowed.\nWhen the agent's performance is compared to that of an agent that acts optimally, the difference in performance gives rise to the notion of regret. In order to act near optimally, the agent must reason about the long-term consequences of its actions (i.e., maximize future income), although the immediate reward associated with this might be negative.\nThus, reinforcement learning is particularly well-suited to problems that include a long-term versus short-term reward trade-off. It has been applied successfully to various problems, including energy storage operation,[6] robot control,[7] photovoltaic generators dispatch,[8] backgammon, checkers,[9] Go (AlphaGo), and autonomous driving systems.[10]\nTwo elements make reinforcement learning powerful: the use of samples to optimize performance and the use of function approximation to deal with large environments. Thanks to these two key components, reinforcement learning can be used in large environments in the following situations:\nThe first two of these problems could be considered planning problems (since some form of model is available), while the last one could be considered to be a genuine learning problem. However, reinforcement learning converts both planning problems to machine learning problems.\nThe exploration vs. exploitation trade-off has been most thoroughly studied through the multi-armed bandit problem and for finite state space Markov decision processes in Burnetas and Katehakis (1997).[12]\nReinforcement learning requires clever exploration mechanisms; randomly selecting actions, without reference to an estimated probability distribution, shows poor performance. The case of (small) finite Markov decision processes is relatively well understood. However, due to the lack of algorithms that scale well with the number of states (or scale to problems with infinite state spaces), simple exploration methods are the most practical.\nOne such method is \n  \n    \n      \n        ε\n      \n    \n    {\\displaystyle \\varepsilon }\n  \n-greedy, where \n  \n    \n      \n        0\n        \u003c\n        ε\n        \u003c\n        1\n      \n    \n    {\\displaystyle 0\u003c\\varepsilon \u003c1}\n  \n is a parameter controlling the amount of exploration vs. exploitation.  With probability \n  \n    \n      \n        1\n        −\n        ε\n      \n    \n    {\\displaystyle 1-\\varepsilon }\n  \n, exploitation is chosen, and the agent chooses the action that it believes has the best long-term effect (ties between actions are broken uniformly at random). Alternatively, with probability \n  \n    \n      \n        ε\n      \n    \n    {\\displaystyle \\varepsilon }\n  \n, exploration is chosen, and the action is chosen uniformly at random. \n  \n    \n      \n        ε\n      \n    \n    {\\displaystyle \\varepsilon }\n  \n is usually a fixed parameter but can be adjusted either according to a schedule (making the agent explore progressively less), or adaptively based on heuristics.[13]\nEven if the issue of exploration is disregarded and even if the state was observable (assumed hereafter), the problem remains to use past experience to find out which actions lead to higher cumulative rewards.\nThe agent's action selection is modeled as a map called policy:\nThe policy map gives the probability of taking action \n  \n    \n      \n        a\n      \n    \n    {\\displaystyle a}\n  \n when in state \n  \n    \n      \n        s\n      \n    \n    {\\displaystyle s}\n  \n.[14]: 61  There are also deterministic policies.\nThe state-value function \n  \n    \n      \n        \n          V\n          \n            π\n          \n        \n        (\n        s\n        )\n      \n    \n    {\\displaystyle V_{\\pi }(s)}\n  \n is defined as, expected discounted return starting with state \n  \n    \n      \n        s\n      \n    \n    {\\displaystyle s}\n  \n, i.e. \n  \n    \n      \n        \n          S\n          \n            0\n          \n        \n        =\n        s\n      \n    \n    {\\displaystyle S_{0}=s}\n  \n, and successively following policy \n  \n    \n      \n        π\n      \n    \n    {\\displaystyle \\pi }\n  \n. Hence, roughly speaking, the value function estimates \"how good\" it is to be in a given state.[14]: 60 \nwhere the random variable \n  \n    \n      \n        G\n      \n    \n    {\\displaystyle G}\n  \n denotes the discounted return, and is defined as the sum of future discounted rewards:\nwhere \n  \n    \n      \n        \n          R\n          \n            t\n            +\n            1\n          \n        \n      \n    \n    {\\displaystyle R_{t+1}}\n  \n is the reward for transitioning from state \n  \n    \n      \n        \n          S\n          \n            t\n          \n        \n      \n    \n    {\\displaystyle S_{t}}\n  \n to \n  \n    \n      \n        \n          S\n          \n            t\n            +\n            1\n          \n        \n      \n    \n    {\\displaystyle S_{t+1}}\n  \n, \n  \n    \n      \n        0\n        ≤\n        γ\n        \u003c\n        1\n      \n    \n    {\\displaystyle 0\\leq \\gamma \u003c1}\n  \n is the discount rate. \n  \n    \n      \n        γ\n      \n    \n    {\\displaystyle \\gamma }\n  \n is less than 1, so rewards in the distant future are weighted less than rewards in the immediate future.\nThe algorithm must find a policy with maximum expected discounted return. From the theory of Markov decision processes it is known that, without loss of generality, the search can be restricted to the set of so-called stationary policies. A policy is stationary if the action-distribution returned by it depends only on the last state visited (from the observation agent's history). The search can be further restricted to deterministic stationary policies. A deterministic stationary policy deterministically selects actions based on the current state. Since any such policy can be identified with a mapping from the set of states to the set of actions, these policies can be identified with such mappings with no loss of generality.\nThe brute force approach entails two steps:\nOne problem with this is that the number of policies can be large, or even infinite. Another is that the variance of the returns may be large, which requires many samples to accurately estimate the discounted return of each policy.\nThese problems can be ameliorated if we assume some structure and allow samples generated from one policy to influence the estimates made for others. The two main approaches for achieving this are value function estimation and direct policy search.\nValue function approaches attempt to find a policy that maximizes the discounted return by maintaining a set of estimates of expected discounted returns \n  \n    \n      \n        \n          \n            E\n          \n        \n        ⁡\n        [\n        G\n        ]\n      \n    \n    {\\displaystyle \\operatorname {\\mathbb {E} } [G]}\n  \n for some policy (usually either the \"current\" [on-policy] or the optimal [off-policy] one).\nThese methods rely on the theory of Markov decision processes, where optimality is defined in a sense stronger than the one above: A policy is optimal if it achieves the best-expected discounted return from any initial state (i.e., initial distributions play no role in this definition). Again, an optimal policy can always be found among stationary policies.\nTo define optimality in a formal manner, define the state-value of a policy \n  \n    \n      \n        π\n      \n    \n    {\\displaystyle \\pi }\n  \n by\nwhere \n  \n    \n      \n        G\n      \n    \n    {\\displaystyle G}\n  \n stands for the discounted return associated with following \n  \n    \n      \n        π\n      \n    \n    {\\displaystyle \\pi }\n  \n from the initial state \n  \n    \n      \n        s\n      \n    \n    {\\displaystyle s}\n  \n. Defining \n  \n    \n      \n        \n          V\n          \n            ∗\n          \n        \n        (\n        s\n        )\n      \n    \n    {\\displaystyle V^{*}(s)}\n  \n as the maximum possible state-value of \n  \n    \n      \n        \n          V\n          \n            π\n          \n        \n        (\n        s\n        )\n      \n    \n    {\\displaystyle V^{\\pi }(s)}\n  \n, where \n  \n    \n      \n        π\n      \n    \n    {\\displaystyle \\pi }\n  \n is allowed to change,\nA policy that achieves these optimal state-values in each state is called optimal. Clearly, a policy that is optimal in this strong sense is also optimal in the sense that it maximizes the expected discounted return \n  \n    \n      \n        \n          ρ\n          \n            π\n          \n        \n      \n    \n    {\\displaystyle \\rho ^{\\pi }}\n  \n, since \n  \n    \n      \n        \n          ρ\n          \n            π\n          \n        \n        =\n        \n          \n            E\n          \n        \n        ⁡\n        [\n        \n          V\n          \n            π\n          \n        \n        (\n        s\n        )\n        ]\n      \n    \n    {\\displaystyle \\rho ^{\\pi }=\\operatorname {\\mathbb {E} } [V^{\\pi }(s)]}\n  \n, where \n  \n    \n      \n        s\n      \n    \n    {\\displaystyle s}\n  \n is a state randomly sampled from the distribution \n  \n    \n      \n        μ\n      \n    \n    {\\displaystyle \\mu }\n  \n of initial states (so \n  \n    \n      \n        μ\n        (\n        s\n        )\n        =\n        Pr\n        (\n        \n          S\n          \n            0\n          \n        \n        =\n        s\n        )\n      \n    \n    {\\displaystyle \\mu (s)=\\Pr(S_{0}=s)}\n  \n).\nAlthough state-values suffice to define optimality, it is useful to define action-values. Given a state \n  \n    \n      \n        s\n      \n    \n    {\\displaystyle s}\n  \n, an action \n  \n    \n      \n        a\n      \n    \n    {\\displaystyle a}\n  \n and a policy \n  \n    \n      \n        π\n      \n    \n    {\\displaystyle \\pi }\n  \n, the action-value of the pair \n  \n    \n      \n        (\n        s\n        ,\n        a\n        )\n      \n    \n    {\\displaystyle (s,a)}\n  \n under \n  \n    \n      \n        π\n      \n    \n    {\\displaystyle \\pi }\n  \n is defined by\nwhere \n  \n    \n      \n        G\n      \n    \n    {\\displaystyle G}\n  \n now stands for the random discounted return associated with first taking action \n  \n    \n      \n        a\n      \n    \n    {\\displaystyle a}\n  \n in state \n  \n    \n      \n        s\n      \n    \n    {\\displaystyle s}\n  \n and following \n  \n    \n      \n        π\n      \n    \n    {\\displaystyle \\pi }\n  \n, thereafter.\nThe theory of Markov decision processes states that if \n  \n    \n      \n        \n          π\n          \n            ∗\n          \n        \n      \n    \n    {\\displaystyle \\pi ^{*}}\n  \n is an optimal policy, we act optimally (take the optimal action) by choosing the action from \n  \n    \n      \n        \n          Q\n          \n            \n              π\n              \n                ∗\n              \n            \n          \n        \n        (\n        s\n        ,\n        ⋅\n        )\n      \n    \n    {\\displaystyle Q^{\\pi ^{*}}(s,\\cdot )}\n  \n with the highest action-value at each state, \n  \n    \n      \n        s\n      \n    \n    {\\displaystyle s}\n  \n. The action-value function of such an optimal policy (\n  \n    \n      \n        \n          Q\n          \n            \n              π\n              \n                ∗\n              \n            \n          \n        \n      \n    \n    {\\displaystyle Q^{\\pi ^{*}}}\n  \n) is called the optimal action-value function and is commonly denoted by \n  \n    \n      \n        \n          Q\n          \n            ∗\n          \n        \n      \n    \n    {\\displaystyle Q^{*}}\n  \n. In summary, the knowledge of the optimal action-value function alone suffices to know how to act optimally.\nAssuming full knowledge of the Markov decision process, the two basic approaches to compute the optimal action-value function are value iteration and policy iteration. Both algorithms compute a sequence of functions \n  \n    \n      \n        \n          Q\n          \n            k\n          \n        \n      \n    \n    {\\displaystyle Q_{k}}\n  \n (\n  \n    \n      \n        k\n        =\n        0\n        ,\n        1\n        ,\n        2\n        ,\n        …\n      \n    \n    {\\displaystyle k=0,1,2,\\ldots }\n  \n) that converge to \n  \n    \n      \n        \n          Q\n          \n            ∗\n          \n        \n      \n    \n    {\\displaystyle Q^{*}}\n  \n. Computing these functions involves computing expectations over the whole state-space, which is impractical for all but the smallest (finite) Markov decision processes. In reinforcement learning methods, expectations are approximated by averaging over samples and using function approximation techniques to cope with the need to represent value functions over large state-action spaces.\nMonte Carlo methods can be used in an algorithm that mimics policy iteration. Policy iteration consists of two steps: policy evaluation and policy improvement.\nMonte Carlo is used in the policy evaluation step. In this step, given a stationary, deterministic policy \n  \n    \n      \n        π\n      \n    \n    {\\displaystyle \\pi }\n  \n, the goal is to compute the function values \n  \n    \n      \n        \n          Q\n          \n            π\n          \n        \n        (\n        s\n        ,\n        a\n        )\n      \n    \n    {\\displaystyle Q^{\\pi }(s,a)}\n  \n (or a good approximation to them) for all state-action pairs \n  \n    \n      \n        (\n        s\n        ,\n        a\n        )\n      \n    \n    {\\displaystyle (s,a)}\n  \n. Assume (for simplicity) that the Markov decision process is finite, that sufficient memory is available to accommodate the action-values and that the problem is episodic and after each episode a new one starts from some random initial state. Then, the estimate of the value of a given state-action pair \n  \n    \n      \n        (\n        s\n        ,\n        a\n        )\n      \n    \n    {\\displaystyle (s,a)}\n  \n can be computed by averaging the sampled returns that originated from \n  \n    \n      \n        (\n        s\n        ,\n        a\n        )\n      \n    \n    {\\displaystyle (s,a)}\n  \n over time.  Given sufficient time, this procedure can thus construct a precise estimate \n  \n    \n      \n        Q\n      \n    \n    {\\displaystyle Q}\n  \n of the action-value function \n  \n    \n      \n        \n          Q\n          \n            π\n          \n        \n      \n    \n    {\\displaystyle Q^{\\pi }}\n  \n. This finishes the description of the policy evaluation step.\nIn the policy improvement step, the next policy is obtained by computing a greedy policy with respect to \n  \n    \n      \n        Q\n      \n    \n    {\\displaystyle Q}\n  \n: Given a state \n  \n    \n      \n        s\n      \n    \n    {\\displaystyle s}\n  \n, this new policy returns an action that maximizes \n  \n    \n      \n        Q\n        (\n        s\n        ,\n        ⋅\n        )\n      \n    \n    {\\displaystyle Q(s,\\cdot )}\n  \n. In practice lazy evaluation can defer the computation of the maximizing actions to when they are needed.\nProblems with this procedure include:\nThe first problem is corrected by allowing the procedure to change the policy (at some or all states) before the values settle. This too may be problematic as it might prevent convergence. Most current algorithms do this, giving rise to the class of generalized policy iteration algorithms. Many actor-critic methods belong to this category.\nThe second issue can be corrected by allowing trajectories to contribute to any state-action pair in them. This may also help to some extent with the third problem, although a better solution when returns have high variance is Sutton's temporal difference (TD) methods that are based on the recursive Bellman equation.[15][16] The computation in TD methods can be incremental (when after each transition the memory is changed and the transition is thrown away), or batch (when the transitions are batched and the estimates are computed once based on the batch). Batch methods, such as the least-squares temporal difference method,[17] may use the information in the samples better, while incremental methods are the only choice when batch methods are infeasible due to their high computational or memory complexity. Some methods try to combine the two approaches. Methods based on temporal differences also overcome the fourth issue.\nAnother problem specific to TD comes from their reliance on the recursive Bellman equation. Most TD methods have a so-called \n  \n    \n      \n        λ\n      \n    \n    {\\displaystyle \\lambda }\n  \n parameter \n  \n    \n      \n        (\n        0\n        ≤\n        λ\n        ≤\n        1\n        )\n      \n    \n    {\\displaystyle (0\\leq \\lambda \\leq 1)}\n  \n that can continuously interpolate between Monte Carlo methods that do not rely on the Bellman equations and the basic TD methods that rely entirely on the Bellman equations. This can be effective in palliating this issue.\nIn order to address the fifth issue, function approximation methods are used. Linear function approximation starts with a mapping \n  \n    \n      \n        ϕ\n      \n    \n    {\\displaystyle \\phi }\n  \n that assigns a finite-dimensional vector to each state-action pair. Then, the action values of a state-action pair \n  \n    \n      \n        (\n        s\n        ,\n        a\n        )\n      \n    \n    {\\displaystyle (s,a)}\n  \n are obtained by linearly combining the components of \n  \n    \n      \n        ϕ\n        (\n        s\n        ,\n        a\n        )\n      \n    \n    {\\displaystyle \\phi (s,a)}\n  \n with some weights \n  \n    \n      \n        θ\n      \n    \n    {\\displaystyle \\theta }\n  \n:\nThe algorithms then adjust the weights, instead of adjusting the values associated with the individual state-action pairs. Methods based on ideas from nonparametric statistics (which can be seen to construct their own features) have been explored.\nValue iteration can also be used as a starting point, giving rise to the Q-learning algorithm and its many variants.[18] Including Deep Q-learning methods when a neural network is used to represent Q, with various applications in stochastic search problems.[19]\nThe problem with using action-values is that they may need highly precise estimates of the competing action values that can be hard to obtain when the returns are noisy, though this problem is mitigated to some extent by temporal difference methods. Using the so-called compatible function approximation method compromises generality and efficiency.\nAn alternative method is to search directly in (some subset of) the policy space, in which case the problem becomes a case of stochastic optimization. The two approaches available are gradient-based and gradient-free methods.\nGradient-based methods (policy gradient methods) start with a mapping from a finite-dimensional (parameter) space to the space of policies: given the parameter vector \n  \n    \n      \n        θ\n      \n    \n    {\\displaystyle \\theta }\n  \n, let \n  \n    \n      \n        \n          π\n          \n            θ\n          \n        \n      \n    \n    {\\displaystyle \\pi _{\\theta }}\n  \n denote the policy associated to \n  \n    \n      \n        θ\n      \n    \n    {\\displaystyle \\theta }\n  \n. Defining the performance function by \n  \n    \n      \n        ρ\n        (\n        θ\n        )\n        =\n        \n          ρ\n          \n            \n              π\n              \n                θ\n              \n            \n          \n        \n      \n    \n    {\\displaystyle \\rho (\\theta )=\\rho ^{\\pi _{\\theta }}}\n  \n under mild conditions this function will be differentiable as a function of the parameter vector \n  \n    \n      \n        θ\n      \n    \n    {\\displaystyle \\theta }\n  \n. If the gradient of \n  \n    \n      \n        ρ\n      \n    \n    {\\displaystyle \\rho }\n  \n was known, one could use gradient ascent. Since an analytic expression for the gradient is not available, only a noisy estimate is available. Such an estimate can be constructed in many ways, giving rise to algorithms such as Williams' REINFORCE method[20] (which is known as the likelihood ratio method in the simulation-based optimization literature).[21]\nA large class of methods avoids relying on gradient information. These include simulated annealing, cross-entropy search or methods of evolutionary computation. Many gradient-free methods can achieve (in theory and in the limit) a global optimum.\nPolicy search methods may converge slowly given noisy data. For example, this happens in episodic problems when the trajectories are long and the variance of the returns is large. Value-function based methods that rely on temporal differences might help in this case. In recent years, actor–critic methods have been proposed and performed well on various problems.[22]\nPolicy search methods have been used in the robotics context.[23] Many policy search methods may get stuck in local optima (as they are based on local search).\nFinally, all of the above methods can be combined with algorithms that first learn a model of the Markov Decision Process, the probability of each next state given an action taken from an existing state. For instance, the Dyna algorithm[24] learns a model from experience, and uses that to provide more modelled transitions for a value function, in addition to the real transitions.  Such methods can sometimes be extended to use of non-parametric models, such as when the transitions are simply stored and 'replayed'[25] to the learning algorithm.\nModel-based methods can be more computationally intensive than model-free approaches, and their utility can be limited by the extent to which the Markov Decision Process can be learnt.[26]\nThere are other ways to use models than to update a value function.[27] For instance, in model predictive control the model is used to update the behavior directly.\nBoth the asymptotic and finite-sample behaviors of most algorithms are well understood. Algorithms with provably good online performance (addressing the exploration issue) are known.\nEfficient exploration of Markov decision processes is given in  Burnetas and Katehakis (1997).[12] Finite-time performance bounds have also appeared for many algorithms, but these bounds are expected to be rather loose and thus more work is needed to better understand the relative advantages and limitations.\nFor incremental algorithms, asymptotic convergence issues have been settled[clarification needed]. Temporal-difference-based algorithms converge under a wider set of conditions than was previously possible (for example, when used with arbitrary, smooth function approximation).\nResearch topics include:\nAssociative reinforcement learning tasks combine facets of stochastic learning automata tasks and supervised learning pattern classification tasks. In associative reinforcement learning tasks, the learning system interacts in a closed loop with its environment.[44]\nThis approach extends reinforcement learning by using a deep neural network and without explicitly designing the state space.[45] The work on learning ATARI games by Google DeepMind increased attention to deep reinforcement learning or end-to-end reinforcement learning.[46]\nAdversarial deep reinforcement learning is an active area of research in reinforcement learning focusing on vulnerabilities of learned policies. In this research area some studies initially showed that reinforcement learning policies are susceptible to imperceptible adversarial manipulations.[47][48][49] While some methods have been proposed to overcome these susceptibilities, in the most recent studies it has been shown that these proposed solutions are far from providing an accurate representation of current vulnerabilities of deep reinforcement learning policies.[50]\nBy introducing fuzzy inference in reinforcement learning,[51] approximating the state-action value function with fuzzy rules in continuous space becomes possible. The IF - THEN form of fuzzy rules make this approach suitable for expressing the results in a form close to natural language. Extending FRL with Fuzzy Rule Interpolation [52] allows the use of reduced size sparse fuzzy rule-bases to emphasize cardinal rules (most important state-action values).\nIn inverse reinforcement learning (IRL), no reward function is given. Instead, the reward function is inferred given an observed behavior from an expert. The idea is to mimic observed behavior, which is often optimal or close to optimal.[53] One popular IRL paradigm is named maximum entropy inverse reinforcement learning (MaxEnt IRL). [54] MaxEnt IRL estimates the parameters of a linear model of the reward function by maximizing the entropy of the probability distribution of observed trajectories subject to constraints related to matching expected feature counts. Recently it has been shown that MaxEnt IRL is a particular case of a more general framework named random utility inverse reinforcement learning (RU-IRL). [55] RU-IRL is based on random utility theory and Markov decision processes. While prior IRL approaches assume that the apparent random behavior of an observed agent is due to it following a random policy, RU-IRL assumes that the observed agent follows a deterministic policy but randomness in observed behavior is due to the fact that an observer only has partial access to the features the observed agent uses in decision making. The utility function is modeled as a random variable to account for the ignorance of the observer regarding the features the observed agent actually considers in its utility function.\nSafe reinforcement learning (SRL) can be defined as the process of learning policies that maximize the expectation of the return in problems in which it is important to ensure reasonable system performance and/or respect safety constraints during the learning and/or deployment processes.[56]"}
{"url":"https://en.wikipedia.org/wiki/Robot_Operating_System","text":"Robot Operating System (ROS or ros) is an open-source robotics middleware suite. Although ROS is not an operating system (OS) but a set of software frameworks for robot software  development, it provides services designed for a heterogeneous computer cluster such as hardware abstraction, low-level device control, implementation of commonly used functionality, message-passing between processes, and package management. Running sets of ROS-based processes are represented in a graph architecture where processing takes place in nodes that may receive, post, and multiplex sensor data, control, state, planning, actuator, and other messages. Despite the importance of reactivity and low latency in robot control, ROS is not a real-time operating system (RTOS). However, it is possible to integrate ROS with real-time computing code.[3] The lack of support for real-time systems has been addressed in the creation of ROS 2,[4][5][6] a major revision of the ROS API which will take advantage of modern libraries and technologies for core ROS functions and add support for real-time code and embedded system hardware.\nSoftware in the ROS Ecosystem[7] can be separated into three groups:\nBoth the language-independent tools and the main client libraries (C++, Python, and Lisp) are released under the terms of the BSD license, and as such are open-source software and free for both commercial and research use. The majority of other packages are licensed under a variety of open-source licenses. These other packages implement commonly used functionality and applications such as hardware drivers, robot models, datatypes, planning, perception, simultaneous localization and mapping (SLAM), simulation tools, and other algorithms.\nThe main ROS client libraries are geared toward a Unix-like system, mostly because of their dependence on large sets of open-source software dependencies. For these client libraries, Ubuntu Linux is listed as \"Supported\" while other variants such as Fedora Linux, macOS, and Microsoft Windows are designated \"experimental\" and are supported by the community.[12] The native Java ROS client library, rosjava,[13] however, does not share these limitations and has enabled ROS-based software to be written for the Android OS.[14] rosjava has also enabled ROS to be integrated into an officially supported MATLAB toolbox which can be used on Linux, macOS, and Microsoft Windows.[15] A JavaScript client library, roslibjs[16] has also been developed which enables integration of software into a ROS system via any standards-compliant web browser.\nSometime before 2007, the first pieces of what eventually would become ROS began coalescing at Stanford University.[17][18] Eric Berger and Keenan Wyrobek, PhD students working in Kenneth Salisbury's[19] robotics laboratory at Stanford, were leading the Personal Robotics Program.[20] While working on robots to do manipulation tasks in human environments, the two students noticed that many of their colleagues were held back by the diverse nature of robotics: an excellent software developer might not have the hardware knowledge required, someone developing state of the art path planning might not know how to do the computer vision required. In an attempt to remedy this situation, the two students set out to make a baseline system that would provide a starting place for others in academia to build upon. In the words of Eric Berger, \"something that didn’t suck, in all of those different dimensions\".[17]\nIn their first steps towards this unifying system, the two built the PR1 as a hardware prototype and began to work on software from it, borrowing the best practices from other early open-source robotic software frameworks, particularly switchyard, a system that Morgan Quigley, another Stanford PhD student, had been working on in support of the STanford Artificial Intelligence Robot (STAIR)[21][22][23][24] by the Stanford Artificial Intelligence Laboratory. Early funding of US$50,000 was provided by Joanna Hoffman and Alain Rossmann, which supported the development of the PR1. While seeking funding for further development,[25] Eric Berger and Keenan Wyrobek met Scott Hassan, the founder of Willow Garage, a technology incubator which was working on an autonomous SUV and a solar autonomous boat. Hassan shared Berger and Wyrobek's vision of a \"Linux for robotics\", and invited them to come and work at Willow Garage. Willow Garage was started in January 2007, and the first commit of ROS code was made to SourceForge on 7 November 2007.[26]\nWillow Garage began developing the PR2 robot as a follow-up to the PR1, and ROS as the software to run it. Groups from more than twenty institutions made contributions to ROS, both the core software and the growing number of packages which worked with ROS to form a greater software ecosystem.[27][28] That people outside of Willow were contributing to ROS (especially from Stanford's STAIR project) meant that ROS was a multi-robot platform from the start. While Willow Garage had originally had other projects in progress, they were scrapped in favor of the Personal Robotics Program: focused on producing the PR2 as a research platform for academia and ROS as the open-source robotics stack that would underlie both academic research and tech startups, much like the LAMP stack did for web-based startups.\nIn December 2008, Willow Garage met the first of their three internal milestones: continuous navigation for the PR2 over a period of two days and a distance of pi kilometers.[29] Soon after, an early version of ROS (0.4 Mango Tango)[30] was released, followed by the first RVIZ documentation and the first paper on ROS.[28] In early summer, the second internal milestone: having the PR2 navigate the office, open doors, and plug itself it in, was reached.[31] This was followed in August by the initiation of the ROS.org website.[32] Early tutorials on ROS were posted in December,[33] preparing for the release of ROS 1.0, in January 2010.[34] This was Milestone 3: producing tons of documentation and tutorials for the enormous abilities that Willow Garage's engineers had developed over the preceding 3 years.\nFollowing this, Willow Garage achieved one of its longest held goals: giving away 10 PR2 robots to worthy academic institutions. This had long been a goal of the founders, as they felt that the PR2 could kick-start robotics research around the world. They ended up awarding eleven PR2s to different institutions, including University of Freiburg (Germany), Robert Bosch GmbH, Georgia Institute of Technology, KU Leuven (Belgium), Massachusetts Institute of Technology (MIT), Stanford University, Technical University of Munich (Germany), University of California, Berkeley, University of Pennsylvania, University of Southern California (USC), and University of Tokyo (Japan).[35] This, combined with Willow Garage's highly successful internship program[36] (run from 2008 to 2010 by Melonee Wise), helped to spread the word about ROS throughout the robotics world. The first official ROS distribution release: ROS Box Turtle, was released on 2 March 2010, marking the first time that ROS was officially distributed with a set of versioned packages for public use. These developments led to the first drone running ROS,[37] the first autonomous car running ROS,[38] and the adaption of ROS for Lego Mindstorms.[39] With the PR2 Beta program well underway, the PR2 robot was officially released for commercial purchase on 9 September 2010.[40]\n2011 was a banner year for ROS with the launch of ROS Answers, a Q/A forum for ROS users, on 15 February;[41] the introduction of the highly successful TurtleBot robot kit on 18 April;[42] and the total number of ROS repositories passing 100 on 5 May.[43] Willow Garage began 2012 by creating the Open Source Robotics Foundation (OSRF)[44] in April. The OSRF was immediately awarded a software contract by the Defense Advanced Research Projects Agency (DARPA).[45] Later that year, the first ROSCon was held in St. Paul, Minnesota,[46] the first book on ROS, ROS By Example,[47] was published, and Baxter, the first commercial robot to run ROS, was announced by Rethink Robotics.[48] Soon after passing its fifth anniversary in November, ROS began running on every continent on 3 December 2012.[49]\nIn February 2013, the OSRF became the primary software maintainers for ROS,[50] foreshadowing the announcement in August that Willow Garage would be absorbed by its founders, Suitable Technologies.[51] At this point, ROS had released seven major versions (up to ROS Groovy),[52] and had users all over the globe. This chapter of ROS development would be finalized when Clearpath Robotics took over support responsibilities for the PR2 in early 2014.[53]\nIn the years since OSRF took over primary development of ROS, a new version has been released every year,[52] while interest in ROS continues to grow. ROSCons have occurred every year since 2012, co-located with either ICRA or IROS, two flagship robotics conferences. Meetups of ROS developers have been organized in a variety of countries,[54][55][56] a number of ROS books have been published,[57] and many educational programs initiated.[58][59] On 1 September 2014, NASA announced the first robot to run ROS in space: Robotnaut 2, on the International Space Station.[60] In 2017, the OSRF changed its name to Open Robotics. Tech giants Amazon and Microsoft began to take an interest in ROS during this time, with Microsoft porting core ROS to Windows in September 2018,[61] followed by Amazon Web Services releasing RoboMaker in November 2018.[62]\nPerhaps the most important development of the OSRF/Open Robotics years thus far (not to discount the explosion of robot platforms which began to support ROS or the enormous improvements in each ROS version) was the proposal of ROS 2, a significant API change to ROS which is intended to support real-time programming, a wider variety of computing environments, and more modern technology.[63] ROS 2 was announced at ROSCon 2014,[64] the first commits to the ros2 repository were made in February 2015, followed by alpha releases in August 2015.[65] The first distribution release of ROS 2, Ardent Apalone, was released on 8 December 2017,[65] ushering in a new era of next-generation ROS development.\nROS was designed to be open source, intending that users would be able to choose the configuration of tools and libraries which interacted with the core of ROS so that users could shift their software stacks to fit their robot and application area. As such, there is very little which is core to ROS, beyond the general structure within which programs must exist and communicate. In one sense, ROS is the underlying plumbing behind nodes and message passing. However, in reality, ROS is not only that plumbing, but a rich and mature set of tools, a wide-ranging set of robot-agnostic abilities provided by packages, and a greater ecosystem of additions to ROS.\nROS processes are represented as nodes in a graph structure, connected by edges called topics.[66] ROS nodes can pass messages to one another through topics, make service calls to other nodes, provide a service for other nodes, or set or retrieve shared data from a communal database called the parameter server. A process called the ROS Master[66] makes all of this possible by registering nodes to itself, setting up node-to-node communication for topics, and controlling parameter server updates. Messages and service calls do not pass through the master, rather the master sets up peer-to-peer communication between all node processes after they register themselves with the master. This decentralized architecture lends itself well to robots, which often consist of a subset of networked computer hardware, and may communicate with off-board computers for heavy computing or commands.\nA node represents one process running the ROS graph. Every node has a name, which it registers with the ROS master before it can take any other actions. Multiple nodes with different names can exist under different namespaces, or a node can be defined as anonymous, in which case it will randomly generate an additional identifier to add to its given name. Nodes are at the center of ROS programming, as most ROS client code is in the form of a ROS node which takes actions based on information received from other nodes, sends information to other nodes, or sends and receives requests for actions to and from other nodes.\nTopics are named buses over which nodes send and receive messages.[67] Topic names must be unique within their namespace as well. To send messages to a topic, a node must publish to said topic, while to receive messages it must subscribe. The publish/subscribe model is anonymous: no node knows which nodes are sending or receiving on a topic, only that it is sending/receiving on that topic. The types of messages passed on a topic vary widely and can be user-defined. The content of these messages can be sensor data, motor control commands, state information, actuator commands, or anything else.\nA node may also advertise services.[68] A service represents an action that a node can take which will have a single result. As such, services are often used for actions which have a defined start and end, such as capturing a one-frame image, rather than processing velocity commands to a wheel motor or odometer data from a wheel encoder. Nodes advertise services and call services from one another.\nThe parameter server[68] is a database shared between nodes which allows for communal access to static or semi-static information. Data which does not change frequently and as such will be infrequently accessed, such as the distance between two fixed points in the environment, or the weight of the robot, are good candidates for storage in the parameter server.\nROS's core functionality is augmented by a variety of tools which allow developers to visualize and record data, easily navigate the ROS package structures, and create scripts automating complex configuration and setup processes. The addition of these tools greatly increases the abilities of systems using ROS by simplifying and providing solutions to a number of common robotics development problems. These tools are provided in packages like any other algorithm, but rather than providing implementations of hardware drivers or algorithms for various robotic tasks, these packages provide task and robot-agnostic tools which come with the core of most modern ROS installations.\nrviz[69] (Robot Visualization tool) is a three-dimensional visualizer used to visualize robots, the environments they work in, and sensor data. It is a highly configurable tool, with many different types of visualizations and plugins. Unified Robot Description Format (URDF) is an XML file format for robot model description.\nrosbag[70] is a command line tool used to record and playback ROS message data. rosbag uses a file format called bags,[71] which log ROS messages by listening to topics and recording messages as they come in. Playing messages back from a bag is largely the same as having the original nodes which produced the data in the ROS computation graph, making bags a useful tool for recording data to be used in later development. While rosbag is a command line only tool, rqt_bag[72] provides a GUI interface to rosbag.\ncatkin[73] is the ROS build system, having replaced rosbuild[74] as of ROS Groovy. catkin is based on CMake, and is similarly cross-platform, open-source, and language-independent.\nThe rosbash[75] package provides a suite of tools which augment the functionality of the bash shell. These tools include rosls, roscd, and roscp, which replicate the functionalities of ls, cd, and cp respectively. The ROS versions of these tools allow users to use ros package names in place of the file path where the package is located. The package also adds tab-completion to most ROS utilities, and includes rosed, which edits a given file with the chosen default text editor, as well rosrun, which runs executables in ROS packages. rosbash supports the same functionalities for zsh and tcsh, to a lesser extent.\nroslaunch[76] is a tool used to launch multiple ROS nodes both locally and remotely, as well as setting parameters on the ROS parameter server. roslaunch configuration files, which are written using XML can easily automate a complex startup and configuration process into a single command. roslaunch scripts can include other roslaunch scripts, launch nodes on specific machines, and even restart processes which die during execution.\nROS contains many open-source implementations of common robotics functionality and algorithms. These open-source implementations are organized into packages. Many packages are included as part of ROS distributions, while others may be developed by individuals and distributed through code sharing sites such as github. Some packages of note include:\nROS releases may be incompatible with other releases and are often referred to by code name rather than version number. ROS currently releases a version every year in May, following the release of Ubuntu LTS versions.[92] ROS 2 currently releases a new version every six months (in December and July). These releases are supported for a single year. There are currently two active major versions seeing releases: ROS 1 and ROS 2. Aside to this there is the ROS-Industrial or ROS-I derivate project since at least 2012.\nROS-Industrial[107] is an open-source project (BSD (legacy)/Apache 2.0 (preferred) license) that extends the advanced abilities of ROS to manufacturing automation and robotics. In the industrial environment, there are two different approaches to programming a robot: either through an external proprietary controller, typically implemented using ROS, or via the respective native programming language of the robot. ROS can therefore be seen as the software-based approach to program industrial robots instead of the classic robot controller-based approach.\nThe ROS-Industrial repository includes interfaces for common industrial manipulators, grippers, sensors, and device networks. It also provides software libraries for automatic 2D/3D sensor calibration, process path/motion planning, applications like Scan-N-Plan, developer tools like the Qt Creator ROS Plugin, and training curriculum that is specific to the needs of manufacturers. ROS-I is supported by an international Consortium of industry and research members. The project began as a collaborative endeavor between Yaskawa Motoman Robotics, Southwest Research Institute, and Willow Garage to support the use of ROS for manufacturing automation, with the GitHub repository being founded in January 2012 by Shaun Edwards (SwRI). Currently, the Consortium is divided into three groups; the ROS-Industrial Consortium Americas (led by SwRI and located in San Antonio, Texas), the ROS-Industrial Consortium Europe (led by Fraunhofer IPA and located in Stuttgart, Germany) and the ROS-Industrial Consortium Asia Pacific (led by Advanced Remanufacturing and Technology Centre (ARTC) and Nanyang Technological University (NTU) and located in Singapore).\nThe Consortia supports the global ROS-Industrial community by conducting ROS-I training, providing technical support and setting the future roadmap for ROS-I, as well as conducting precompetitive joint industry projects to develop new ROS-I abilities.[108]"}
{"url":"https://en.wikipedia.org/wiki/Intelligent_agent","text":"In intelligence and artificial intelligence, an intelligent agent (IA) is an agent acting in an intelligent manner; It perceives its environment, takes actions autonomously in order to achieve goals, and may improve its performance with learning or acquiring knowledge. An intelligent agent may be simple or complex: A thermostat or other control system is considered an example of an intelligent agent, as is a human being, as is any system that meets the definition, such as a firm, a state, or a biome.[1]\nLeading AI textbooks define \"artificial intelligence\" as the \"study and design of intelligent agents\", a definition that considers goal-directed behavior to be the essence of intelligence. Goal-directed agents are also described using a term borrowed from economics, \"rational agent\".[1]\nAn agent has an \"objective function\" that encapsulates all the IA's goals. Such an agent is designed to create and execute whatever plan will, upon completion, maximize the expected value of the objective function.[2] For example, a reinforcement learning agent has a \"reward function\" that allows the programmers to shape the IA's desired behavior,[3] and an evolutionary algorithm's behavior is shaped by a \"fitness function\".[4] \nIntelligent agents in artificial intelligence are closely related to agents in economics, and versions of the intelligent agent paradigm are studied in cognitive science, ethics, the philosophy of practical reason, as well as in many interdisciplinary socio-cognitive modeling and computer social simulations. \nIntelligent agents are often described schematically as an abstract functional system similar to a computer program. Abstract descriptions of intelligent agents are called abstract intelligent agents (AIA) to distinguish them from their real-world implementations. An autonomous intelligent agent is designed to function in the absence of human intervention. Intelligent agents are also closely related to software agents (an autonomous computer program that carries out tasks on behalf of users).\nArtificial Intelligence: A Modern Approach[5][6][2] defines an \"agent\" as \n\"Anything that can be viewed as perceiving its environment through sensors and acting upon that environment through actuators\"It defines a \"rational agent\" as:\n\"An agent that acts so as to maximize the expected value of a performance measure based on past experience and knowledge.\"It also defines the field of \"artificial intelligence research\" as:\n\"The study and design of rational agents\"Padgham \u0026 Winikoff (2005) agree that an intelligent agent is situated in an environment and responds in a timely (though not necessarily real-time) manner to changes in the environment. However, intelligent agents must also proactively pursue goals in a flexible and robust way.[a] Optional desiderata include that the agent be rational, and that the agent be capable of belief-desire-intention analysis.[7]\nKaplan and Haenlein define artificial intelligence as \"A system's ability to correctly interpret external data, to learn from such data, and to use those learnings to achieve specific goals and tasks through flexible adaptation.\"[8] This definition is closely related to that of an intelligent agent.\nPhilosophically, this definition of artificial intelligence avoids several lines of criticism. Unlike the Turing test, it does not refer to human intelligence in any way. Thus, there is no need to discuss if it is \"real\" vs \"simulated\" intelligence (i.e., \"synthetic\" vs \"artificial\" intelligence) and does not indicate that such a machine has a mind, consciousness or true understanding (i.e., it does not imply John Searle's \"strong AI hypothesis\"). It also doesn't attempt to draw a sharp dividing line between behaviors that are \"intelligent\" and behaviors that are \"unintelligent\"—programs need only be measured in terms of their objective function.\nMore importantly, it has a number of practical advantages that have helped move AI research forward. It provides a reliable and scientific way to test programs; researchers can directly compare or even combine different approaches to isolated problems, by asking which agent is best at maximizing a given \"goal function\".  It also gives them a common language to communicate with other fields—such as mathematical optimization (which is defined in terms of \"goals\") or economics (which uses the same definition of a \"rational agent\").[9]\nAn agent that is assigned an explicit \"goal function\" is considered more intelligent if it consistently takes actions that successfully maximize its programmed goal function. The goal can be simple (\"1 if the IA wins a game of Go, 0 otherwise\") or complex (\"Perform actions mathematically similar to ones that succeeded in the past\"). The \"goal function\" encapsulates all of the goals the agent is driven to act on; in the case of rational agents, the function also encapsulates the acceptable trade-offs between accomplishing conflicting goals. (Terminology varies; for example, some agents seek to maximize or minimize a \"utility function\", \"objective function\", or \"loss function\".)[6][2]\nGoals can be explicitly defined or induced. If the AI is programmed for \"reinforcement learning\", it has a \"reward function\" that encourages some types of behavior and punishes others. Alternatively, an evolutionary system can induce goals by using a \"fitness function\" to mutate and preferentially replicate high-scoring AI systems, similar to how animals evolved to innately desire certain goals such as finding food.[10] Some AI systems, such as nearest-neighbor, instead of reason by analogy, these systems are not generally given goals, except to the degree that goals are implicit in their training data.[11] Such systems can still be benchmarked if the non-goal system is framed as a system whose \"goal\" is to accomplish its narrow classification task.[12]\nSystems that are not traditionally considered agents, such as knowledge-representation systems, are sometimes subsumed into the paradigm by framing them as agents that have a goal of (for example) answering questions as accurately as possible; the concept of an \"action\" is here extended to encompass the \"act\" of giving an answer to a question. As an additional extension, mimicry-driven systems can be framed as agents who are optimizing a \"goal function\" based on how closely the IA succeeds in mimicking the desired behavior.[6][2] In the generative adversarial networks of the 2010s, an \"encoder\"/\"generator\" component attempts to mimic and improvise human text composition. The generator is attempting to maximize a function encapsulating how well it can fool an antagonistic \"predictor\"/\"discriminator\" component.[13]\nWhile symbolic AI systems often accept an explicit goal function, the paradigm can also be applied to neural networks and to evolutionary computing. Reinforcement learning can generate intelligent agents that appear to act in ways intended to maximize a \"reward function\".[14] Sometimes, rather than setting the reward function to be directly equal to the desired benchmark evaluation function, machine learning programmers will use reward shaping to initially give the machine rewards for incremental progress in learning.[15] Yann LeCun stated in 2018 that \"Most of the learning algorithms that people have come up with essentially consist of minimizing some objective function.\"[16] AlphaZero chess had a simple objective function; each win counted as +1 point, and each loss counted as -1 point. An objective function for a self-driving car would have to be more complicated.[17] Evolutionary computing can evolve intelligent agents that appear to act in ways intended to maximize a \"fitness function\" that influences how many descendants each agent is allowed to leave.[4]\nThe theoretical and uncomputable AIXI design is a maximally intelligent agent in this paradigm;[18] however, in the real world, the IA is constrained by finite time and hardware resources, and scientists compete to produce algorithms that can achieve progressively higher scores on benchmark tests with real-world hardware.[19][relevant?]\nRussell \u0026 Norvig (2003) group agents into five classes based on their degree of perceived intelligence and capability:[20]\nSimple reflex agents act only on the basis of the current percept, ignoring the rest of the percept history. The agent function is based on the condition-action rule: \"if condition, then action\".\nThis agent function only succeeds when the environment is fully observable. Some reflex agents can also contain information on their current state which allows them to disregard conditions whose actuators are already triggered.\nInfinite loops are often unavoidable for simple reflex agents operating in partially observable environments. If the agent can randomize its actions, it may be possible to escape from infinite loops.\nA model-based agent can handle partially observable environments. Its current state is stored inside the agent maintaining some kind of structure that describes the part of the world which cannot be seen. This knowledge about \"how the world works\" is called a model of the world, hence the name \"model-based agent\".\nA model-based reflex agent should maintain some sort of internal model that depends on the percept history and thereby reflects at least some of the unobserved aspects of the current state. Percept history and impact of action on the environment can be determined by using the internal model. It then chooses an action in the same way as reflex agent.\nAn agent may also use models to describe and predict the behaviors of other agents in the environment.[21]\nGoal-based agents further expand on the capabilities of the model-based agents, by using \"goal\" information. Goal information describes situations that are desirable. This provides the agent a way to choose among multiple possibilities, selecting the one which reaches a goal state. Search and planning are the subfields of artificial intelligence devoted to finding action sequences that achieve the agent's goals.\nGoal-based agents only distinguish between goal states and non-goal states. It is also possible to define a measure of how desirable a particular state is. This measure can be obtained through the use of a utility function which maps a state to a measure of the utility of the state. A more general performance measure should allow a comparison of different world states according to how well they satisfied the agent's goals. The term utility can be used to describe how \"happy\" the agent is.\n\nA rational utility-based agent chooses the action that maximizes the expected utility of the action outcomes - that is, what the agent expects to derive, on average, given the probabilities and utilities of each outcome. A utility-based agent has to model and keep track of its environment, tasks that have involved a great deal of research on perception, representation, reasoning, and learning.\nLearning has the advantage that it allows the agents to initially operate in unknown environments and to become more competent than its initial knowledge alone might allow. The most important distinction is between the \"learning element\", which is responsible for making improvements, and the \"performance element\", which is responsible for selecting external actions.\nThe learning element uses feedback from the \"critic\" on how the agent is doing and determines how the performance element, or \"actor\", should be modified to do better in the future.  The performance element is what we have previously considered to be the entire agent: it takes in percepts and decides on actions.\nThe last component of the learning agent is the \"problem generator\". It is responsible for suggesting actions that will lead to new and informative experiences.\nWeiss (2013) defines four classes of agents:\n\nIn 2013, Alexander Wissner-Gross published a theory pertaining to Freedom and Intelligence for intelligent agents.[22][23]\nTo actively perform their functions, Intelligent Agents today are normally gathered in a hierarchical structure containing many “sub-agents”. Intelligent sub-agents process and perform lower-level functions. Taken together, the intelligent agent and sub-agents create a complete system that can accomplish difficult tasks or goals with behaviors and responses that display a form of intelligence.\nGenerally, an agent can be constructed by separating the body into the sensors and actuators, and so that it operates with a complex perception system that takes the description of the world as input for a controller and outputs commands to the actuator. However, a hierarchy of controller layers is often necessary to balance the immediate reaction desired for low-level tasks and the slow reasoning about complex, high-level goals.[24]\nA simple agent program can be defined mathematically as a function f (called the \"agent function\")[25] which maps every possible percepts sequence to a possible action the agent can perform or to a coefficient, feedback element, function or constant that affects eventual actions:\nAgent function is an abstract concept as it could incorporate various principles of decision making like calculation of utility of individual options, deduction over logic rules, fuzzy logic, etc.[26]\nThe program agent, instead, maps every possible percept to an action.[27]\nWe use the term percept to refer to the agent's perceptional inputs at any given instant. In the following figures, an agent is anything that can be viewed as perceiving its environment through sensors and acting upon that environment through actuators.\nHallerbach et al. discussed the application of agent-based approaches for the development and validation of automated driving systems via a digital twin of the vehicle-under-test and microscopic traffic simulation based on independent agents.[28] Waymo has created a multi-agent simulation environment Carcraft to test algorithms for self-driving cars.[29][30] It simulates traffic interactions between human drivers, pedestrians and automated vehicles. People's behavior is imitated by artificial agents based on data of real human behavior. The basic idea of using agent-based modeling to understand self-driving cars was discussed as early as 2003.[31]\n\"Intelligent agent\" is also often used as a vague marketing term, sometimes synonymous with \"virtual personal assistant\".[32] Some 20th-century definitions characterize an agent as a program that aids a user or that acts on behalf of a user.[33] These examples are known as software agents, and sometimes an \"intelligent software agent\" (that is, a software agent with intelligence) is referred to as an \"intelligent agent\".\nAccording to Nikola Kasabov, IA systems should exhibit the following characteristics:[34]"}
{"url":"https://en.wikipedia.org/wiki/Software_agent","text":"In computer science, a software agent is a computer program that acts for a user or another program in a relationship of agency.\nThe term agent is derived from the Latin agere (to do): an agreement to act on one's behalf. Such \"action on behalf of\" implies the authority to decide which, if any, action is appropriate.[1][2] Some agents are colloquially known as bots, from robot. They may be embodied, as when execution is paired with a robot body, or as software such as a chatbot executing on a computer, such as a mobile device, e.g. Siri. Software agents may be autonomous or work together with other agents or people. Software agents interacting with people (e.g. chatbots, human-robot interaction environments) may possess human-like qualities such as natural language understanding and speech, personality or embody humanoid form (see Asimo).\nRelated and derived concepts include intelligent agents (in particular exhibiting some aspects of artificial intelligence, such as reasoning), autonomous agents (capable of modifying the methods of achieving their objectives), distributed agents (being executed on physically distinct computers), multi-agent systems (distributed agents that work together to achieve an objective that could not be accomplished by a single agent acting alone), and mobile agents (agents that can relocate their execution onto different processors).\nThe basic attributes of an autonomous software agent are that agents:\nThe term \"agent\" describes a software abstraction, an idea, or a concept, similar to OOP terms such as methods, functions, and objects.[citation needed] The concept of an agent provides a convenient and powerful way to describe a complex software entity that is capable of acting with a certain degree of autonomy in order to accomplish tasks on behalf of its host. But unlike objects, which are defined in terms of methods and attributes, an agent is defined in terms of its behavior.[3]\nVarious authors have proposed different definitions of agents, these commonly include concepts such as\nAll agents are programs, but not all programs are agents. Contrasting the term with related concepts may help clarify its meaning. Franklin \u0026 Graesser (1997)[4] discuss four key notions that distinguish agents from arbitrary programs: reaction to the environment, autonomy, goal-orientation and persistence.\nSoftware agents may offer various benefits to their end users by automating complex or repetitive tasks.[6] However, there are organizational and cultural impacts of this technology that need to be considered prior to implementing software agents.\nPeople like to perform easy tasks providing the sensation of success unless the repetition of the simple tasking is affecting the overall output. In general implementing software agents to perform administrative requirements provides a substantial increase in work contentment, as administering their own work does never please the worker. The effort freed up serves for a higher degree of engagement in the substantial tasks of individual work. Hence, software agents may provide the basics to implement self-controlled work, relieved from hierarchical controls and interference.[7] Such conditions may be secured by application of software agents for required formal support.\nThe cultural effects of the implementation of software agents include trust affliction, skills erosion, privacy attrition and social detachment. Some users may not feel entirely comfortable fully delegating important tasks to software applications. Those who start relying solely on intelligent agents may lose important skills, for example, relating to information literacy. In order to act on a user's behalf, a software agent needs to have a complete understanding of a user's profile, including his/her personal preferences. This, in turn, may lead to unpredictable privacy issues. When users start relying on their software agents more, especially for communication activities, they may lose contact with other human users and look at the world with the eyes of their agents. These consequences are what agent researchers and users must consider when dealing with intelligent agent technologies.[8]\nThe concept of an agent can be traced back to Hewitt's Actor Model (Hewitt, 1977) - \"A self-contained, interactive and concurrently-executing object, possessing internal state and communication capability.\"[citation needed]\nTo be more academic, software agent systems are a direct evolution of Multi-Agent Systems (MAS). MAS evolved from Distributed Artificial Intelligence (DAI), Distributed Problem Solving (DPS) and Parallel AI (PAI), thus inheriting all characteristics (good and bad) from DAI and AI.\nJohn Sculley's 1987 \"Knowledge Navigator\" video portrayed an image of a relationship between end-users and agents. Being an ideal first, this field experienced a series of unsuccessful top-down implementations, instead of a piece-by-piece, bottom-up approach. The range of agent types is now (from 1990) broad: WWW, search engines, etc.\nBuyer agents[9] travel around a network (e.g. the internet) retrieving information about goods and services. These agents, also known as 'shopping bots', work very efficiently for commodity products such as CDs, books, electronic components, and other one-size-fits-all products. Buyer agents are typically optimized to allow for digital payment services used in e-commerce and traditional businesses.[10]\nUser agents, or personal agents, are intelligent agents that take action on your behalf. In this category belong those intelligent agents that already perform, or will shortly perform, the following tasks:\nMonitoring and surveillance agents are used to observe and report on equipment, usually computer systems. The agents may keep track of company inventory levels, observe competitors' prices and relay them back to the company, watch stock manipulation by insider trading and rumors, etc.\nFor example, NASA's Jet Propulsion Laboratory has an agent that monitors inventory, planning, schedules equipment orders to keep costs down, and manages food storage facilities. These agents usually monitor complex computer networks that can keep track of the configuration of each computer connected to the network.\nA special case of Monitoring-and-Surveillance agents are organizations of agents used to emulate the Human Decision-Making process during tactical operations. The agents monitor the status of assets (ammunition, weapons available, platforms for transport, etc.) and receive Goals (Missions) from higher level agents. The Agents then pursue the Goals with the Assets at hand, minimizing expenditure of the Assets while maximizing Goal Attainment. (See Popplewell, \"Agents and Applicability\")\nThis agent uses information technology to find trends and patterns in an abundance of information from many different sources. The user can sort through this information in order to find whatever information they are seeking.\nA data mining agent operates in a data warehouse discovering information. A 'data warehouse' brings together information from many different sources. \"Data mining\" is the process of looking through the data warehouse to find information that you can use to take action, such as ways to increase sales or keep customers who are considering defecting.\n'Classification' is one of the most common types of data mining, which finds patterns in information and categorizes them into different classes. Data mining agents can also detect major shifts in trends or a key indicator and can detect the presence of new information and alert you to it. For example, the agent may detect a decline in the construction industry for an economy; based on this relayed information construction companies will be able to make intelligent decisions regarding the hiring/firing of employees or the purchase/lease of equipment in order to best suit their firm.\nSome other examples of current intelligent agents include some spam filters, game bots, and server monitoring tools. Search engine indexing bots also qualify as intelligent agents.\nSoftware bots are becoming important in software engineering.[12]\nAgents are also used in software security application to intercept, examine and act on various types of content.  Example include: \nIssues to consider in the development of agent-based systems include \nFor software agents to work together efficiently they must share semantics of their data elements. This can be done by having computer systems publish their metadata.\nThe definition of agent processing can be approached from two interrelated directions:\nAgent systems are used to model real-world systems with concurrency or parallel processing.\nThe agent uses its access methods to go out into local and remote databases to forage for content. These access methods may include setting up news stream delivery to the agent, or retrieval from bulletin boards, or using a spider to walk the Web. The content that is retrieved in this way is probably already partially filtered – by the selection of the newsfeed or the databases that are searched. The agent next may use its detailed searching or language-processing machinery to extract keywords or signatures from the body of the content that has been received or retrieved. This abstracted content (or event) is then passed to the agent's Reasoning or inferencing machinery in order to decide what to do with the new content. This process combines the event content with the rule-based or knowledge content provided by the user. If this process finds a good hit or match in the new content, the agent may use another piece of its machinery to do a more detailed search on the content. Finally, the agent may decide to take an action based on the new content; for example, to notify the user that an important event has occurred. This action is verified by a security function and then given the authority of the user. The agent makes use of a user-access method to deliver that message to the user. If the user confirms that the event is important by acting quickly on the notification, the agent may also employ its learning machinery to increase its weighting for this kind of event.\nBots can act on behalf of their creators to do good as well as bad. There are a few ways which bots can be created to demonstrate that they are designed with the best intention and are not built to do harm. This is first done by having a bot identify itself in the user-agent HTTP header when communicating with a site. The source IP address must also be validated to establish itself as legitimate. Next, the bot must also always respect a site's robots.txt file since it has become the standard across most of the web. And like respecting the robots.txt file, bots should shy away from being too aggressive and respect any crawl delay instructions.[14]"}
{"url":"https://en.wikipedia.org/wiki/Robotic_process_automation","text":"Robotic process automation (RPA) is a form of business process automation that is based on software robots (bots) or artificial intelligence (AI) agents.[1] RPA should not be confused with artificial intelligence as it is based on automotive technology following a predefined workflow.[2]  It is sometimes referred to as software robotics (not to be confused with robot software).\nIn traditional workflow automation tools, a software developer produces a list of actions to automate a task and interface to the back end system using internal application programming interfaces (APIs) or dedicated scripting language. In contrast, RPA systems develop the action list by watching the user perform that task in the application's graphical user interface (GUI), and then perform the automation by repeating those tasks directly in the GUI. This can lower the barrier to the use of automation in products that might not otherwise feature APIs for this purpose.\nRPA tools have strong technical similarities to graphical user interface testing tools. These tools also automate interactions with the GUI, and often do so by repeating a set of demonstration actions performed by a user. RPA tools differ from such systems in that they allow data to be handled in and between multiple applications, for instance, receiving email containing an invoice, extracting the data, and then typing that into a bookkeeping system.\nThe typical benefits of robotic automation include reduced cost; increased speed, accuracy, and consistency; improved quality and scalability of production. Automation can also provide extra security, especially for sensitive data and financial services.\nAs a form of automation, the concept has been around for a long time in the form of screen scraping, which can be traced back to early forms of malware[ambiguous]. However, RPA is much more extensible, consisting of API integration into other enterprise applications, connectors into ITSM systems, terminal services and even some types of AI (e.g. Machine Learning) services such as image recognition.  It is considered to be a significant technological evolution in the sense that new software platforms are emerging which are sufficiently mature, resilient, scalable and reliable to make this approach viable for use in large enterprises[3] (who would otherwise be reluctant due to perceived risks to quality and reputation).\nA principal barrier to the adoption of self-service is often technological: it may not always be feasible or economically viable to retrofit new interfaces onto existing systems. Moreover, organisations may wish to layer a variable and configurable set of process rules on top of the system interfaces which may vary according to market offerings and the type of customer. This only adds to the cost and complexity of the technological implementation. Robotic automation software provides a pragmatic means of deploying new services in this situation, where the robots simply mimic the behaviour of humans to perform the back-end transcription or processing. The relative affordability of this approach arises from the fact that no new IT transformation or investment is required; instead the software robots simply leverage greater use out of existing IT assets.\nThe hosting of RPA services also aligns with the metaphor of a software robot, with each robotic instance having its own virtual workstation, much like a human worker. The robot uses keyboard and mouse controls to take actions and execute automations. Normally all of these actions take place in a virtual environment and not on screen; the robot does not need a physical screen to operate, rather it interprets the screen display electronically. The scalability of modern solutions based on architectures such as these owes much to the advent of virtualization technology, without which the scalability of large deployments would be limited by the available capacity to manage physical hardware and by the associated costs. The implementation of RPA in business enterprises has shown dramatic cost savings when compared to traditional non-RPA solutions.[4]\nThere are however several risks with RPA. Criticism includes risks of stifling innovation and creating a more complex maintenance environment of existing software that now needs to consider the use of graphical user interfaces in a way they weren't intended to be used.[5]\nAccording to Harvard Business Review, most operations groups adopting RPA have promised their employees that automation would not result in layoffs.[6] Instead, workers have been redeployed to do more interesting work. One academic study highlighted that knowledge workers did not feel threatened by automation: they embraced it and viewed the robots as team-mates.[7] The same study highlighted that, rather than resulting in a lower \"headcount\", the technology was deployed in such a way as to achieve more work and greater productivity with the same number of people.\nConversely, however, some analysts proffer that RPA represents a threat to the business process outsourcing (BPO) industry.[8] The thesis behind this notion is that RPA will enable enterprises to \"repatriate\" processes from offshore locations into local data centers, with the benefit of this new technology. The effect, if true, will be to create high-value jobs for skilled process designers in onshore locations (and within the associated supply chain of IT hardware, data center management, etc.) but to decrease the available opportunity to low-skilled workers offshore. On the other hand, this discussion appears to be healthy ground for debate as another academic study was at pains to counter the so-called \"myth\" that RPA will bring back many jobs from offshore.[7]\nAcademic studies[9][10] project that RPA, among other technological trends, is expected to drive a new wave of productivity and efficiency gains in the global labour market. Although not directly attributable to RPA alone, Oxford University conjectures that up to 35% of all jobs might be automated by 2035.[9]\nThere are geographic implications to the trend in robotic automation. In the example above where an offshored process is \"repatriated\" under the control of the client organization (or even displaced by a Business Process Outsourcer) from an offshore location to a data centre, the impact will be a deficit in economic activity to the offshore location and an economic benefit to the originating economy. On this basis, developed economies – with skills and technological infrastructure to develop and support a robotic automation capability – can be expected to achieve a net benefit from the trend.\nIn a TEDx talk[11] hosted by University College London (UCL), entrepreneur David Moss explains that digital labour in the form of RPA is likely to revolutionize the cost model of the services industry by driving the price of products and services down, while simultaneously improving the quality of outcomes and creating increased opportunity for the personalization of services.\nIn a separate TEDx in 2019 talk,[12] Japanese business executive, and former CIO of Barclays bank, Koichi Hasegawa noted that digital robots can be a positive effect on society if we start using a robot with empathy to help every person. He provides a case study of the Japanese insurance companies – Sompo Japan and Aioi – both of whom introduced bots to speed up the process of insurance pay-outs in past massive disaster incidents.\nMeanwhile, Professor Willcocks, author of the LSE paper[10] cited above, speaks of increased job satisfaction and intellectual stimulation, characterising the technology as having the ability to \"take the robot out of the human\",[13] a reference to the notion that robots will take over the mundane and repetitive portions of people's daily workload, leaving them to be used in more interpersonal roles or to concentrate on the remaining, more meaningful, portions of their day.\nIt was also found in a 2021 study observing the effects of robotization in Europe that, the gender pay gap increased at a rate of .18% for every 1% increase in robotization of a given industry.[14]\nUnassisted RPA, or RPAAI,[15][16] is the next generation of RPA related technologies. Technological advancements around artificial intelligence allow a process to be run on a computer without needing input from a user.\nHyperautomation is the application of advanced technologies like RPA, artificial intelligence, machine learning (ML) and process mining to augment workers and automate processes in ways that are significantly more impactful than traditional automation capabilities.[17][18][19] Hyperautomation is the combination of automation tools to deliver work.[20]\nGartner's report notes that this trend was kicked off with robotic process automation (RPA). The report notes that, \"RPA alone is not hyperautomation. Hyperautomation requires a combination of tools to help support replicating pieces of where the human is involved in a task.\"[21]\nBack office clerical processes outsourced by large organisations - particularly those sent offshore - tend to be simple and transactional in nature, requiring little (if any) analysis or subjective judgement. This would seem to make an ideal starting point for organizations beginning to adopt robotic automation for the back office. Whether client organisations choose to take outsourced processes back \"in house\" from their Business Process Outsourcing (BPO) providers, thus representing a threat to the future of the BPO business,[22] or whether the BPOs implement such automations on their clients' behalf may well depend on a number of factors.\nConversely however, a BPO provider may seek to effect some form of client lock-in by means of automation. By removing cost from a business operation, where the BPO provider is considered to be the owner of the intellectual property and physical implementation of a robotic automation solution (perhaps in terms of hardware, ownership of software licences, etc.), the provider can make it very difficult for the client to take a process back \"in house\" or elect a new BPO provider. This effect occurs as the associated cost savings made through automation would - temporarily at least - have to be reintroduced to the business whilst the technical solution is reimplemented in the new operational context.\nThe geographically agnostic nature of software means that new business opportunities may arise for those organisations that have a political or regulatory impediment to offshoring or outsourcing. A robotised automation can be hosted in a data centre in any jurisdiction and this has two major consequences for BPO providers. Firstly, for example, a sovereign government may not be willing or legally able to outsource the processing of tax affairs and security administration.  On this basis, if robots are compared to a human workforce, this creates a genuinely new opportunity for a \"third sourcing\" option, after the choices of onshore vs. offshore. Secondly, and conversely, BPO providers have previously relocated outsourced operations to different political and geographic territories in response to changing wage inflation and new labor arbitrage opportunities elsewhere. By contrast, a data centre solution would seem to offer a fixed and predictable cost base that, if sufficiently low in cost on a robot vs. human basis,  would seem to eliminate any potential need or desire to continually relocate operational bases.\nWhile robotic process automation has many benefits including cost efficiency and consistency in performance, it also has some limitations. Current RPA solutions demand continual technical support to handle system changes, therefore it lacks the ability to autonomously adapt to new conditions. Because of this limitation, the system sometimes needs manual reconfiguration, which in turn has an effect on efficiency.[23]\nRPA is based on automotive technology following a predefined workflow, and artificial intelligence is data-driven and focuses on processing information to make predictions. Therefore there is a distinct difference between how the two systems operate. AI aims to mimic human intelligence, whereas RPA is focused on reproducing tasks that are typically human-directed.[24] Moreover, RPA could also be explained as virtual robots that take over routinized human work, it can identify data by interpreting the underlying tags. RPA, therefore, is based on machine learning, whereas AI utilizes self-learning technologies.[25]"}
{"url":"https://en.wikipedia.org/wiki/Chatbot","text":"A chatbot (originally chatterbot)[1] is a software application or web interface that is designed to mimic human conversation through text or voice interactions.[2][3][4] Modern chatbots are typically online and use generative artificial intelligence systems that are capable of maintaining a conversation with a user in natural language and simulating the way a human would behave as a conversational partner. Such chatbots often use deep learning and natural language processing, but simpler chatbots have existed for decades.\nSince late 2022, the field has gained widespread attention due to the popularity of OpenAI's ChatGPT,[5][6] followed by alternatives such as Microsoft's Copilot and Google's Gemini.[7] Such examples reflect the recent practice of basing such products upon broad foundational large language models, such as GPT-4 or the Gemini language model, that get fine-tuned so as to target specific tasks or applications (i.e., simulating human conversation, in the case of chatbots). Chatbots can also be designed or customized to further target even more specific situations and/or particular subject-matter domains.[8]\nA major area where chatbots have long been used is in customer service and support, with various sorts of virtual assistants.[9] Companies spanning a wide range of industries have begun using the latest generative artificial intelligence technologies to power more advanced developments in such areas.[8]\nAs chatbots work by predicting responses rather than knowing the meaning of their responses, this means they can produce coherent-sounding but inaccurate or fabricated content, referred to as ‘hallucinations’. When humans use and apply chatbot content contaminated with hallucinations, this results in ‘botshit’.[10] Given the increasing adoption and use of chatbots for generating content, there are concerns that this technology will significantly reduce the cost it takes humans to generate, spread and consume bullshit.[11]\nIn 1950, Alan Turing's famous article \"Computing Machinery and Intelligence\" was published,[12] which proposed what is now called the Turing test as a criterion of intelligence. This criterion depends on the ability of a computer program to impersonate a human in a real-time written conversation with a human judge to the extent that the judge is unable to distinguish reliably—on the basis of the conversational content alone—between the program and a real human. The notoriety of Turing's proposed test stimulated great interest in Joseph Weizenbaum's program ELIZA, published in 1966, which seemed to be able to fool users into believing that they were conversing with a real human. However Weizenbaum himself did not claim that ELIZA was genuinely intelligent, and the introduction to his paper presented it more as a debunking exercise:\nIn artificial intelligence, machines are made to behave in wondrous ways, often sufficient to dazzle even the most experienced observer. But once a particular program is unmasked, once its inner workings are explained, its magic crumbles away; it stands revealed as a mere collection of procedures. The observer says to himself \"I could have written that\". With that thought, he moves the program in question from the shelf marked \"intelligent\", to that reserved for curios. The object of this paper is to cause just such a re-evaluation of the program about to be \"explained\". Few programs ever needed it more.[13]ELIZA's key method of operation (copied by chatbot designers ever since) involves the recognition of clue words or phrases in the input, and the output of the corresponding pre-prepared or pre-programmed responses that can move the conversation forward in an apparently meaningful way (e.g. by responding to any input that contains the word 'MOTHER' with 'TELL ME MORE ABOUT YOUR FAMILY').[13] Thus an illusion of understanding is generated, even though the processing involved has been merely superficial. ELIZA showed that such an illusion is surprisingly easy to generate because human judges are so ready to give the benefit of the doubt when conversational responses are capable of being interpreted as \"intelligent\".\nInterface designers have come to appreciate that humans' readiness to interpret computer output as genuinely conversational—even when it is actually based on rather simple pattern-matching—can be exploited for useful purposes. Most people prefer to engage with programs that are human-like, and this gives chatbot-style techniques a potentially useful role in interactive systems that need to elicit information from users, as long as that information is relatively straightforward and falls into predictable categories. Thus, for example, online help systems can usefully employ chatbot techniques to identify the area of help that users require, potentially providing a \"friendlier\" interface than a more formal search or menu system. This sort of usage holds the prospect of moving chatbot technology from Weizenbaum's \"shelf ... reserved for curios\" to that marked \"genuinely useful computational methods\".\nAmong the most notable early chatbots are ELIZA (1966) and PARRY (1972).[14][15][16][17] More recent notable programs include A.L.I.C.E., Jabberwacky and D.U.D.E (Agence Nationale de la Recherche and CNRS 2006). While ELIZA and PARRY were used exclusively to simulate typed conversation, many chatbots now include other functional features, such as games and web searching abilities. In 1984, a book called The Policeman's Beard is Half Constructed was published, allegedly written by the chatbot Racter (though the program as released would not have been capable of doing so).[18]\nFrom 1978[19] to some time after 1983,[20] the CYRUS project led by Janet Kolodner constructed a chatbot simulating Cyrus Vance (57th United States Secretary of State). It used case-based reasoning, and updated its database daily by parsing wire news from United Press International. The program was unable to process the news items subsequent to the surprise resignation of Cyrus Vance in April 1980, and the team constructed another chatbot simulating his successor, Edmund Muskie.[21][20]\nOne pertinent field of AI research is natural-language processing. Usually, weak AI fields employ specialized software or programming languages created specifically for the narrow function required. For example, A.L.I.C.E. uses a markup language called AIML,[3] which is specific to its function as a conversational agent, and has since been adopted by various other developers of, so-called, Alicebots. Nevertheless, A.L.I.C.E. is still purely based on pattern matching techniques without any reasoning capabilities, the same technique ELIZA was using back in 1966. This is not strong AI, which would require sapience and logical reasoning abilities.\nJabberwacky learns new responses and context based on real-time user interactions, rather than being driven from a static database. Some more recent chatbots also combine real-time learning with evolutionary algorithms that optimize their ability to communicate based on each conversation held. Still, there is currently no general purpose conversational artificial intelligence, and some software developers focus on the practical aspect, information retrieval.\nChatbot competitions focus on the Turing test or more specific goals. Two such annual contests are the Loebner Prize and The Chatterbox Challenge (the latter has been offline since 2015, however, materials can still be found from web archives).[22]\nChatbots may use artificial neural networks as a language model. For example, generative pre-trained transformers (GPT), which use the transformer architecture, have become common to build sophisticated chatbots. The \"pre-training\" in its name refers to the initial training process on a large text corpus, which provides a solid foundation for the model to perform well on downstream tasks with limited amounts of task-specific data. An example of a GPT chatbot is ChatGPT.[23] Despite criticism of its accuracy and tendency to “hallucinate”—that is, to confidently output false information and even cite non-existent sources—ChatGPT has gained attention for its detailed responses and historical knowledge. Another example is BioGPT, developed by Microsoft, which focuses on answering biomedical questions.[24][25] In November 2023, Amazon announced a new chatbot, called Q, for people to use at work.[26]\nDBpedia created a chatbot during the GSoC of 2017.[27][28][29] It can communicate through Facebook Messenger (see Master of Code Global article).\nMany companies' chatbots run on messaging apps or simply via SMS. They are used for B2C customer service, sales and marketing.[30]\nIn 2016, Facebook Messenger allowed developers to place chatbots on their platform. There were 30,000 bots created for Messenger in the first six months, rising to 100,000 by September 2017.[31]\nSince September 2017, this has also been as part of a pilot program on WhatsApp. Airlines KLM and Aeroméxico both announced their participation in the testing;[32][33][34][35] both airlines had previously launched customer services on the Facebook Messenger platform. \nThe bots usually appear as one of the user's contacts, but can sometimes act as participants in a group chat.\nMany banks, insurers, media companies, e-commerce companies, airlines, hotel chains, retailers, health care providers, government entities and restaurant chains have used chatbots to answer simple questions, increase customer engagement,[36] for promotion, and to offer additional ways to order from them.[37] Chatbots are also used in market research to collect short survey responses.[38]\nA 2017 study showed 4% of companies used chatbots.[39] According to a 2016 study, 80% of businesses said they intended to have one by 2020.[40]\nPrevious generations of chatbots were present on company websites, e.g. Ask Jenn from Alaska Airlines which debuted in 2008[41] or Expedia's virtual customer service agent which launched in 2011.[41][42] The newer generation of chatbots includes IBM Watson-powered \"Rocky\", introduced in February 2017 by the New York City-based e-commerce company Rare Carat to provide information to prospective diamond buyers.[43][44]\nUsed by marketers to script sequences of messages, very similar to an autoresponder sequence. Such sequences can be triggered by user opt-in or the use of keywords within user interactions. After a trigger occurs a sequence of messages is delivered until the next anticipated user response. Each user response is used in the decision tree to help the chatbot navigate the response sequences to deliver the correct response message.\nOther companies explore ways they can use chatbots internally, for example for Customer Support, Human Resources, or even in Internet-of-Things (IoT) projects. Overstock.com, for one, has reportedly launched a chatbot named Mila to automate certain simple yet time-consuming processes when requesting sick leave.[45] Other large companies such as Lloyds Banking Group, Royal Bank of Scotland, Renault and Citroën are now using automated online assistants instead of call centres with humans to provide a first point of contact. A SaaS chatbot business ecosystem has been steadily growing since the F8 Conference when Facebook’s Mark Zuckerberg unveiled that Messenger would allow chatbots into the app.[46] In large companies, like in hospitals and aviation organizations, IT architects are designing reference architectures for Intelligent Chatbots that are used to unlock and share knowledge and experience in the organization more efficiently, and reduce the errors in answers from expert service desks significantly.[47] These Intelligent Chatbots make use of all kinds of artificial intelligence like image moderation and natural-language understanding (NLU), natural-language generation (NLG), machine learning and deep learning.\nMany high-tech banking organizations are looking to integrate automated AI-based solutions such as chatbots into their customer service in order to provide faster and cheaper assistance to their clients who are becoming increasingly comfortable with technology.  In particular, chatbots can efficiently conduct a dialogue, usually replacing other communication tools such as email, phone, or SMS. In banking, their major application is related to quick customer service answering common requests, as well as transactional support.\nSeveral studies report significant reduction in the cost of customer services, expected to lead to billions of dollars of economic savings in the next ten years.[48] In 2019, Gartner predicted that by 2021, 15% of all customer service interactions globally will be handled completely by AI.[49]  A study by Juniper Research in 2019 estimates retail sales resulting from chatbot-based interactions will reach $112 billion by 2023.[50]\nSince 2016, when Facebook allowed businesses to deliver automated customer support, e-commerce guidance, content, and interactive experiences through chatbots, a large variety of chatbots were developed for the Facebook Messenger platform.[51]\nIn 2016, Russia-based Tochka Bank launched the world's first Facebook bot for a range of financial services, including a possibility of making payments.[52]\nIn July 2016, Barclays Africa also launched a Facebook chatbot, making it the first bank to do so in Africa.[53]\nThe France's third-largest bank by total assets[54] Société Générale launched their chatbot called SoBot in March 2018. While 80% of users of the SoBot expressed their satisfaction after having tested it, Société Générale deputy director Bertrand Cozzarolo stated that it will never replace the expertise provided by a human advisor.\n[55]\nThe advantages of using chatbots for customer interactions in banking include cost reduction, financial advice, and 24/7 support.[56][57]\nChatbots are also appearing in the healthcare industry.[58][59] A study suggested that physicians in the United States believed that chatbots would be most beneficial for scheduling doctor appointments, locating health clinics, or providing medication information.[60]\nWhatsapp has teamed up with the World Health Organization (WHO) to make a chatbot service that answers users' questions on COVID-19.[61]\nIn 2020, The Indian Government launched a chatbot called MyGov Corona Helpdesk,[62] that worked through Whatsapp and helped people access information about the Coronavirus (COVID-19) pandemic.[63][64]\nCertain patient groups are still reluctant to use chatbots. A mixed-methods study showed that people are still hesitant to use chatbots for their healthcare due to poor understanding of the technological complexity, the lack of empathy, and concerns about cyber-security.[65] The analysis showed that while 6% had heard of a health chatbot and 3% had experience of using it, 67% perceived themselves as likely to use one within 12 months. The majority of participants would use a health chatbot for seeking general health information (78%), booking a medical appointment (78%), and looking for local health services (80%). However, a health chatbot was perceived as less suitable for seeking results of medical tests and seeking specialist advice such as sexual health. \nThe analysis of attitudinal variables showed that most participants reported their preference for discussing their health with doctors (73%) and having access to reliable and accurate health information (93%). While 80% were curious about new technologies that could improve their health, 66% reported only seeking a doctor when experiencing a health problem and 65% thought that a chatbot was a good idea. 30% reported dislike about talking to computers, 41% felt it would be strange to discuss health matters with a chatbot and about half were unsure if they could trust the advice given by a chatbot. Therefore, perceived trustworthiness, individual attitudes towards bots, and dislike for talking to computers are the main barriers to health chatbots.\nIn New Zealand, the chatbot SAM – short for Semantic Analysis Machine[66] (made by Nick Gerritsen of Touchtech[67]) – has been developed. It is designed to share its political thoughts, for example on topics such as climate change, healthcare and education, etc. It talks to people through Facebook Messenger.[68][69][70][71]\nIn 2022, the chatbot \"Leader Lars\" or \"Leder Lars\" was nominated for The Synthetic Party to run in the Danish parliamentary election,[72] and was built by the artist collective Computer Lars.[73] Leader Lars differed from earlier virtual politicians by leading a political party and by not pretending to be an objective candidate.[74] This chatbot engaged in critical discussions on politics with users from around the world.[75]\nIn India, the state government has launched a chatbot for its Aaple Sarkar platform,[76] which provides conversational access to information regarding public services managed.[77][78]\nChatbots have been used at different levels of government departments, including local, national and regional contexts. Chatbots are used to provide services like citizenship and immigration, court administrations, financial aid, and migrants’ rights inquiries. For example, EMMA answers more than 500,000 inquiries monthly, regarding services on citizenship and immigration in the US.[79]\nChatbots have also been incorporated into devices not primarily meant for computing, such as toys.[80]\nHello Barbie is an Internet-connected version of the doll that uses a chatbot provided by the company ToyTalk,[81] which previously used the chatbot for a range of smartphone-based characters for children.[82] These characters' behaviors are constrained by a set of rules that in effect emulate a particular character and produce a storyline.[83]\nThe My Friend Cayla doll was marketed as a line of 18-inch (46 cm) dolls which uses speech recognition technology in conjunction with an Android or iOS mobile app to recognize the child's speech and have a conversation. It, like the Hello Barbie doll, attracted controversy due to vulnerabilities with the doll's Bluetooth stack and its use of data collected from the child's speech.\nIBM's Watson computer has been used as the basis for chatbot-based educational toys for companies such as CogniToys[80] intended to interact with children for educational purposes.[84]\nMalicious chatbots are frequently used to fill chat rooms with spam and advertisements, by mimicking human behavior and conversations or to entice people into revealing personal information, such as bank account numbers. They were commonly found on Yahoo! Messenger, Windows Live Messenger, AOL Instant Messenger and other instant messaging protocols. There has also been a published report of a chatbot used in a fake personal ad on a dating service's website.[85]\nTay, an AI chatbot that learns from previous interaction, caused major controversy due to it being targeted by internet trolls on Twitter. The bot was exploited, and after 16 hours began to send extremely offensive Tweets to users. This suggests that although the bot learned effectively from experience, adequate protection was not put in place to prevent misuse.[86]\nIf a text-sending algorithm can pass itself off as a human instead of a chatbot, its message would be more credible. Therefore, human-seeming chatbots with well-crafted online identities could start scattering fake news that seems plausible, for instance making false claims during an election. With enough chatbots, it might be even possible to achieve artificial social proof.[87][88]\nThe creation and implementation of chatbots is still a developing area, heavily related to artificial intelligence and machine learning, so the provided solutions, while possessing obvious advantages, have some important limitations in terms of functionalities and use cases. However, this is changing over time.\nThe most common limitations are listed below:[89]\nChatbots are increasingly present in businesses and often are used to automate tasks that do not require skill-based talents. With customer service taking place via messaging apps as well as phone calls, there are growing numbers of use-cases where chatbot deployment gives organizations a clear return on investment. Call center workers may be particularly at risk from AI-driven chatbots.[91]\nChatbot jobs\nChatbot developers create, debug, and maintain applications that automate customer services or other communication processes. Their duties include reviewing and simplifying code when needed. They may also help companies implement bots in their operations.\nA study by Forrester (June 2017) predicted that 25% of all jobs would be impacted by AI technologies by 2019.[92]\nPrompt Engineering, the task of designing and refining prompts (inputs) leading to desired AI-generated responses has gained significant demand and popularity in recent years, with the advent of sophisticated models, notably OpenAI’s GPT series (which still contain notable flaws and limitations, as previously outlined)."}
{"url":"https://en.wikipedia.org/wiki/Applications_of_artificial_intelligence","text":"Artificial intelligence (AI) has been used in applications throughout industry and academia. Similar to electricity or computers, AI serves as a general-purpose technology that has numerous applications. Its applications span language translation, image recognition, decision-making,[1] credit scoring, e-commerce and various other domains. AI which accommodates such technologies as machines being equipped perceive, understand, act and learning a scientific discipline.[2]\nA recommendation system predicts the rating or preference a user would give to an item.[3][4] Artificial intelligence recommendation systems are designed to offer suggestions based on previous behavior. These systems have been used by companies such as Netflix, Amazon, Instagram and YouTube, where they generate personalized playlists, product suggestions, and video recommendations.[5]\nMachine learning is also used in web feeds such as for determining which posts should show up in social media feeds.[6][7] Various types of social media analysis also make use of machine learning[8][9] and there is research into its use for (semi-)automated tagging/enhancement/correction of online misinformation and related filter bubbles.[10][11][12]\nAI is used to target web advertisements to those most likely to click or engage in them. It is also used to increase time spent on a website by selecting attractive content for the viewer. It can predict or generalize the behavior of customers from their digital footprints.[13] Both AdSense[citation needed] and Facebook[14] use AI for advertising.\nOnline gambling companies use AI to improve customer targeting.[15]\nPersonality computing AI models add psychological targeting to more traditional social demographics or behavioral targeting.[16] AI has been used to customize shopping options and personalize offers.[17]\nIntelligent personal assistants use AI to understand many natural language requests in other ways than rudimentary commands. Common examples are Apple's Siri, Amazon's Alexa, and a more recent AI, ChatGPT by OpenAI.[18]\nBing Chat has used artificial intelligence as part of its search engine.[19]\nMachine learning can be used to fight against spam, scams, and phishing. It can scrutinize the contents of spam and phishing attacks to attempt to identify malicious elements.[20] Some models built via machine learning algorithms have over 90% accuracy in distinguishing between spam and legitimate emails.[21] These models can be refined from new data and evolving spam tactics. Machine learning also analyzes traits such as sender behavior, email header information, and attachment types.[22]\nSpeech translation technology attempts to convert one language's spoken words into another. This potentially reduces language barriers in global commerce and cross-cultural exchange by allowing speakers of various languages to communicate with one another.[23] \nAI has been used to automatically translate spoken language and textual content, in products such as Microsoft Translator, Google Translate and DeepL Translator.[24] Additionally, research and development are in progress to decode and conduct animal communication.[25][26]\nMeaning is conveyed not only by text, but also through usage and context (see semantics and pragmatics). As a result, the two primary categorization approaches for machine translations are statistical and neural machine translations (NMTs). The old method of performing translation was to use a statistical machine translation (SMT) methodology to forecast the best probable output with specific algorithms. However, with NMT, the approach employs dynamic algorithms to achieve better translations based on context.[27]\nAI has been used in facial recognition systems, with a 99% accuracy rate. Some examples are Apple's Face ID and Android's Face Unlock, which are used to secure mobile devices.[28]\nImage labeling has been used by Google to detect products in photos and to allow people to search based on a photo. Image labeling has also been demonstrated to generate speech to describe images to blind people. [29] Facebook's DeepFace identifies human faces in digital images.\nGames have been a major application[relevant?] of AI's capabilities since the 1950s. In the 21st century, AIs have beaten human players in many games, including chess (Deep Blue), Jeopardy! (Watson),[30] Go (AlphaGo),[31][32][33][34][35][36][37] poker (Pluribus[38] and Cepheus),[39] E-sports (StarCraft),[40][41] and general game playing (AlphaZero[42][43][44] and MuZero).[45][46][47][48] AI has replaced hand-coded algorithms in most chess programs.[49] Unlike go or chess, poker is an imperfect-information game, so a program that plays poker has to reason under uncertainty. The general game players work using feedback from the game system, without knowing the rules.\nAI for Good is an ITU initiative supporting institutions employing AI to tackle some of the world's greatest economic and social challenges. For example, the University of Southern California launched the Center for Artificial Intelligence in Society, with the goal of using AI to address problems such as homelessness. At Stanford, researchers use AI to analyze satellite images to identify high poverty areas.[50]\nIn agriculture, AI has helped farmers identify areas that need irrigation, fertilization, pesticide treatments or increasing yield.[51] Agronomists use AI to conduct research and development. AI has been used to predict the ripening time for crops such as tomatoes,[52] monitor soil moisture, operate agricultural robots, conduct predictive analytics,[53][54] classify livestock pig call emotions,[25] automate greenhouses,[55] detect diseases and pests,[56][57] and save water.[58]\nAI helps in achieving precise farming, which calls for the use of algorithims to analyze data retrieved from satellite imagery and on-site field sensors. It allows for optimization of resource usage and helps to make the right decisions regarding the kind of nutrients, water, and pesticides required to maximize yield.[59] \nUsing machine learning models to monitor the health of crops and the soil. The models will be able to detect and predict diseases and pests in crops ahead of time to allow timely interventions.[60] \nThere are automated machinery such as tractors and harvesters, which can operate autonomously with minimal human labor. With the use of AI many duties in the area are possible to be done with precision.[61] \nCyber security companies are adopting neural networks, machine learning, and natural language processing to improve their systems.[62]\nApplications of AI in cyber security include:\nGoogle fraud czar Shuman Ghosemajumder has said that AI will be used to completely automate most cyber security operations over time.[67]\nAI elevates teaching, focusing on significant issues like the knowledge nexus and educational equality. The evolution of AI in education and technology should be used to improve human capabilities in relationships where they do not replace humans. UNESCO recognizes the future of AI in education as an instrument to reach Sustainable Development Goal 4, called \"Inclusive and Equitable Quality Education.” [68]\nThe World Economic Forum also stresses AI's contribution to students' overall improvement and transforming teaching into a more enjoyable process.[68]\nPersonalized Learning\nAI driven tutoring systems, such as Khan Academy, Duo-lingo and Carnegie Learning  are the forefoot of delivering personalized education.[69]\nThese platforms leverage AI algorithms to analyze individual learning patterns, strengths, and weaknesses, enabling the customization of content to suit each student's pace and style of learning.[69]\nAdministrative Efficiency\nIn educational institutions, AI is increasingly used to automate routine tasks like grading and attendance tracking, which allows educators to devote more time to interactive teaching and direct student engagement.[70]\nFurthermore, AI tools are employed to monitor student progress, analyze learning behaviors, and predict academic challenges, facilitating timely and proactive interventions for students who may be at risk of falling behind.[70]\nEthical and Privacy Concerns\nDespite the benefits, the integration of AI in education raises significant ethical and privacy concerns, particularly regarding the handling of sensitive student data.[69] \nIt is imperative that AI systems in education are designed and operated with a strong emphasis on transparency, security, and respect for privacy to maintain trust and uphold the integrity of educational practices.[69]\nFinancial institutions have long used artificial neural network systems to detect charges or claims outside of the norm, flagging these for human investigation. The use of AI in banking began in 1987 when Security Pacific National Bank launched a fraud prevention taskforce to counter the unauthorized use of debit cards.[71] Kasisto and Moneystream use AI.\nBanks use AI to organize operations, for bookkeeping, investing in stocks, and managing properties. AI can react to changes when business is not taking place.[72] AI is used to combat fraud and financial crimes by monitoring behavioral patterns for any abnormal changes or anomalies.[73][74][75]\nThe use of AI in applications such as online trading and decision-making has changed major economic theories.[76] For example, AI-based buying and selling platforms estimate individualized demand and supply curves and thus enable individualized pricing. AI machines reduce information asymmetry in the market and thus make markets more efficient.[77] The application of artificial intelligence in the financial industry can alleviate the financing constraints of non-state-owned enterprises. Especially for smaller and more innovative enterprises.[78]\nAlgorithmic trading involves the use of AI systems to make trading decisions at speeds orders of magnitude greater than any human is capable of, making millions of trades in a day without human intervention. Such high-frequency trading represents a fast-growing sector. Many banks, funds, and proprietary trading firms now have entire portfolios that are AI-managed. Automated trading systems are typically used by large institutional investors but include smaller firms trading with their own AI systems.[79]\nLarge financial institutions use AI to assist with their investment practices. BlackRock's AI engine, Aladdin, is used both within the company and by clients to help with investment decisions. Its functions include the use of natural language processing to analyze text such as news, broker reports, and social media feeds. It then gauges the sentiment on the companies mentioned and assigns a score. Banks such as UBS and Deutsche Bank use SQREEM (Sequential Quantum Reduction and Extraction Model) to mine data to develop consumer profiles and match them with wealth management products.[80]\nOnline lender Upstart uses machine learning for underwriting.[81]\nZestFinance's Zest Automated Machine Learning (ZAML) platform is used for credit underwriting. This platform uses machine learning to analyze data including purchase transactions and how a customer fills out a form to score borrowers. The platform is particularly useful to assign credit scores to those with limited credit histories.[82]\nAI makes continuous auditing possible. Potential benefits include reducing audit risk, increasing the level of assurance, and reducing audit duration.[83][quantify]\nContinuous auditing with AI allows a real-time monitoring and reporting of financial activities and providing businesses with timely insights that can lead to quick decision making.[84] \nAI software, such as LaundroGraph which uses contemporary suboptimal datasets, could be used for anti-money laundering (AML).[85][86] AI can be used to \"develop the AML pipeline into a robust, scalable solution with a reduced false positive rate and high adaptability\".[87] A study about deep learning for AML identified \"key challenges for researchers\" to have \"access to recent real transaction data and scarcity of labelled training data; and data being highly imbalanced\" and suggests future research should bring-out \"explainability, graph deep learning using natural language processing (NLP), unsupervised and reinforcement learning to handle lack of labelled data; and joint research programs between the research community and industry to benefit from domain knowledge and controlled access to data\".[88]\nBanks use machine learning (ML) to upgrade process monitoring and demonstrating the ability of  responding efficiently to evolving techniques.[89]\nThrough ML and other methods, financial organizations can detect laundering operations and run compliance in an automated and very fast mode.[89]\nIn the 1980s, AI started to become prominent in finance as expert systems were commercialized. For example, Dupont created 100 expert systems, which helped them to save almost $10 million per year.[90] One of the first systems was the Pro-trader expert system that predicted the 87-point drop in the Dow Jones Industrial Average in 1986. \"The major junctions of the system were to monitor premiums in the market, determine the optimum investment strategy, execute transactions when appropriate and modify the knowledge base through a learning mechanism.\"[91]\nOne of the first expert systems to help with financial plans was PlanPowerm and Client Profiling System, created by Applied Expert Systems (APEX). It was launched in 1986. It helped create personal financial plans for people.[92]\nIn the 1990s AI was applied to fraud detection. In 1993 FinCEN Artificial Intelligence System (FAIS) launched. It was able to review over 200,000 transactions per week and over two years it helped identify 400 potential cases of money laundering equal to $1 billion.[93] These expert systems were later replaced by machine learning systems.[94]\nAI can enhance entrepreneurial activity and AI is one of the most dynamic areas for start-ups, with significant venture capital flowing into AI.[95]\nAI facial recognition systems are used for mass surveillance, notably in China.[96][97]\nIn 2019, Bengaluru, India deployed AI-managed traffic signals. This system uses cameras to monitor traffic density and adjust signal timing based on the interval needed to clear traffic.[98]\nVarious countries are deploying AI military applications.[99] The main applications enhance command and control, communications, sensors, integration and interoperability.[100] Research is targeting intelligence collection and analysis, logistics, cyber operations, information operations, and semiautonomous and autonomous vehicles.[99] AI technologies enable coordination of sensors and effectors, threat detection and identification, marking of enemy positions, target acquisition, coordination and deconfliction of distributed Joint Fires between networked combat vehicles involving manned and unmanned teams.[100] AI was incorporated into military operations in Iraq and Syria.[99]\nIn 2023, the United States Department of Defense tested generative AI based on large language models to digitize and integrate data across the military.[101]\nIn the 2023 Israel–Hamas war, Israel used two AI systems to generate targets to strike: Habsora (translated: \"the gospel\") was used to compile a list of buildings to target, while \"Lavender\" produced a list of people. \"Lavender\" produced a list of 37,000 people to target.[102][103] The list of buildings to target included Gazan private homes of people that were suspected of affiliation to Hamas operatives. The combination of AI targeting technology with policy shift away from avoiding civilian targets resulted in unprecedented numbers of civilian deaths. IDF officials say the program addresses the previous issue of the air force running out of targets. Using Habsora, officials say that suspected and junior Hamas members homes significantly expand the \"AI target bank.\" An internal source describes the process as a “mass assassination factory”.[104][103]\nIn 2024, the U.S. military trained artificial intelligence to identify airstrike targets during its operations in Iraq and Syria.[105]\nWorldwide annual military spending on robotics rose from US$5.1 billion in 2010 to US$7.5 billion in 2015.[106][107] Military drones capable of autonomous action are in wide use.[108] Many researchers avoid military applications.[100]\nAI in healthcare is often used for classification, to evaluate a CT scan or electrocardiogram or to identify high-risk patients for population health. AI is helping with the high-cost problem of dosing. One study suggested that AI could save $16 billion. In 2016, a study reported that an AI-derived formula derived the proper dose of immunosuppressant drugs to give to transplant patients.[109] Current research has indicated that non-cardiac vascular illnesses are also being treated with artificial intelligence (AI). For certain disorders, AI algorithms can assist with diagnosis, recommended treatments, outcome prediction, and patient progress tracking. As AI technology advances, it is anticipated that it will become more significant in the healthcare industry.[110]\nThe early detection of diseases like cancer is made possible by AI algorithms, which diagnose diseases by analyzing complex sets of medical data. For example, the IBM Watson system might be used to comb through massive data such as medical records and clinical trials to help diagnose a problem.[111] Microsoft's AI project Hanover helps doctors choose cancer treatments from among the more than 800 medicines and vaccines.[112][113] Its goal is to memorize all the relevant papers to predict which (combinations of) drugs will be most effective for each patient. Myeloid leukemia is one target. Another study reported on an AI that was as good as doctors in identifying skin cancers.[114] Another project monitors multiple high-risk patients by asking each patient questions based on data acquired from doctor/patient interactions.[115] In one study done with transfer learning, an AI diagnosed eye conditions similar to an ophthalmologist and recommended treatment referrals.[116]\nAnother study demonstrated surgery with an autonomous robot. The team supervised the robot while it performed soft-tissue surgery, stitching together a pig's bowel judged better than a surgeon.[117]\nArtificial neural networks are used as clinical decision support systems for medical diagnosis,[118] such as in concept processing technology in EMR software.\nOther healthcare tasks thought suitable for an AI that are in development include:\nAI-enabled chatbots decrease the need for humans to perform basic call center tasks.[134]\nMachine learning in sentiment analysis can spot fatigue in order to prevent overwork.[134] Similarly, decision support systems can prevent industrial disasters and make disaster response more efficient.[135] For manual workers in material handling, predictive analytics may be used to reduce musculoskeletal injury.[136] Data collected from wearable sensors can improve workplace health surveillance, risk assessment, and research.[135][how?]\nAI can auto-code workers' compensation claims.[137][138] AI-enabled virtual reality systems can enhance safety training for hazard recognition.[135] AI can more efficiently detect accident near misses, which are important in reducing accident rates, but are often underreported.[139]\nAlphaFold 2 can determine the 3D structure of a (folded) protein in hours rather than the months required by earlier automated approaches and was used to provide the likely structures of all proteins in the human body and essentially all proteins known to science (more than 200 million).[140][141][142][143]\nMachine learning has been used for drug design. It has also been used for predicting molecular properties and exploring large chemical/reaction spaces.[144] Computer-planned syntheses via computational reaction networks, described as a platform that combines \"computational synthesis with AI algorithms to predict molecular properties\",[145] have been used to explore the origins of life on Earth,[146] drug-syntheses and developing routes for recycling 200 industrial waste chemicals into important drugs and agrochemicals (chemical synthesis design).[147] There is research about which types of computer-aided chemistry would benefit from machine learning.[148] It can also be used for \"drug discovery and development, drug repurposing, improving pharmaceutical productivity, and clinical trials\".[149] It has been used for the design of proteins with prespecified functional sites.[150][151]\nIt has been used with databases for the development of a 46-day process to design, synthesize and test a drug which inhibits enzymes of a particular gene, DDR1. DDR1 is involved in cancers and fibrosis which is one reason for the high-quality datasets that enabled these results.[152]\nThere are various types of applications for machine learning in decoding human biology, such as helping to map gene expression patterns to functional activation patterns[153] or identifying functional DNA motifs.[154] It is widely used in genetic research.[155]\nThere also is some use of machine learning in synthetic biology,[156][157] disease biology,[157] nanotechnology (e.g. nanostructured materials and bionanotechnology),[158][159] and materials science.[160][161][162]\nThere are also prototype robot scientists, including robot-embodied ones like the two Robot Scientists, which show a form of \"machine learning\" not commonly associated with the term.[163][164]\nSimilarly, there is research and development of biological \"wetware computers\" that can learn (e.g. for use as biosensors) and/or implantation into an organism's body (e.g. for use to control prosthetics).[165][166][167] Polymer-based artificial neurons operate directly in biological environments and define biohybrid neurons made of artificial and living components.[168][169]\nMoreover, if whole brain emulation is possible via both scanning and replicating the, at least, bio-chemical brain – as premised in the form of digital replication in The Age of Em, possibly using physical neural networks – that may have applications as or more extensive than e.g. valued human activities and may imply that society would face substantial moral choices, societal risks and ethical problems[170][171] such as whether (and how) such are built, sent through space and used compared to potentially competing e.g. potentially more synthetic and/or less human and/or non/less-sentient types of artificial/semi-artificial intelligence.[additional citation(s) needed] An alternative or additive approach to scanning are types of reverse engineering of the brain.[172][173]\nA subcategory of artificial intelligence is embodied,[174][175] some of which are mobile robotic systems that each consist of one or multiple robots that are able to learn in the physical world.\nHowever, biological computers, even if both highly artificial and intelligent, are typically distinguished from synthetic, often silicon-based, computers – they could however be combined or used for the design of either. Moreover, many tasks may be carried out inadequately by artificial intelligence even if its algorithms were transparent, understood, bias-free, apparently effective, and goal-aligned and its trained data sufficiently large and cleansed – such as in cases were the underlying or available metrics, values or data are inappropriate. Computer-aided is a phrase used to describe human activities that make use of computing as tool in more comprehensive activities and systems such as AI for narrow tasks or making use of such without substantially relying on its results (see also: human-in-the-loop).[citation needed] A study described the biological as a limitation of AI with \"as long as the biological system cannot be understood, formalized, and imitated, we will not be able to develop technologies that can mimic it\" and that if it was understood this does not mean there being \"a technological solution to imitate natural intelligence\".[176] Technologies that integrate biology and are often AI-based include biorobotics.\nArtificial intelligence is used in astronomy to analyze increasing amounts of available data[177][178] and applications, mainly for \"classification, regression, clustering, forecasting, generation, discovery, and the development of new scientific insights\" for example for discovering exoplanets, forecasting solar activity, and distinguishing between signals and instrumental effects in gravitational wave astronomy.[179] It could also be used for activities in space such as space exploration, including analysis of data from space missions, real-time science decisions of spacecraft, space debris avoidance,[180] and more autonomous operation.[181][182][183][178]\nIn the search for extraterrestrial intelligence (SETI), machine learning has been used in attempts to identify artificially generated electromagnetic waves in available data[184][185] – such as real-time observations[186] – and other technosignatures, e.g. via anomaly detection.[187] In ufology, the SkyCAM-5 project headed by Prof. Hakan Kayal[188] and the Galileo Project headed by Prof. Avi Loeb use machine learning to detect and classify peculiar types of UFOs.[189][190][191][192][193] The Galileo Project also seeks to detect two further types of potential extraterrestrial technological signatures with the use of AI: 'Oumuamua-like interstellar objects, and non-manmade artificial satellites.[194][195]\nLoeb has speculated that one type of technological equipment the project may detect could be \"AI astronauts\"[196] and in 2021 – in an opinion piece – that AI \"will\" \"supersede natural intelligence\",[197] while Martin Rees stated that there \"may\" be more civilizations than thought with the \"majority of them\" being artificial.[198] In particular, mid/far future or non-human applications of artificial intelligence could include advanced forms of artificial general intelligence that engages in space colonization or more narrow spaceflight-specific types of AI. In contrast, there have been concerns in relation to potential AGI or AI capable of embryo space colonization, or more generally natural intelligence-based space colonization, such as \"safety of encounters with an alien AI\",[199][200] suffering risks (or inverse goals),[201][202] moral license/responsibility in respect to colonization-effects,[203] or AI gone rogue (e.g. as portrayed with fictional David8 and HAL 9000). See also: space law and space ethics. Loeb has described the possibility of \"AI astronauts\" that engage in \"supervised evolution\" (see also: directed evolution, uplift, directed panspermia and space colonization).[204]\nIt can also be used to produce datasets of spectral signatures of molecules that may be involved in the atmospheric production or consumption of particular chemicals – such as phosphine possibly detected on Venus – which could prevent miss assignments and, if accuracy is improved, be used in future detections and identifications of molecules on other planets.[205]\nIn April 2024, the Scientific Advice Mechanism to the European Commission published advice[206] including a comprehensive evidence review of the opportunities and challenges posed by artificial intelligence in scientific research.\nAs benefits, the evidence review[207] highlighted:\nAs challenges:\nMachine learning can help to restore and attribute ancient texts.[208] It can help to index texts for example to enable better and easier searching[209] and classification of fragments.[210]\n\nArtificial intelligence can also be used to investigate genomes to uncover genetic history, such as interbreeding between archaic and modern humans by which for example the past existence of a ghost population, not Neanderthal or Denisovan, was inferred.[211] \nIt can also be used for \"non-invasive and non-destructive access to internal structures of archaeological remains\".[212] A deep learning system was reported to learn intuitive physics from visual data (of virtual 3D environments) based on an unpublished approach inspired by studies of visual cognition in infants.[213][214] Other researchers have developed a machine learning algorithm that could discover sets of basic variables of various physical systems and predict the systems' future dynamics from video recordings of their behavior.[215][216] In the future, it may be possible that such can be used to automate the discovery of physical laws of complex systems.[215]\nAI could be used for materials optimization and discovery such as the discovery of stable materials and the prediction of their crystal structure.[217][218][219]\nIn November 2023, researchers at Google DeepMind and Lawrence Berkeley National Laboratory announced that they had developed an AI system known as GNoME. This system has contributed to materials science by discovering over 2 million new materials within a relatively short timeframe. GNoME employs deep learning techniques to efficiently explore potential material structures, achieving a significant increase in the identification of stable inorganic crystal structures. The system's predictions were validated through autonomous robotic experiments, demonstrating a noteworthy success rate of 71%. The data of newly discovered materials is publicly available through the Materials Project database, offering researchers the opportunity to identify materials with desired properties for various applications. This development has implications for the future of scientific discovery and the integration of AI in material science research, potentially expediting material innovation and reducing costs in product development. The use of AI and deep learning suggests the possibility of minimizing or eliminating manual lab experiments and allowing scientists to focus more on the design and analysis of unique compounds.[220][221][222]\nMachine learning is used in diverse types of reverse engineering. For example, machine learning has been used to reverse engineer a composite material part, enabling unauthorized production of high quality parts,[223] and for quickly understanding the behavior of malware.[224][225][226] It can be used to reverse engineer artificial intelligence models.[227] It can also design components by engaging in a type of reverse engineering of not-yet existent virtual components such as inverse molecular design for particular desired functionality[228] or protein design for prespecified functional sites.[150][151] Biological network reverse engineering could model interactions in a human understandable way, e.g. bas on time series data of gene expression levels.[229]\nAI is a mainstay of law-related professions. Algorithms and machine learning do some tasks previously done by entry-level lawyers.[230] While its use is common, it is not expected to replace most work done by lawyers in the near future.[231]\nThe electronic discovery industry uses machine learning to reduce manual searching.[232]\nCOMPAS is a commercial system used by U.S. courts to assess the likelihood of recidivism.[233]\nOne concern relates to algorithmic bias, AI programs may become biased after processing data that exhibits bias.[234] ProPublica claims that the average COMPAS-assigned recidivism risk level of black defendants is significantly higher than that of white defendants.[233]\nIn 2019, the city of Hangzhou, China established a pilot program artificial intelligence-based Internet Court to adjudicate disputes related to ecommerce and internet-related intellectual property claims.[235]: 124  Parties appear before the court via videoconference and AI evaluates the evidence presented and applies relevant legal standards.[235]: 124 \nAnother application of AI is in human resources. AI can screen resumes and rank candidates based on their qualifications, predict candidate success in given roles, and automate repetitive communication tasks via chatbots.[236]\nAI has simplified the recruiting /job search process for both recruiters and job seekers. According to Raj Mukherjee from Indeed, 65% of job searchers search again within 91 days after hire. An AI-powered engine streamlines the complexity of job hunting by assessing information on job skills, salaries, and user tendencies, matching job seekers to the most relevant positions. Machine intelligence calculates appropriate wages and highlights resume information for recruiters using NLP, which extracts relevant words and phrases from text. Another application is an AI resume builder that compiles a CV in 5 minutes.[237] Chatbots assist website visitors and refine workflows.\nAI underlies avatars (automated online assistants) on web pages.[238] It can reduce operation and training costs.[238] Pypestream automated customer service for its mobile application to streamline communication with customers.[239]\nA Google app analyzes language and converts speech into text. The platform can identify angry customers through their language and respond appropriately.[240] Amazon uses a chatbot for customer service that can perform tasks like checking the status of an order, cancelling orders, offering refunds and connecting the customer with a human representative.[241]\nIn the hospitality industry, AI is used to reduce repetitive tasks, analyze trends, interact with guests, and predict customer needs.[242] AI hotel services come in the form of a chatbot,[243] application, virtual voice assistant and service robots.\nAI applications analyze media content such as movies, TV programs, advertisement videos or user-generated content. The solutions often involve computer vision.\nTypical scenarios include the analysis of images using object recognition or face recognition techniques, or the analysis of video for scene recognizing scenes, objects or faces. AI-based media analysis can facilitate media search, the creation of descriptive keywords for content, content policy monitoring (such as verifying the suitability of content for a particular TV viewing time), speech to text for archival or other purposes, and the detection of logos, products or celebrity faces for ad placement.\nDeep-fakes can be used for comedic purposes but are better known for fake news and hoaxes.\nIn January 2016,[256] the Horizon 2020 program financed the InVID Project[257][258] to help journalists and researchers detect fake documents, made available as browser plugins.[259][260]\nIn June 2016, the visual computing group of the Technical University of Munich and from Stanford University developed Face2Face,[261] a program that animates photographs of faces, mimicking the facial expressions of another person. The technology has been demonstrated animating the faces of people including Barack Obama and Vladimir Putin. Other methods have been demonstrated based on deep neural networks, from which the name deep fake was taken.\nIn September 2018, U.S. Senator Mark Warner proposed to penalize social media companies that allow sharing of deep-fake documents on their platforms.[262]\nIn 2018, Darius Afchar and Vincent Nozick found a way to detect faked content by analyzing the mesoscopic properties of video frames.[263] DARPA gave 68 million dollars to work on deep-fake detection.[263]\nAudio deepfakes[264][265] and AI software capable of detecting deep-fakes and cloning human voices have been developed.[266][267]\nRespeecher is a program that enables one person to speak with the voice of another.\nAI algorithms have been used to detect deepfake videos.[268][269]\nArtificial Intelligence is also starting to be used in video production, with tools and softwares being developed that utilize generative AI in order to create new video, or alter existing video. Some of the major tools that are being used in these processes currently are DALL-E, Mid-journey, and Runway.[270]  Way mark Studios utilized the tools offered by both DALL-E and Mid-journey to create a fully AI generated film called The Frost in the summer of 2023.[270] Way mark Studios is experimenting with using these AI tools to generate advertisements and commercials for companies in mere seconds.[270]  Yves Bergquist, a director of the AI \u0026 Neuroscience in Media Project at USC's Entertainment Technology Center, says post production crews in Hollywood are already using generative AI, and predicts that in the future more companies will embrace this new technology.[271]\nAI has been used to compose music of various genres.\nDavid Cope created an AI called Emily Howell that managed to become well known in the field of algorithmic computer music.[272] The algorithm behind Emily Howell is registered as a US patent.[273]\nIn 2012, AI Iamus created the first complete classical album.[274]\nAIVA (Artificial Intelligence Virtual Artist), composes symphonic music, mainly classical music for film scores.[275] It achieved a world first by becoming the first virtual composer to be recognized by a musical professional association.[276]\nMelomics creates computer-generated music for stress and pain relief.[277]\nAt Sony CSL Research Laboratory, the Flow Machines software creates pop songs by learning music styles from a huge database of songs. It can compose in multiple styles.\nThe Watson Beat uses reinforcement learning and deep belief networks to compose music on a simple seed input melody and a select style. The software was open sourced[278] and musicians such as Taryn Southern[279] collaborated with the project to create music.\nSouth Korean singer Hayeon's debut song, \"Eyes on You\" was composed using AI which was supervised by real composers, including NUVO.[280]\nNarrative Science sells computer-generated news and reports. It summarizes sporting events based on statistical data from the game. It also creates financial reports and real estate analyses.[281] Automated Insights generates personalized recaps and previews for Yahoo Sports Fantasy Football.[282]\nYseop, uses AI to turn structured data into natural language comments and recommendations. Yseop writes financial reports, executive summaries, personalized sales or marketing documents and more in multiple languages, including English, Spanish, French, and German.[283]\nTALESPIN made up stories similar to the fables of Aesop. The program started with a set of characters who wanted to achieve certain goals. The story narrated their attempts to satisfy these goals.[citation needed] Mark Riedl and Vadim Bulitko asserted that the essence of storytelling was experience management, or \"how to balance the need for a coherent story progression with user agency, which is often at odds\".[284]\nWhile AI storytelling focuses on story generation (character and plot), story communication also received attention. In 2002, researchers developed an architectural framework for narrative prose generation. They faithfully reproduced text variety and complexity on stories such as Little Red Riding Hood.[285] In 2016, a Japanese AI co-wrote a short story and almost won a literary prize.[286]\nSouth Korean company Hanteo Global uses a journalism bot to write articles.[287]\nLiterary authors are also exploring uses of AI. An example is David Jhave Johnston's work ReRites (2017-2019), where the poet created a daily rite of editing the poetic output of a neural network to create a series of performances and publications.\nIn 2010, artificial intelligence used baseball statistics to automatically generate news articles. This was launched by The Big Ten Network using a software from Narrative Science.[288]\nAfter being unable to cover every Minor League Baseball game with a large team of people, Associated Press collaborated with Automated Insights in 2016 to create game recaps that were automated by artificial intelligence.[289]\nUOL in Brazil expanded the use of AI in their writing. Rather than just generating news stories, they programmed the AI to include commonly searched words on Google.[289]\nEl Pais, a Spanish news site that covers many things including sports, allows users to make comments on each news article. They use the Perspective API to moderate these comments and if the software deems a comment to contain toxic language, the commenter will be forced to change their comment in order to publish it.[289]\nA local Dutch media group used AI to create automatic coverage of amateur soccer, set to cover 60,000 games in just a single season. NDC partnered with United Robots to create this algorithm and cover what would have never been able to be done before without an extremely large team.[289]\nLede AI has been used in 2023 to take scores from high school football games to generate stories automatically for the local news paper. This was met with a lot of criticism from readers for the very robotic diction that was published. With some descriptions of games being a \"close encounter of the athletic kind,\" readers were not pleased and let the publishing company, Gannett, know on social media. Gannett has since halted their used of Lede AI until they come up with a solution for what they call an experiment.[290]\n Millions of its articles have been edited by bots[294] which however are usually not artificial intelligence software. Many AI platforms use Wikipedia data,[295] mainly for training machine learning applications. There is research and development of various artificial intelligence applications for Wikipedia such as for identifying outdated sentences,[296] detecting covert vandalism[297] or recommending articles and tasks to new editors.\nMachine translation .mw-parser-output div.crossreference{padding-left:0}(see above) has also be used for translating Wikipedia articles and could play a larger role in creating, updating, expanding, and generally improving articles in the future. A content translation tool allows editors of some Wikipedias to more easily translate articles across several select languages.[298][299]\nIn video games, AI is routinely used to generate behavior in non-player characters (NPCs). In addition, AI is used for pathfinding. Some researchers consider NPC AI in games to be a \"solved problem\" for most production tasks.[who?] Games with less typical AI include the AI director of Left 4 Dead (2008) and the neuroevolutionary training of platoons in Supreme Commander 2 (2010).[300][301] AI is also used in Alien Isolation (2014) as a way to control the actions the Alien will perform next.[302]\nKinect, which provides a 3D body–motion interface for the Xbox 360 and the Xbox One, uses algorithms that emerged from AI research.[303][which?]\nAI has been used to produce visual art. The first AI art program, called AARON, was developed by Harold Cohen in 1968[304] with the goal of being able to code the act of drawing. It started by creating simple black and white drawings, and later to paint using special brushes and dyes that were chosen by the program itself without mediation from Cohen.[305]\nAI platforms such as \"DALL-E\",[306] Stable Diffusion,[306] Imagen,[307] and Midjourney[308] have been used for generating visual images from inputs such as text or other images.[309] Some AI tools allow users to input images and output changed versions of that image, such as to display an object or product in different environments. AI image models can also attempt to replicate the specific styles of artists, and can add visual complexity to rough sketches.\nSince their design in 2014, generative adversarial networks (GANs) have been used by AI artists. GAN computer programming, generates technical images through machine learning frameworks that surpass the need for human operators.[304] Examples of GAN programs that generate art include Artbreeder and DeepDream.\nIn addition to the creation of original art, research methods that utilize AI have been generated to quantitatively analyze digital art collections. Although the main goal of the large-scale digitization of artwork in the past few decades was to allow for accessibility and exploration of these collections, the use of AI in analyzing them has brought about new research perspectives.[310]\nTwo computational methods, close reading and distant viewing, are the typical approaches used to analyze digitized art.[311] While distant viewing includes the analysis of large collections, close reading involves one piece of artwork.\nAI has been in use since the early 2000s, most notably by a system designed by Pixar called \"Genesis\".[312] It was designed to learn algorithms and create 3D models for its characters and props. Notable movies that used this technology included Up and The Good Dinosaur.[313] AI has been used less ceremoniously in recent years. In 2023, it was revealed Netflix of Japan was using AI to generate background images for their upcoming show to be met with backlash online.[314]  In recent years, motion capture became an easily accessible form of AI animation. For example, Move AI is a program built to capture any human movement and reanimate it in its animation program using learning AI.[315]\nPower electronics converters are used in renewable energy, energy storage, electric vehicles and high-voltage direct current transmission. These converters are failure-prone, which can interrupt service and require costly maintenance or catastrophic consequences in mission critical applications.[citation needed] AI can guide the design process for reliable power electronics converters, by calculating exact design parameters that ensure the required lifetime.[316]\nMachine learning can be used for energy consumption prediction and scheduling, e.g. to help with renewable energy intermittency management (see also: smart grid and climate change mitigation in the power grid).[317][318][319][320][better source needed]\nMany telecommunications companies make use of heuristic search to manage their workforces. For example, BT Group deployed heuristic search[321] in an application that schedules 20,000 engineers. Machine learning is also used for speech recognition (SR), including of voice-controlled devices, and SR-related transcription, including of videos.[322][323]\nArtificial intelligence has been combined with digital spectrometry by IdeaCuria Inc.,[324][325] enable applications such as at-home water quality monitoring.\nIn the 1990s early AIs controlled Tamagotchis and Giga Pets, the Internet, and the first widely released robot, Furby. Aibo was a domestic robot in the form of a robotic dog with intelligent features and autonomy.\nMattel created an assortment of AI-enabled toys that \"understand\" conversations, give intelligent responses, and learn.[326]\nOil and gas companies have used artificial intelligence tools to automate functions, foresee equipment issues, and increase oil and gas output.[327][328]\nAI in transport is expected to provide safe, efficient, and reliable transportation while minimizing the impact on the environment and communities. The major development challenge is the complexity of transportation systems that involves independent components and parties, with potentially conflicting objectives.[329]\nAI-based fuzzy logic controllers operate gearboxes. For example, the 2006 Audi TT, VW Touareg [citation needed] and VW Caravell feature the DSP transmission. A number of Škoda variants (Škoda Fabia) include a fuzzy logic-based controller. Cars have AI-based driver-assist features such as self-parking and adaptive cruise control.\nThere are also prototypes of autonomous automotive public transport vehicles such as electric mini-buses[330][331][332][333] as well as autonomous rail transport in operation.[334][335][336]\nThere also are prototypes of autonomous delivery vehicles, sometimes including delivery robots.[337][338][339][340][341][342][343]\nTransportation's complexity means that in most cases training an AI in a real-world driving environment is impractical. Simulator-based testing can reduce the risks of on-road training.[344]\nAI underpins self-driving vehicles. Companies involved with AI include Tesla, Waymo, and General Motors. AI-based systems control functions such as braking, lane changing, collision prevention, navigation and mapping.[345]\nAutonomous trucks are in the testing phase. The UK government passed legislation to begin testing of autonomous truck platoons in 2018.[346] A group of autonomous trucks follow closely behind each other. German corporation Daimler is testing its Freightliner Inspiration.[347]\nAutonomous vehicles require accurate maps to be able to navigate between destinations.[348] Some autonomous vehicles do not allow human drivers (they have no steering wheels or pedals).[349]\nAI has been used to optimize traffic management, which reduces wait times, energy use, and emissions by as much as 25 percent.[350]\nSmart traffic lights have been developed at Carnegie Mellon since 2009.  Professor Stephen Smith has started a company since then Surtrac that has installed smart traffic control systems in 22 cities.  It costs about $20,000 per intersection to install. Drive time has been reduced by 25% and traffic jam waiting time has been reduced by 40% at the intersections it has been installed.[351]\nThe Royal Australian Air Force (RAAF) Air Operations Division (AOD) uses AI for expert systems. AIs operate as surrogate operators for combat and training simulators, mission management aids, support systems for tactical decision making, and post processing of the simulator data into symbolic summaries.[352]\nAircraft simulators use AI for training aviators. Flight conditions can be simulated that allow pilots to make mistakes without risking themselves or expensive aircraft. Air combat can also be simulated.\nAI can also be used to operate planes analogously to their control of ground vehicles. Autonomous drones can fly independently or in swarms.[353]\nAOD uses the Interactive Fault Diagnosis and Isolation System, or IFDIS, which is a rule-based expert system using information from TF-30 documents and expert advice from mechanics that work on the TF-30. This system was designed to be used for the development of the TF-30 for the F-111C. The system replaced specialized workers. The system allowed regular workers to communicate with the system and avoid mistakes, miscalculations, or having to speak to one of the specialized workers.\nSpeech recognition allows traffic controllers to give verbal directions to drones.\nArtificial intelligence supported design of aircraft,[354] or AIDA, is used to help designers in the process of creating conceptual designs of aircraft. This program allows the designers to focus more on the design itself and less on the design process. The software also allows the user to focus less on the software tools. The AIDA uses rule-based systems to compute its data. This is a diagram of the arrangement of the AIDA modules. Although simple, the program is proving effective.\nIn 2003 a Dryden Flight Research Center project created software that could enable a damaged aircraft to continue flight until a safe landing can be achieved.[355] The software compensated for damaged components by relying on the remaining undamaged components.[356]\nThe 2016 Intelligent Autopilot System combined apprenticeship learning and behavioral cloning whereby the autopilot observed low-level actions required to maneuver the airplane and high-level strategy used to apply those actions.[357]\nNeural networks are used by situational awareness systems in ships and boats.[358] There also are autonomous boats.\nAutonomous ships that monitor the ocean, AI-driven satellite data analysis, passive acoustics[359] or remote sensing and other applications of environmental monitoring make use of machine learning.[360][361][362][183]\nFor example, \"Global Plastic Watch\" is an AI-based satellite monitoring-platform for analysis/tracking of plastic waste sites to help prevention of plastic pollution – primarily ocean pollution – by helping identify who and where mismanages plastic waste, dumping it into oceans.[363][364]\nMachine learning can be used to spot early-warning signs of disasters and environmental issues, possibly including natural pandemics,[365][366] earthquakes,[367][368][369] landslides,[370] heavy rainfall,[371] long-term water supply vulnerability,[372] tipping-points of ecosystem collapse,[373] cyanobacterial bloom outbreaks,[374] and droughts.[375][376][377]\nAI can be used for real-time code completion, chat, and automated test generation. These tools are typically integrated with editors and IDEs as plugins. They differ in functionality, quality, speed, and approach to privacy.[378]\nCode suggestions could be incorrect, and should be carefully reviewed by software developers before accepted.\nGitHub Copilot is an artificial intelligence model developed by GitHub and OpenAI that is able to autocomplete code in multiple programming languages.[379] Price for individuals: $10/mo or $100/yr, with one free month trial.\nTabnine was created by Jacob Jackson and was originally owned by Tabnine company. In late 2019, Tabnine was acquired by Codota.[380] Tabnine tool is available as plugin to most popular IDEs. It offers multiple pricing options, including limited \"starter\" free version.[381]\nCodiumAI by CodiumAI, small startup in Tel Aviv, offers automated test creation. Currently supports Python, JS, and TS.[382]\nGhostwriter by Replit offers code completion and chat.[383] They have multiple pricing plans, including a free one and a \"Hacker\" plan for $7/month.\nCodeWhisperer by Amazon collects individual users' content, including files open in the IDE. They claim to focus on security both during transmission and when storing.[384] Individual plan is free, professional plan is $19/user/month.\nOther tools: SourceGraph Cody, CodeCompleteFauxPilot, Tabby[378]\nAI can be used to create other AIs. For example, around November 2017, Google's AutoML project to evolve new neural net topologies created NASNet, a system optimized for ImageNet and POCO F1. NASNet's performance exceeded all previously published performance on ImageNet.[385]\nMachine learning has been used for noise-cancelling in quantum technology,[386] including quantum sensors.[387] Moreover, there is substantial research and development of using quantum computers with machine learning algorithms. For example, there is a prototype, photonic, .mw-parser-output .tooltip-dotted{border-bottom:1px dotted;cursor:help}quantum memristive device for neuromorphic (quantum-)computers (NC)/artificial neural networks and NC-using quantum materials with some variety of potential neuromorphic computing-related applications,[388][389] and quantum machine learning is a field with some variety of applications under development. AI could be used for quantum simulators which may have the application of solving physics and chemistry[390][391] problems as well as for quantum annealers for training of neural networks for AI applications.[392] There may also be some usefulness in chemistry, e.g. for drug discovery, and in materials science, e.g. for materials optimization/discovery (with possible relevance to quantum materials manufacturing[218][219]).[393][394][395][better source needed]\nAI researchers have created many tools to solve the most difficult problems in computer science. Many of their inventions have been adopted by mainstream computer science and are no longer considered AI. All of the following were originally developed in AI laboratories:[396]\nAn optical character reader is used in the extraction of data in business documents like invoices and receipts. It can also be used in business contract documents e.g. employment agreements to extract critical data like employment terms, delivery terms, termination clauses, etc.[397]\nArtificial intelligence in architecture describes the use of artificial intelligence in automation, design and planning in the architectural process or in assisting human skills in the field of architecture. Artificial Intelligence is thought to potentially lead to and ensue major changes in Architecture.[398][399][400]  \nAI in architecture has created a way for architects to create things beyond human understanding. AI implementation of machine learning text-to-render technologies, like DALL-E and stable Diffusion, gives power to visualization complex.[401] \nAI allows designers to demonstrate their creativity and even invent new ideas while designing. In future, AI will not replace architects; instead, it will improve the speed of translating ideas sketching.[401]"}
{"url":"https://en.wikipedia.org/wiki/Android_(robot)","text":"An android is a humanoid robot[1] or other artificial being[2][3][4] often made from a flesh-like material.[2] Historically, androids were completely within the domain of science fiction and frequently seen in film and television, but advances in robot technology now allow the design of functional and realistic[5] humanoid robots.[6]\nThe Oxford English Dictionary traces the earliest use (as \"Androides\") to Ephraim Chambers' 1728 Cyclopaedia, in reference to an automaton that St. Albertus Magnus allegedly created.[3][7] By the late 1700s, \"androides\", elaborate mechanical devices resembling humans performing human activities, were displayed in exhibit halls.[8][1]\nThe term \"android\" appears in US patents as early as 1863 in reference to miniature human-like toy automatons.[9] The term android was used in a more modern sense by the French author Auguste Villiers de l'Isle-Adam in his work Tomorrow's Eve (1886), featuring an artificial humanoid robot named Hadaly.[3] The term made an impact into English pulp science fiction starting from Jack Williamson's The Cometeers (1936) and the distinction between mechanical robots and fleshy androids was popularized by Edmond Hamilton's Captain Future stories (1940–1944).[3]\nAlthough Karel Čapek's robots in R.U.R. (Rossum's Universal Robots) (1921)—the play that introduced the word robot to the world—were organic artificial humans, the word \"robot\" has come to primarily refer to mechanical humans, animals, and other beings.[3] The term \"android\" can mean either one of these,[3] while a cyborg (\"cybernetic organism\" or \"bionic man\") would be a creature that is a combination of organic and mechanical parts.\nThe term \"droid\", popularized by George Lucas in the original Star Wars film and now used widely within science fiction, originated as an abridgment of \"android\", but has been used by Lucas and others to mean any robot, including distinctly non-human form machines like R2-D2. The word \"android\" was used in Star Trek: The Original Series episode \"What Are Little Girls Made Of?\" The abbreviation \"andy\", coined as a pejorative by writer Philip K. Dick in his novel Do Androids Dream of Electric Sheep?, has seen some further usage, such as within the TV series Total Recall 2070.[10]\nWhile the term \"android\" is used in reference to human-looking robots in general (not necessarily male-looking humanoid robots), a robot with a female appearance can also be referred to as a gynoid. Besides one can refer to robots without alluding to their sexual appearance by calling them anthrobots (a portmanteau of anthrōpos and robot; see anthrobotics) or anthropoids (short for anthropoid robots; the term humanoids is not appropriate because it is already commonly used to refer to human-like organic species in the context of science fiction, futurism and speculative astrobiology).[11]\nAuthors have used the term android in more diverse ways than robot or cyborg. In some fictional works, the difference between a robot and android is only superficial, with androids being made to look like humans on the outside but with robot-like internal mechanics.[3] In other stories, authors have used the word \"android\" to mean a wholly organic, yet artificial, creation.[3] Other fictional depictions of androids fall somewhere in between.[3]\nEric G. Wilson, who defines an android as a \"synthetic human being\", distinguishes between three types of android, based on their body's composition:\nAlthough human morphology is not necessarily the ideal form for working robots, the fascination in developing robots that can mimic it can be found historically in the assimilation of two concepts: simulacra (devices that exhibit likeness) and automata (devices that have independence).\nSeveral projects aiming to create androids that look, and, to a certain degree, speak or act like a human being have been launched or are underway.\nJapanese robotics have been leading the field since the 1970s.[12] Waseda University initiated the WABOT project in 1967, and in 1972 completed the WABOT-1, the first android, a full-scale humanoid intelligent robot.[13][14] Its limb control system allowed it to walk with the lower limbs, and to grip and transport objects with hands, using tactile sensors. Its vision system allowed it to measure distances and directions to objects using external receptors, artificial eyes and ears. And its conversation system allowed it to communicate with a person in Japanese, with an artificial mouth.[14][15][16]\nIn 1984, WABOT-2 was revealed, and made a number of improvements. It was capable of playing the organ. Wabot-2 had ten fingers and two feet, and was able to read a score of music. It was also able to accompany a person.[17] In 1986, Honda began its humanoid research and development program, to create humanoid robots capable of interacting successfully with humans.[18]\nThe Intelligent Robotics Lab, directed by Hiroshi Ishiguro at Osaka University, and the Kokoro company demonstrated the Actroid at Expo 2005 in Aichi Prefecture, Japan and released the Telenoid R1 in 2010. In 2006, Kokoro developed a new DER 2 android. The height of the human body part of DER2 is 165 cm. There are 47 mobile points. DER2 can not only change its expression but also move its hands and feet and twist its body. The \"air servosystem\" which Kokoro developed originally is used for the actuator. As a result of having an actuator controlled precisely with air pressure via a servosystem, the movement is very fluid and there is very little noise. DER2 realized a slimmer body than that of the former version by using a smaller cylinder. Outwardly DER2 has a more beautiful proportion. Compared to the previous model, DER2 has thinner arms and a wider repertoire of expressions. Once programmed, it is able to choreograph its motions and gestures with its voice.\nThe Intelligent Mechatronics Lab, directed by Hiroshi Kobayashi at the Tokyo University of Science, has developed an android head called Saya, which was exhibited at Robodex 2002 in Yokohama, Japan. There are several other initiatives around the world involving humanoid research and development at this time, which will hopefully introduce a broader spectrum of realized technology in the near future. Now Saya is working at the Science University of Tokyo as a guide.\nThe Waseda University (Japan) and NTT docomo's manufacturers have succeeded in creating a shape-shifting robot WD-2. It is capable of changing its face. At first, the creators decided the positions of the necessary points to express the outline, eyes, nose, and so on of a certain person. The robot expresses its face by moving all points to the decided positions, they say. The first version of the robot was first developed back in 2003. After that, a year later, they made a couple of major improvements to the design. The robot features an elastic mask made from the average head dummy. It uses a driving system with a 3DOF unit. The WD-2 robot can change its facial features by activating specific facial points on a mask, with each point possessing three degrees of freedom. This one has 17 facial points, for a total of 56 degrees of freedom. As for the materials they used, the WD-2's mask is fabricated with a highly elastic material called Septom, with bits of steel wool mixed in for added strength. Other technical features reveal a shaft driven behind the mask at the desired facial point, driven by a DC motor with a simple pulley and a slide screw. Apparently, the researchers can also modify the shape of the mask based on actual human faces. To \"copy\" a face, they need only a 3D scanner to determine the locations of an individual's 17 facial points. After that, they are then driven into position using a laptop and 56 motor control boards. In addition, the researchers also mention that the shifting robot can even display an individual's hair style and skin color if a photo of their face is projected onto the 3D Mask.\nProf Nadia Thalmann, a Nanyang Technological University scientist, directed efforts of the Institute for Media Innovation along with the School of Computer Engineering in the development of a social robot, Nadine. Nadine is powered by software similar to Apple's Siri or Microsoft's Cortana. Nadine may become a personal assistant in offices and homes in future, or she may become a companion for the young and the elderly.\nAssoc Prof Gerald Seet from the School of Mechanical \u0026 Aerospace Engineering and the BeingThere Centre led a three-year R\u0026D development in tele-presence robotics, creating EDGAR. A remote user can control EDGAR with the user's face and expressions displayed on the robot's face in real time. The robot also mimics their upper body movements.\n[19]\nKITECH researched and developed EveR-1, an android interpersonal communications model capable of emulating human emotional expression via facial \"musculature\" and capable of rudimentary conversation, having a vocabulary of around 400 words. She is 160 cm tall and weighs 50 kg, matching the average figure of a Korean woman in her twenties. EveR-1's name derives from the Biblical Eve, plus the letter r for robot. EveR-1's advanced computing processing power enables speech recognition and vocal synthesis, at the same time processing lip synchronization and visual recognition by 90-degree micro-CCD cameras with face recognition technology. An independent microchip inside her artificial brain handles gesture expression, body coordination, and emotion expression. Her whole body is made of highly advanced synthetic jelly silicon and with 60 artificial joints in her face, neck, and lower body; she is able to demonstrate realistic facial expressions and sing while simultaneously dancing. In South Korea, the Ministry of Information and Communication had an ambitious plan to put a robot in every household by 2020.[20] Several robot cities have been planned for the country: the first will be built in 2016 at a cost of 500 billion won (US$440 million), of which 50 billion is direct government investment.[21] The new robot city will feature research and development centers for manufacturers and part suppliers, as well as exhibition halls and a stadium for robot competitions. The country's new Robotics Ethics Charter will establish ground rules and laws for human interaction with robots in the future, setting standards for robotics users and manufacturers, as well as guidelines on ethical standards to be programmed into robots to prevent human abuse of robots and vice versa.[22]\nWalt Disney and a staff of Imagineers created Great Moments with Mr. Lincoln that debuted at the 1964 New York World's Fair.[23]\nDr. William Barry, an Education Futurist and former visiting West Point Professor of Philosophy and Ethical Reasoning at the United States Military Academy, created an AI android character named \"Maria Bot\". This Interface AI android was named after the infamous fictional robot Maria in the 1927 film Metropolis, as a well-behaved distant relative. Maria Bot is the first AI Android Teaching Assistant at the university level.[24][25] Maria Bot has appeared as a keynote speaker as a duo with Barry for a TEDx talk in Everett, Washington in February 2020.[26]\nResembling a human from the shoulders up, Maria Bot is a virtual being android that has complex facial expressions and head movement and engages in conversation about a variety of subjects. She uses AI to process and synthesize information to make her own decisions on how to talk and engage. She collects data through conversations, direct data inputs such as books or articles, and through internet sources.\nMaria Bot was built by an international high-tech company for Barry to help improve education quality and eliminate education poverty. Maria Bot is designed to create new ways for students to engage and discuss ethical issues raised by the increasing presence of robots and artificial intelligence. Barry also uses Maria Bot to demonstrate that programming a robot with life-affirming, ethical framework makes them more likely to help humans to do the same.[27]\nMaria Bot is an ambassador robot for good and ethical AI technology.[28]\nHanson Robotics, Inc., of Texas and KAIST produced an android portrait of Albert Einstein, using Hanson's facial android technology mounted on KAIST's life-size walking bipedal robot body. This Einstein android, also called \"Albert Hubo\", thus represents the first full-body walking android in history.[29] Hanson Robotics, the FedEx Institute of Technology,[30] and the University of Texas at Arlington also developed the android portrait of sci-fi author Philip K. Dick (creator of Do Androids Dream of Electric Sheep?, the basis for the film Blade Runner), with full conversational capabilities that incorporated thousands of pages of the author's works.[31] In 2005, the PKD android won a first-place artificial intelligence award from AAAI.\nAndroids are a staple of science fiction. Isaac Asimov pioneered the fictionalization of the science of robotics and artificial intelligence, notably in his 1950s series I, Robot.[32] One thing common to most fictional androids is that the real-life technological challenges associated with creating thoroughly human-like robots—such as the creation of strong artificial intelligence—are assumed to have been solved.[33] Fictional androids are often depicted as mentally and physically equal or superior to humans—moving, thinking and speaking as fluidly as them.[3][33]\nThe tension between the nonhuman substance and the human appearance—or even human ambitions—of androids is the dramatic impetus behind most of their fictional depictions.[4][33] Some android heroes seek, like Pinocchio, to become human, as in the film Bicentennial Man,[33] or Data in Star Trek: The Next Generation. Others, as in the film Westworld, rebel against abuse by careless humans.[33] Android hunter Deckard in Do Androids Dream of Electric Sheep? and its film adaptation Blade Runner discovers that his targets appear to be, in some ways, more \"human\" than he is.[33] Android stories, therefore, are not essentially stories \"about\" androids; they are stories about the human condition and what it means to be human.[33]\nOne aspect of writing about the meaning of humanity is to use discrimination against androids as a mechanism for exploring racism in society, as in Blade Runner.[34] Perhaps the clearest example of this is John Brunner's 1968 novel Into the Slave Nebula, where the blue-skinned android slaves are explicitly shown to be fully human.[35] More recently, the androids Bishop and Annalee Call in the films Aliens and Alien Resurrection are used as vehicles for exploring how humans deal with the presence of an \"Other\".[36] The 2018 video game Detroit: Become Human also explores how androids are treated as second class citizens in a near future society.\nFemale androids, or \"gynoids\", are often seen in science fiction, and can be viewed as a continuation of the long tradition of men attempting to create the stereotypical \"perfect woman\".[37] Examples include the Greek myth of Pygmalion and the female robot Maria in Fritz Lang's Metropolis. Some gynoids, like Pris in Blade Runner, are designed as sex-objects, with the intent of \"pleasing men's violent sexual desires\",[38] or as submissive, servile companions, such as in The Stepford Wives. Fiction about gynoids has therefore been described as reinforcing \"essentialist ideas of femininity\",[39] although others have suggested that the treatment of androids is a way of exploring racism and misogyny in society.[40]\nThe 2015 Japanese film Sayonara, starring Geminoid F, was promoted as \"the first movie to feature an android performing opposite a human actor\".[41]"}
{"url":"https://en.wikipedia.org/wiki/Robotics","text":"Robotics is the interdisciplinary study and practice of the design, construction, operation, and use of robots.[1]\nWithin mechanical engineering, robotics is the design and construction of the physical structures of robots, while in computer science, robotics focuses on robotic automation algorithms. Other disciplines contributing to robotics include electrical, control, software, information, electronic, telecommunication, computer, mechatronic, and materials engineering. \nThe goal of most robotics is to design machines that can help and assist humans. Many robots are built to do jobs that are hazardous to people, such as finding survivors in unstable ruins, and exploring space, mines and shipwrecks. Others replace people in jobs that are boring, repetitive, or unpleasant, such as cleaning, monitoring, transporting, and assembling. Today, robotics is a rapidly growing field, as technological advances continue; researching, designing, and building new robots serve various practical purposes.\nThere are many types of robots; they are used in many different environments and for many different uses. Although diverse in application and form, they all share three basic aspects when it comes to their design and construction:\nAs more and more robots are designed for specific tasks, this method of classification becomes more relevant. For example, many robots are designed for assembly work, which may not be readily adaptable for other applications. They are termed \"assembly robots\". For seam welding, some suppliers provide complete welding systems with the robot i.e. the welding equipment along with other material handling facilities like turntables, etc. as an integrated unit. Such an integrated robotic system is called a \"welding robot\" even though its discrete manipulator unit could be adapted to a variety of tasks. Some robots are specifically designed for heavy load manipulation, and are labeled as \"heavy-duty robots\".[3]\nCurrent and potential applications include:\nAt present, mostly (lead–acid) batteries are used as a power source. Many different types of batteries can be used as a power source for robots. They range from lead–acid batteries, which are safe and have relatively long shelf lives but are rather heavy compared to silver–cadmium batteries which are much smaller in volume and are currently much more expensive. Designing a battery-powered robot needs to take into account factors such as safety, cycle lifetime, and weight. Generators, often some type of internal combustion engine, can also be used. However, such designs are often mechanically complex and need fuel, require heat dissipation, and are relatively heavy. A tether connecting the robot to a power supply would remove the power supply from the robot entirely. This has the advantage of saving weight and space by moving all power generation and storage components elsewhere. However, this design does come with the drawback of constantly having a cable connected to the robot, which can be difficult to manage.[15] \nPotential power sources could be:\nActuators are the \"muscles\" of a robot, the parts which convert stored energy into movement.[16] By far the most popular actuators are electric motors that rotate a wheel or gear, and linear actuators that control industrial robots in factories. There are some recent advances in alternative types of actuators, powered by electricity, chemicals, or compressed air.\nThe vast majority of robots use electric motors, often brushed and brushless DC motors in portable robots or AC motors in industrial robots and CNC machines. These motors are often preferred in systems with lighter loads, and where the predominant form of motion is rotational.\nVarious types of linear actuators move in and out instead of by spinning, and often have quicker direction changes, particularly when very large forces are needed such as with industrial robotics. They are typically powered by compressed and oxidized air (pneumatic actuator) or an oil (hydraulic actuator) Linear actuators can also be powered by electricity which usually consists of a motor and a leadscrew. Another common type is a mechanical linear actuator such as a rack and pinion on a car.\nSeries elastic actuation (SEA) relies on the idea of introducing intentional elasticity between the motor actuator and the load for robust force control. Due to the resultant lower reflected inertia, series elastic actuation improves safety when a robot interacts with the environment (e.g., humans or workpieces) or during collisions.[17] Furthermore, it also provides energy efficiency and shock absorption (mechanical filtering) while reducing excessive wear on the transmission and other mechanical components. This approach has successfully been employed in various robots, particularly advanced manufacturing robots[18] and walking humanoid robots.[19][20]\nThe controller design of a series elastic actuator is most often performed within the passivity framework as it ensures the safety of interaction with unstructured environments.[21] Despite its remarkable stability and robustness, this framework suffers from the stringent limitations imposed on the controller which may trade-off performance. The reader is referred to the following survey which summarizes the common controller architectures for SEA along with the corresponding sufficient passivity conditions.[22] One recent study has derived the necessary and sufficient passivity conditions for one of the most common impedance control architectures, namely velocity-sourced SEA.[23] This work is of particular importance as it drives the non-conservative passivity bounds in an SEA scheme for the first time which allows a larger selection of control gains.\nPneumatic artificial muscles also known as air muscles, are special tubes that expand (typically up to 42%) when air is forced inside them. They are used in some robot applications.[24][25][26]\nMuscle wire, also known as shape memory alloy, Nitinol® or Flexinol® wire, is a material that contracts (under 5%) when electricity is applied. They have been used for some small robot applications.[27][28]\nEAPs or EPAMs are a plastic material that can contract substantially (up to 380% activation strain) from electricity, and have been used in facial muscles and arms of humanoid robots,[29] and to enable new robots to float,[30] fly, swim or walk.[31]\nRecent alternatives to DC motors are piezo motors or ultrasonic motors. These work on a fundamentally different principle, whereby tiny piezoceramic elements, vibrating many thousands of times per second, cause linear or rotary motion. There are different mechanisms of operation; one type uses the vibration of the piezo elements to step the motor in a circle or a straight line.[32] Another type uses the piezo elements to cause a nut to vibrate or to drive a screw. The advantages of these motors are nanometer resolution, speed, and available force for their size.[33] These motors are already available commercially and being used on some robots.[34][35]\nElastic nanotubes are a promising artificial muscle technology in early-stage experimental development. The absence of defects in carbon nanotubes enables these filaments to deform elastically by several percent, with energy storage levels of perhaps 10 J/cm3 for metal nanotubes. Human biceps could be replaced with an 8 mm diameter wire of this material. Such compact \"muscle\" might allow future robots to outrun and outjump humans.[36]\nSensors allow robots to receive information about a certain measurement of the environment, or internal components. This is essential for robots to perform their tasks, and act upon any changes in the environment to calculate the appropriate response. They are used for various forms of measurements, to give the robots warnings about safety or malfunctions, and to provide real-time information about the task it is performing.\nCurrent robotic and prosthetic hands receive far less tactile information than the human hand. Recent research has developed a tactile sensor array that mimics the mechanical properties and touch receptors of human fingertips.[37][38] The sensor array is constructed as a rigid core surrounded by conductive fluid contained by an elastomeric skin. Electrodes are mounted on the surface of the rigid core and are connected to an impedance-measuring device within the core. When the artificial skin touches an object the fluid path around the electrodes is deformed, producing impedance changes that map the forces received from the object. The researchers expect that an important function of such artificial fingertips will be adjusting the robotic grip on held objects.\nScientists from several European countries and Israel developed a prosthetic hand in 2009, called SmartHand, which functions like a real one —allowing patients to write with it, type on a keyboard, play piano, and perform other fine movements. The prosthesis has sensors which enable the patient to sense real feelings in its fingertips.[39]\nOther common forms of sensing in robotics use lidar, radar, and sonar.[40] Lidar measures the distance to a target by illuminating the target with laser light and measuring the reflected light with a sensor. Radar uses radio waves to determine the range, angle, or velocity of objects. Sonar uses sound propagation to navigate, communicate with or detect objects on or under the surface of the water.\nA definition of robotic manipulation has been provided by Matt Mason as: \"manipulation refers to an agent's control of its environment through selective contact\".[41]\nRobots need to manipulate objects; pick up, modify, destroy, move or otherwise have an effect. Thus the functional end of a robot arm intended to make the effect (whether a hand, or tool) are often referred to as end effectors,[42] while the \"arm\" is referred to as a manipulator.[43] Most robot arms have replaceable end-effectors, each allowing them to perform some small range of tasks. Some have a fixed manipulator that cannot be replaced, while a few have one very general-purpose manipulator, for example, a humanoid hand.[44]\nOne of the most common types of end-effectors are \"grippers\". In its simplest manifestation, it consists of just two fingers that can open and close to pick up and let go of a range of small objects. Fingers can, for example, be made of a chain with a metal wire running through it.[45] Hands that resemble and work more like a human hand include the Shadow Hand and the Robonaut hand.[46] Hands that are of a mid-level complexity include the Delft hand.[47][48] Mechanical grippers can come in various types, including friction and encompassing jaws. Friction jaws use all the force of the gripper to hold the object in place using friction. Encompassing jaws cradle the object in place, using less friction.\nSuction end-effectors, powered by vacuum generators, are very simple astrictive[49] devices that can hold very large loads provided the prehension surface is smooth enough to ensure suction.\nPick and place robots for electronic components and for large objects like car windscreens, often use very simple vacuum end-effectors.\nSuction is a highly used type of end-effector in industry, in part because the natural compliance of soft suction end-effectors can enable a robot to be more robust in the presence of imperfect robotic perception. As an example: consider the case of a robot vision system that estimates the position of a water bottle but has 1 centimeter of error. While this may cause a rigid mechanical gripper to puncture the water bottle, the soft suction end-effector may just bend slightly and conform to the shape of the water bottle surface.\nSome advanced robots are beginning to use fully humanoid hands, like the Shadow Hand, MANUS,[50] and the Schunk hand.[51] They have powerful robot dexterity intelligence (RDI), with as many as 20 degrees of freedom and hundreds of tactile sensors.[52]\nFor simplicity, most mobile robots have four wheels or a number of continuous tracks. Some researchers have tried to create more complex wheeled robots with only one or two wheels. These can have certain advantages such as greater efficiency and reduced parts, as well as allowing a robot to navigate in confined places that a four-wheeled robot would not be able to.\nBalancing robots generally use a gyroscope to detect how much a robot is falling and then drive the wheels proportionally in the same direction, to counterbalance the fall at hundreds of times per second, based on the dynamics of an inverted pendulum.[53] Many different balancing robots have been designed.[54] While the Segway is not commonly thought of as a robot, it can be thought of as a component of a robot, when used as such Segway refer to them as RMP (Robotic Mobility Platform). An example of this use has been as NASA's Robonaut that has been mounted on a Segway.[55]\nA one-wheeled balancing robot is an extension of a two-wheeled balancing robot so that it can move in any 2D direction using a round ball as its only wheel. Several one-wheeled balancing robots have been designed recently, such as Carnegie Mellon University's \"Ballbot\" which is the approximate height and width of a person, and Tohoku Gakuin University's \"BallIP\".[56] Because of the long, thin shape and ability to maneuver in tight spaces, they have the potential to function better than other robots in environments with people.[57]\nSeveral attempts have been made in robots that are completely inside a spherical ball, either by spinning a weight inside the ball,[58][59] or by rotating the outer shells of the sphere.[60][61] These have also been referred to as an orb bot[62] or a ball bot.[63][64]\nUsing six wheels instead of four wheels can give better traction or grip in outdoor terrain such as on rocky dirt or grass.\nTank tracks provide even more traction than a six-wheeled robot. Tracked wheels behave as if they were made of hundreds of wheels, therefore are very common for outdoor and military robots, where the robot must drive on very rough terrain. However, they are difficult to use indoors such as on carpets and smooth floors. Examples include NASA's Urban Robot \"Urbie\".[65]\nWalking is a difficult and dynamic problem to solve. Several robots have been made which can walk reliably on two legs, however, none have yet been made which are as robust as a human. There has been much study on human-inspired walking, such as AMBER lab which was established in 2008 by the Mechanical Engineering Department at Texas A\u0026M University.[66] Many other robots have been built that walk on more than two legs, due to these robots being significantly easier to construct.[67][68] Walking robots can be used for uneven terrains, which would provide better mobility and energy efficiency than other locomotion methods. Typically, robots on two legs can walk well on flat floors and can occasionally walk up stairs. None can walk over rocky, uneven terrain. Some of the methods which have been tried are:\nThe zero moment point (ZMP) is the algorithm used by robots such as Honda's ASIMO. The robot's onboard computer tries to keep the total inertial forces (the combination of Earth's gravity and the acceleration and deceleration of walking), exactly opposed by the floor reaction force (the force of the floor pushing back on the robot's foot). In this way, the two forces cancel out, leaving no moment (force causing the robot to rotate and fall over).[69] However, this is not exactly how a human walks, and the difference is obvious to human observers, some of whom have pointed out that ASIMO walks as if it needs the lavatory.[70][71][72] ASIMO's walking algorithm is not static, and some dynamic balancing is used (see below). However, it still requires a smooth surface to walk on.\nSeveral robots, built in the 1980s by Marc Raibert at the MIT Leg Laboratory, successfully demonstrated very dynamic walking. Initially, a robot with only one leg, and a very small foot could stay upright simply by hopping. The movement is the same as that of a person on a pogo stick. As the robot falls to one side, it would jump slightly in that direction, in order to catch itself.[73] Soon, the algorithm was generalised to two and four legs. A bipedal robot was demonstrated running and even performing somersaults.[74] A quadruped was also demonstrated which could trot, run, pace, and bound.[75] For a full list of these robots, see the MIT Leg Lab Robots page.[76]\nA more advanced way for a robot to walk is by using a dynamic balancing algorithm, which is potentially more robust than the Zero Moment Point technique, as it constantly monitors the robot's motion, and places the feet in order to maintain stability.[77] This technique was recently demonstrated by Anybots' Dexter Robot,[78] which is so stable, it can even jump.[79] Another example is the TU Delft Flame.\nPerhaps the most promising approach uses passive dynamics where the momentum of swinging limbs is used for greater efficiency. It has been shown that totally unpowered humanoid mechanisms can walk down a gentle slope, using only gravity to propel themselves. Using this technique, a robot need only supply a small amount of motor power to walk along a flat surface or a little more to walk up a hill. This technique promises to make walking robots at least ten times more efficient than ZMP walkers, like ASIMO.[80][81]\nA modern passenger airliner is essentially a flying robot, with two humans to manage it. The autopilot can control the plane for each stage of the journey, including takeoff, normal flight, and even landing.[82] Other flying robots are uninhabited and are known as unmanned aerial vehicles (UAVs). They can be smaller and lighter without a human pilot on board, and fly into dangerous territory for military surveillance missions. Some can even fire on targets under command. UAVs are also being developed which can fire on targets automatically, without the need for a command from a human. Other flying robots include cruise missiles, the Entomopter, and the Epson micro helicopter robot. Robots such as the Air Penguin, Air Ray, and Air Jelly have lighter-than-air bodies, are propelled by paddles, and are guided by sonar.\nBFRs take inspiration from flying mammals, birds, or insects. BFRs can have flapping wings, which generate the lift and thrust, or they can be propeller actuated. BFRs with flapping wings have increased stroke efficiencies, increased maneuverability, and reduced energy consumption in comparison to propeller actuated BFRs.[83] Mammal and bird inspired BFRs share similar flight characteristics and design considerations. For instance, both mammal and bird inspired BFRs minimize edge fluttering and pressure-induced wingtip curl by increasing the rigidity of the wing edge and wingtips. Mammal and insect inspired BFRs can be impact resistant, making them useful in cluttered environments.\nMammal inspired BFRs typically take inspiration from bats, but the flying squirrel has also inspired a prototype.[84] Examples of bat inspired BFRs include Bat Bot[85] and the DALER.[86] Mammal inspired BFRs can be designed to be multi-modal; therefore, they're capable of both flight and terrestrial movement. To reduce the impact of landing, shock absorbers can be implemented along the wings.[86] Alternatively, the BFR can pitch up and increase the amount of drag it experiences.[84] By increasing the drag force, the BFR will decelerate and minimize the impact upon grounding. Different land gait patterns can also be implemented.[84]\nBird inspired BFRs can take inspiration from raptors, gulls, and everything in-between. Bird inspired BFRs can be feathered to increase the angle of attack range over which the prototype can operate before stalling.[87] The wings of bird inspired BFRs allow for in-plane deformation, and the in-plane wing deformation can be adjusted to maximize flight efficiency depending on the flight gait.[87] An example of a raptor inspired BFR is the prototype by Savastano et al.[88] The prototype has fully deformable flapping wings and is capable of carrying a payload of up to 0.8 kg while performing a parabolic climb, steep descent, and rapid recovery. The gull inspired prototype by Grant et al. accurately mimics the elbow and wrist rotation of gulls, and they find that lift generation is maximized when the elbow and wrist deformations are opposite but equal.[89]\nInsect inspired BFRs typically take inspiration from beetles or dragonflies. An example of a beetle inspired BFR is the prototype by Phan and Park,[90] and a dragonfly inspired BFR is the prototype by Hu et al.[91] The flapping frequency of insect inspired BFRs are much higher than those of other BFRs; this is because of the aerodynamics of insect flight.[92] Insect inspired BFRs are much smaller than those inspired by mammals or birds, so they are more suitable for dense environments.\nA class of robots that are biologically inspired, but which do not attempt to mimic biology, are creations such as the Entomopter. Funded by DARPA, NASA, the United States Air Force, and the Georgia Tech Research Institute and patented by Prof. Robert C. Michelson for covert terrestrial missions as well as flight in the lower Mars atmosphere, the Entomopter flight propulsion system uses low Reynolds number wings similar to those of the hawk moth (Manduca sexta), but flaps them in a non-traditional \"opposed x-wing fashion\" while \"blowing\" the surface to enhance lift based on the Coandă effect as well as to control vehicle attitude and direction. Waste gas from the propulsion system not only facilitates the blown wing aerodynamics, but also serves to create ultrasonic emissions like that of a Bat for obstacle avoidance. The Entomopter and other biologically-inspired robots leverage features of biological systems, but do not attempt to create mechanical analogs.\nSeveral snake robots have been successfully developed. Mimicking the way real snakes move, these robots can navigate very confined spaces, meaning they may one day be used to search for people trapped in collapsed buildings.[93] The Japanese ACM-R5 snake robot[94] can even navigate both on land and in water.[95]\nA small number of skating robots have been developed, one of which is a multi-mode walking and skating device. It has four legs, with unpowered wheels, which can either step or roll.[96] Another robot, Plen, can use a miniature skateboard or roller-skates, and skate across a desktop.[97]\nSeveral different approaches have been used to develop robots that have the ability to climb vertical surfaces. One approach mimics the movements of a human climber on a wall with protrusions; adjusting the center of mass and moving each limb in turn to gain leverage. An example of this is Capuchin,[98] built by Ruixiang Zhang at Stanford University, California. Another approach uses the specialized toe pad method of wall-climbing geckoes, which can run on smooth surfaces such as vertical glass. Examples of this approach include Wallbot[99] and Stickybot.[100]\nChina's Technology Daily reported on 15 November 2008, that Li Hiu Yeung and his research group of New Concept Aircraft (Zhuhai) Co., Ltd. had successfully developed a bionic gecko robot named \"Speedy Freelander\". According to Yeung, the gecko robot could rapidly climb up and down a variety of building walls, navigate through ground and wall fissures, and walk upside-down on the ceiling. It was also able to adapt to the surfaces of smooth glass, rough, sticky or dusty walls as well as various types of metallic materials. It could also identify and circumvent obstacles automatically. Its flexibility and speed were comparable to a natural gecko. A third approach is to mimic the motion of a snake climbing a pole.[40]\nIt is calculated that when swimming some fish can achieve a propulsive efficiency greater than 90%.[101] Furthermore, they can accelerate and maneuver far better than any man-made boat or submarine, and produce less noise and water disturbance. Therefore, many researchers studying underwater robots would like to copy this type of locomotion.[102] Notable examples are the Essex University Computer Science Robotic Fish G9,[103] and the Robot Tuna built by the Institute of Field Robotics, to analyze and mathematically model thunniform motion.[104] The Aqua Penguin,[105] designed and built by Festo of Germany, copies the streamlined shape and propulsion by front \"flippers\" of penguins. Festo have also built the Aqua Ray and Aqua Jelly, which emulate the locomotion of manta ray, and jellyfish, respectively.\nIn 2014, iSplash-II was developed by PhD student Richard James Clapham and Prof. Huosheng Hu at Essex University. It was the first robotic fish capable of outperforming real carangiform fish in terms of average maximum velocity (measured in body lengths/ second) and endurance, the duration that top speed is maintained.[106] This build attained swimming speeds of 11.6BL/s (i.e. 3.7 m/s).[107] The first build, iSplash-I (2014) was the first robotic platform to apply a full-body length carangiform swimming motion which was found to increase swimming speed by 27% over the traditional approach of a posterior confined waveform.[108]\nSailboat robots have also been developed in order to make measurements at the surface of the ocean. A typical sailboat robot is Vaimos[109] built by IFREMER and ENSTA-Bretagne. Since the propulsion of sailboat robots uses the wind, the energy of the batteries is only used for the computer, for the communication and for the actuators (to tune the rudder and the sail). If the robot is equipped with solar panels, the robot could theoretically navigate forever. The two main competitions of sailboat robots are WRSC, which takes place every year in Europe, and Sailbot.\nThe mechanical structure of a robot must be controlled to perform tasks.[110] The control of a robot involves three distinct phases – perception, processing, and action (robotic paradigms).[111] Sensors give information about the environment or the robot itself (e.g. the position of its joints or its end effector). This information is then processed to be stored or transmitted and to calculate the appropriate signals to the actuators (motors), which move the mechanical structure to achieve the required co-ordinated motion or force actions.\nThe processing phase can range in complexity. At a reactive level, it may translate raw sensor information directly into actuator commands (e.g. firing motor power electronic gates based directly upon encoder feedback signals to achieve the required torque/velocity of the shaft). Sensor fusion and internal models may first be used to estimate parameters of interest (e.g. the position of the robot's gripper) from noisy sensor data. An immediate task (such as moving the gripper in a certain direction until an object is detected with a proximity sensor) is sometimes inferred from these estimates. Techniques from control theory are generally used to convert the higher-level tasks into individual commands that drive the actuators, most often using kinematic and dynamic models of the mechanical structure.[110][111][112]\nAt longer time scales or with more sophisticated tasks, the robot may need to build and reason with a \"cognitive\" model. Cognitive models try to represent the robot, the world, and how the two interact. Pattern recognition and computer vision can be used to track objects.[110] Mapping techniques can be used to build maps of the world. Finally, motion planning and other artificial intelligence techniques may be used to figure out how to act. For example, a planner may figure out how to achieve a task without hitting obstacles, falling over, etc.\nModern commercial robotic control systems are highly complex, integrate multiple sensors and effectors, have many interacting degrees-of-freedom (DOF) and require operator interfaces, programming tools and real-time capabilities.[111] They are oftentimes interconnected to wider communication networks and in many cases are now both IoT-enabled and mobile.[113] Progress towards open architecture, layered, user-friendly and 'intelligent' sensor-based interconnected robots has emerged from earlier concepts related to Flexible Manufacturing Systems (FMS), and several 'open or 'hybrid' reference architectures exist which assist developers of robot control software and hardware to move beyond traditional, earlier notions of 'closed' robot control systems have been proposed.[112] Open architecture controllers are said to be better able to meet the growing requirements of a wide range of robot users, including system developers, end users and research scientists, and are better positioned to deliver the advanced robotic concepts related to Industry 4.0.[112] In addition to utilizing many established features of robot controllers, such as position, velocity and force control of end effectors, they also enable IoT interconnection and the implementation of more advanced sensor fusion and control techniques, including adaptive control, Fuzzy control and Artificial Neural Network (ANN)-based control.[112] When implemented in real-time, such techniques can potentially improve the stability and performance of robots operating in unknown or uncertain environments by enabling the control systems to learn and adapt to environmental changes.[114] There are several examples of reference architectures for robot controllers, and also examples of successful implementations of actual robot controllers developed from them. One example of a generic reference architecture and associated interconnected, open-architecture robot and controller implementation was developed by Michael Short and colleagues at the University of Sunderland in the UK in 2000 (pictured right).[112] The robot was used in a number of research and development studies, including prototype implementation of novel advanced and intelligent control and environment mapping methods in real-time.[114][115]\nControl systems may also have varying levels of autonomy.\nAnother classification takes into account the interaction between human control and the machine motions.\n\nComputer vision is the science and technology of machines that see. As a scientific discipline, computer vision is concerned with the theory behind artificial systems that extract information from images. The image data can take many forms, such as video sequences and views from cameras.\nIn most practical computer vision applications, the computers are pre-programmed to solve a particular task, but methods based on learning are now becoming increasingly common.\nComputer vision systems rely on image sensors that detect electromagnetic radiation which is typically in the form of either visible light or infra-red light. The sensors are designed using solid-state physics. The process by which light propagates and reflects off surfaces is explained using optics. Sophisticated image sensors even require quantum mechanics to provide a complete understanding of the image formation process. Robots can also be equipped with multiple vision sensors to be better able to compute the sense of depth in the environment. Like human eyes, robots' \"eyes\" must also be able to focus on a particular area of interest, and also adjust to variations in light intensities.\nThere is a subfield within computer vision where artificial systems are designed to mimic the processing and behavior of biological system, at different levels of complexity. Also, some of the learning-based methods developed within computer vision have a background in biology.\nThough a significant percentage of robots in commission today are either human controlled or operate in a static environment, there is an increasing interest in robots that can operate autonomously in a dynamic environment. These robots require some combination of navigation hardware and software in order to traverse their environment. In particular, unforeseen events (e.g. people and other obstacles that are not stationary) can cause problems or collisions. Some highly advanced robots such as ASIMO and Meinü robot have particularly good robot navigation hardware and software. Also, self-controlled cars, Ernst Dickmanns' driverless car, and the entries in the DARPA Grand Challenge, are capable of sensing the environment well and subsequently making navigational decisions based on this information, including by a swarm of autonomous robots.[118] Most of these robots employ a GPS navigation device with waypoints, along with radar, sometimes combined with other sensory data such as lidar, video cameras, and inertial guidance systems for better navigation between waypoints.\nThe state of the art in sensory intelligence for robots will have to progress through several orders of magnitude if we want the robots working in our homes to go beyond vacuum-cleaning the floors. If robots are to work effectively in homes and other non-industrial environments, the way they are instructed to perform their jobs, and especially how they will be told to stop will be of critical importance. The people who interact with them may have little or no training in robotics, and so any interface will need to be extremely intuitive. Science fiction authors also typically assume that robots will eventually be capable of communicating with humans through speech, gestures, and facial expressions, rather than a command-line interface. Although speech would be the most natural way for the human to communicate, it is unnatural for the robot. It will probably be a long time before robots interact as naturally as the fictional C-3PO, or Data of Star Trek, Next Generation. Even though the current state of robotics cannot meet the standards of these robots from science-fiction, robotic media characters (e.g., Wall-E, R2-D2) can elicit audience sympathies that increase people's willingness to accept actual robots in the future.[119] Acceptance of social robots is also likely to increase if people can meet a social robot under appropriate conditions. Studies have shown that interacting with a robot by looking at, touching, or even imagining interacting with the robot can reduce negative feelings that some people have about robots before interacting with them.[120] However, if pre-existing negative sentiments are especially strong, interacting with a robot can increase those negative feelings towards robots.[120]\nInterpreting the continuous flow of sounds coming from a human, in real time, is a difficult task for a computer, mostly because of the great variability of speech.[121] The same word, spoken by the same person may sound different depending on local acoustics, volume, the previous word, whether or not the speaker has a cold, etc.. It becomes even harder when the speaker has a different accent.[122] Nevertheless, great strides have been made in the field since Davis, Biddulph, and Balashek designed the first \"voice input system\" which recognized \"ten digits spoken by a single user with 100% accuracy\" in 1952.[123] Currently, the best systems can recognize continuous, natural speech, up to 160 words per minute, with an accuracy of 95%.[124] With the help of artificial intelligence, machines nowadays can use people's voice to identify their emotions such as satisfied or angry.[125]\nOther hurdles exist when allowing the robot to use voice for interacting with humans. For social reasons, synthetic voice proves suboptimal as a communication medium,[126] making it necessary to develop the emotional component of robotic voice through various techniques.[127][128] An advantage of diphonic branching is the emotion that the robot is programmed to project, can be carried on the voice tape, or phoneme, already pre-programmed onto the voice media. One of the earliest examples is a teaching robot named Leachim developed in 1974 by Michael J. Freeman.[129][130] Leachim was able to convert digital memory to rudimentary verbal speech on pre-recorded computer discs.[131] It was programmed to teach students in The Bronx, New York.[131]\nOne can imagine, in the future, explaining to a robot chef how to make a pastry, or asking directions from a robot police officer. In both of these cases, making hand gestures would aid the verbal descriptions. In the first case, the robot would be recognizing gestures made by the human, and perhaps repeating them for confirmation. In the second case, the robot police officer would gesture to indicate \"down the road, then turn right\". It is likely that gestures will make up a part of the interaction between humans and robots.[132] A great many systems have been developed to recognize human hand gestures.[133]\nFacial expressions can provide rapid feedback on the progress of a dialog between two humans, and soon may be able to do the same for humans and robots. Robotic faces have been constructed by Hanson Robotics using their elastic polymer called Frubber, allowing a large number of facial expressions due to the elasticity of the rubber facial coating and embedded subsurface motors (servos).[134] The coating and servos are built on a metal skull. A robot should know how to approach a human, judging by their facial expression and body language. Whether the person is happy, frightened, or crazy-looking affects the type of interaction expected of the robot. Likewise, robots like Kismet and the more recent addition, Nexi[135] can produce a range of facial expressions, allowing it to have meaningful social exchanges with humans.[136]\nArtificial emotions can also be generated, composed of a sequence of facial expressions or gestures. As can be seen from the movie Final Fantasy: The Spirits Within, the programming of these artificial emotions is complex and requires a large amount of human observation. To simplify this programming in the movie, presets were created together with a special software program. This decreased the amount of time needed to make the film. These presets could possibly be transferred for use in real-life robots. An example of a robot with artificial emotions is Robin the Robot developed by an Armenian IT company Expper Technologies, which uses AI-based peer-to-peer interaction. Its main task is achieving emotional well-being, i.e. overcome stress and anxiety. Robin was trained to analyze facial expressions and use his face to display his emotions given the context. The robot has been tested by kids in US clinics, and observations show that Robin increased the appetite and cheerfulness of children after meeting and talking.[137]\nMany of the robots of science fiction have a personality, something which may or may not be desirable in the commercial robots of the future.[138] Nevertheless, researchers are trying to create robots which appear to have a personality:[139][140] i.e. they use sounds, facial expressions, and body language to try to convey an internal state, which may be joy, sadness, or fear. One commercial example is Pleo, a toy robot dinosaur, which can exhibit several apparent emotions.[141]\nProxemics is the study of personal space, and HRI systems may try to model and work with its concepts for human interactions.\n\nMuch of the research in robotics focuses not on specific industrial tasks, but on investigations into new types of robots, alternative ways to think about or design robots, and new ways to manufacture them. Other investigations, such as MIT's cyberflora project, are almost wholly academic.\nTo describe the level of advancement of a robot, the term \"Generation Robots\" can be used. This term is coined by Professor Hans Moravec, Principal Research Scientist at the Carnegie Mellon University Robotics Institute in describing the near future evolution of robot technology. First-generation robots, Moravec predicted in 1997, should have an intellectual capacity comparable to perhaps a lizard and should become available by 2010. Because the first generation robot would be incapable of learning, however, Moravec predicts that the second generation robot would be an improvement over the first and become available by 2020, with the intelligence maybe comparable to that of a mouse. The third generation robot should have intelligence comparable to that of a monkey. Though fourth generation robots, robots with human intelligence, professor Moravec predicts, would become possible, he does not predict this happening before around 2040 or 2050.[142]\nThe study of motion can be divided into kinematics and dynamics.[143] Direct kinematics or forward kinematics refers to the calculation of end effector position, orientation, velocity, and acceleration when the corresponding joint values are known. Inverse kinematics refers to the opposite case in which required joint values are calculated for given end effector values, as done in path planning. Some special aspects of kinematics include handling of redundancy (different possibilities of performing the same movement), collision avoidance, and singularity avoidance. Once all relevant positions, velocities, and accelerations have been calculated using kinematics, methods from the field of dynamics are used to study the effect of forces upon these movements. Direct dynamics refers to the calculation of accelerations in the robot once the applied forces are known. Direct dynamics is used in computer simulations of the robot. Inverse dynamics refers to the calculation of the actuator forces necessary to create a prescribed end-effector acceleration. This information can be used to improve the control algorithms of a robot.\nIn each area mentioned above, researchers strive to develop new concepts and strategies, improve existing ones, and improve the interaction between these areas. To do this, criteria for \"optimal\" performance and ways to optimize design, structure, and control of robots must be developed and implemented.\nOpen source robotics research seeks standards for defining, and methods for designing and building, robots so that they can easily be reproduced by anyone. Research includes legal and technical definitions; seeking out alternative tools and materials to reduce costs and simplify builds; and creating interfaces and standards for designs to work together. Human usability research also investigates how to best document builds through visual, text or video instructions.\nEvolutionary robots is a methodology that uses evolutionary computation to help design robots, especially the body form, or motion and behavior controllers. In a similar way to natural evolution, a large population of robots is allowed to compete in some way, or their ability to perform a task is measured using a fitness function. Those that perform worst are removed from the population and replaced by a new set, which have new behaviors based on those of the winners. Over time the population improves, and eventually a satisfactory robot may appear. This happens without any direct programming of the robots by the researchers. Researchers use this method both to create better robots,[144] and to explore the nature of evolution.[145] Because the process often requires many generations of robots to be simulated,[146] this technique may be run entirely or mostly in simulation, using a robot simulator software package, then tested on real robots once the evolved algorithms are good enough.[147] Currently, there are about 10 million industrial robots toiling around the world, and Japan is the top country having high density of utilizing robots in its manufacturing industry.[citation needed]\nBionics and biomimetics apply the physiology and methods of locomotion of animals to the design of robots. For example, the design of BionicKangaroo was based on the way kangaroos jump.\nSwarm robotics is an approach to the coordination of multiple robots as a system which consist of large numbers of mostly simple physical robots. ″In a robot swarm, the collective behavior of the robots results from local interactions between the robots and between the robots and the environment in which they act.″* [118]\nThere has been some research into whether robotics algorithms can be run more quickly on quantum computers than they can be run on digital computers. This area has been referred to as quantum robotics.[148]\nThe main venues for robotics research are the international conferences ICRA and IROS.\nRobotics engineers design robots, maintain them, develop new applications for them, and conduct research to expand the potential of robotics.[151] Robots have become a popular educational tool in some middle and high schools, particularly in parts of the USA,[152] as well as in numerous youth summer camps, raising interest in programming, artificial intelligence, and robotics among students.\nRobotics is an essential component in many modern manufacturing environments. As factories increase their use of robots, the number of robotics–related jobs grow and have been observed to be steadily rising.[153] The employment of robots in industries has increased productivity and efficiency savings and is typically seen as a long-term investment for benefactors. A study found that 47 percent of US jobs are at risk to automation \"over some unspecified number of years\".[154] These claims have been criticized on the ground that social policy, not AI, causes unemployment.[155] In a 2016 article in The Guardian, Stephen Hawking stated \"The automation of factories has already decimated jobs in traditional manufacturing, and the rise of artificial intelligence is likely to extend this job destruction deep into the middle classes, with only the most caring, creative or supervisory roles remaining\".[156]\nAccording to a GlobalData September 2021 report, the robotics industry was worth $45bn in 2020, and by 2030, it will have grown at a compound annual growth rate (CAGR) of 29% to $568bn, driving jobs in robotics and related industries.[157]\nA discussion paper drawn up by EU-OSHA highlights how the spread of robotics presents both opportunities and challenges for occupational safety and health (OSH).[158]\nThe greatest OSH benefits stemming from the wider use of robotics should be substitution for people working in unhealthy or dangerous environments. In space, defense, security, or the nuclear industry, but also in logistics, maintenance, and inspection, autonomous robots are particularly useful in replacing human workers performing dirty, dull or unsafe tasks, thus avoiding workers' exposures to hazardous agents and conditions and reducing physical, ergonomic and psychosocial risks. For example, robots are already used to perform repetitive and monotonous tasks, to handle radioactive material or to work in explosive atmospheres. In the future, many other highly repetitive, risky or unpleasant tasks will be performed by robots in a variety of sectors like agriculture, construction, transport, healthcare, firefighting or cleaning services.[159]\nMoreover, there are certain skills to which humans will be better suited than machines for some time to come and the question is how to achieve the best combination of human and robot skills. The advantages of robotics include heavy-duty jobs with precision and repeatability, whereas the advantages of humans include creativity, decision-making, flexibility, and adaptability. This need to combine optimal skills has resulted in collaborative robots and humans sharing a common workspace more closely and led to the development of new approaches and standards to guarantee the safety of the \"man-robot merger\". Some European countries are including robotics in their national programs and trying to promote a safe and flexible cooperation between robots and operators to achieve better productivity. For example, the German Federal Institute for Occupational Safety and Health (BAuA) organises annual workshops on the topic \"human-robot collaboration\".\nIn the future, cooperation between robots and humans will be diversified, with robots increasing their autonomy and human-robot collaboration reaching completely new forms. Current approaches and technical standards[160][161] aiming to protect employees from the risk of working with collaborative robots will have to be revised.\nGreat user experience predicts the needs, experiences, behaviors, language and cognitive abilities, and other factors of each user group. It then uses these insights to produce a product or solution that is ultimately useful and usable. For robots, user experience begins with an understanding of the robot's intended task and environment, while considering any possible social impact the robot may have on human operations and interactions with it.[162]\nIt defines that communication as the transmission of information through signals, which are elements perceived through touch, sound, smell and sight.[163] The author states that the signal connects the sender to the receiver and consists of three parts: the signal itself, what it refers to, and the interpreter. Body postures and gestures, facial expressions, hand and head movements are all part of nonverbal behavior and communication. Robots are no exception when it comes to human-robot interaction. Therefore, humans use their verbal and nonverbal behaviors to communicate their defining characteristics. Similarly, social robots need this coordination to perform human-like behaviors.\nRobotics is an interdisciplinary field, combining primarily mechanical engineering and computer science but also drawing on electronic engineering and other subjects. The usual way to build a career in robotics is to complete an undergraduate degree in one of these established subjects, followed by a graduate (masters') degree in Robotics. Graduate degrees are typically joined by students coming from all of the contributing disciplines, and include familiarization of relevant undergraduate level subject matter from each of them, followed by specialist study in pure robotics topics which build upon them. As an interdisciplinary subject, robotics graduate programmes tend to be especially reliant on students working and learning together and sharing their knowledge and skills from their home discipline first degrees.    \nRobotics industry careers then follow the same pattern, with most roboticists working as part of interdisciplinary teams of specialists from these home disciplines followed by the robotics graduate degrees which enable them to work together. Workers typically continue to identify as members of their home disciplines who work in robotics, rather than as 'roboticists'. This structure is reinforced by the nature of some engineering professions, which grant chartered engineer status to members of home disciplines rather than to robotics as a whole.\nRobotics careers are widely predicted to grow during in the 21st century, as robots replace more manual and intellectual human work. Workers who lose their jobs to robotics may be well-placed to retrain to build and maintain these robots, using their domain-specific knowledge and skills.\nIn 1948, Norbert Wiener formulated the principles of cybernetics, the basis of practical robotics.\nFully autonomous robots only appeared in the second half of the 20th century. The first digitally operated and programmable robot, the Unimate, was installed in 1961 to lift hot pieces of metal from a die casting machine and stack them. Commercial and industrial robots are widespread today and used to perform jobs more cheaply, more accurately, and more reliably than humans. They are also employed in some jobs that are too dirty, dangerous, or dull to be suitable for humans. Robots are widely used in manufacturing, assembly, packing and packaging, mining, transport, earth and space exploration, surgery,[164] weaponry, laboratory research, safety, and the mass production of consumer and industrial goods.[165]"}
{"url":"https://en.wikipedia.org/wiki/Robot","text":"A robot is a machine—especially one programmable by a computer—capable of carrying out a complex series of actions automatically.[2] A robot can be guided by an external control device, or the control may be embedded within. Robots may be constructed to evoke human form, but most robots are task-performing machines, designed with an emphasis on stark functionality, rather than expressive aesthetics.\nRobots can be autonomous or semi-autonomous and range from humanoids such as Honda's Advanced Step in Innovative Mobility (ASIMO) and TOSY's TOSY Ping Pong Playing Robot (TOPIO) to industrial robots, medical operating robots, patient assist robots, dog therapy robots, collectively programmed swarm robots, UAV drones such as General Atomics MQ-1 Predator, and even microscopic nano robots. By mimicking a lifelike appearance or automating movements, a robot may convey a sense of intelligence or thought of its own. Autonomous things are expected to proliferate in the future, with home robotics and the autonomous car as some of the main drivers.[3]\nThe branch of technology that deals with the design, construction, operation, and application of robots,[4] as well as computer systems for their control, sensory feedback, and information processing is robotics. These technologies deal with automated machines that can take the place of humans in dangerous environments or manufacturing processes, or resemble humans in appearance, behavior, or cognition. Many of today's robots are inspired by nature contributing to the field of bio-inspired robotics. These robots have also created a newer branch of robotics: soft robotics.\nFrom the time of ancient civilization, there have been many accounts of user-configurable automated devices and even automata resembling humans and other animals, such as animatronics, designed primarily as entertainment. As mechanical techniques developed through the Industrial age, there appeared more practical applications such as automated machines, remote-control and wireless remote-control.\nThe term comes from a Slavic root, robot-, with meanings associated with labor. The word 'robot' was first used to denote a fictional humanoid in a 1920 Czech-language play R.U.R. (Rossumovi Univerzální Roboti – Rossum's Universal Robots) by Karel Čapek, though it was Karel's brother Josef Čapek who was the word's true inventor.[5][6][7] Electronics evolved into the driving force of development with the advent of the first electronic autonomous robots created by William Grey Walter in Bristol, England in 1948, as well as Computer Numerical Control (CNC) machine tools in the late 1940s by John T. Parsons and Frank L. Stulen.\nThe first commercial, digital and programmable robot was built by George Devol in 1954 and was named the Unimate. It was sold to General Motors in 1961 where it was used to lift pieces of hot metal from die casting machines at the Inland Fisher Guide Plant in the West Trenton section of Ewing Township, New Jersey.[8]\nRobots have replaced humans[9] in performing repetitive and dangerous tasks which humans prefer not to do, or are unable to do because of size limitations, or which take place in extreme environments such as outer space or the bottom of the sea. There are concerns about the increasing use of robots and their role in society. Robots are blamed for rising technological unemployment as they replace workers in increasing numbers of functions.[10] The use of robots in military combat raises ethical concerns. The possibilities of robot autonomy and potential repercussions have been addressed in fiction and may be a realistic concern in the future.\nThe word robot can refer to both physical robots and virtual software agents, but the latter are usually referred to as bots.[11] There is no consensus on which machines qualify as robots but there is general agreement among experts, and the public, that robots tend to possess some or all of the following abilities and functions: accept electronic programming, process data or physical perceptions electronically, operate autonomously to some degree, move around, operate physical parts of itself or physical processes, sense and manipulate their environment, and exhibit intelligent behavior, especially behavior which mimics humans or other animals.[12][13] Related to the concept of a robot is the field of synthetic biology, which studies entities whose nature is more comparable to living things than to machines.\nThe idea of automata originates in the mythologies of many cultures around the world. Engineers and inventors from ancient civilizations, including Ancient China,[14] Ancient Greece, and Ptolemaic Egypt,[15] attempted to build self-operating machines, some resembling animals and humans. Early descriptions of automata include the artificial doves of Archytas,[16] the artificial birds of Mozi and Lu Ban,[17] a \"speaking\" automaton by Hero of Alexandria, a washstand automaton by Philo of Byzantium, and a human automaton described in the Lie Zi.[14]\nMany ancient mythologies, and most modern religions include artificial people, such as the mechanical servants built by the Greek god Hephaestus[18] (Vulcan to the Romans), the clay golems of Jewish legend and clay giants of Norse legend, and Galatea, the mythical statue of Pygmalion that came to life. Since circa 400 BC, myths of Crete include Talos, a man of bronze who guarded the island from pirates.\nIn ancient Greece, the Greek engineer Ctesibius (c. 270 BC) \"applied a knowledge of pneumatics and hydraulics to produce the first organ and water clocks with moving figures.\"[19]: 2 [20] In the 4th century BC, the Greek mathematician Archytas of Tarentum postulated a mechanical steam-operated bird he called \"The Pigeon\". Hero of Alexandria (10–70 AD), a Greek mathematician and inventor, created numerous user-configurable automated devices, and described machines powered by air pressure, steam and water.[21]\nThe 11th century Lokapannatti tells of how the Buddha's relics were protected by mechanical robots (bhuta vahana yanta), from the kingdom of Roma visaya (Rome); until they were disarmed by King Ashoka.[22]\nIn ancient China, the 3rd-century text of the Lie Zi describes an account of humanoid automata, involving a much earlier encounter between Chinese emperor King Mu of Zhou and a mechanical engineer known as Yan Shi, an 'artificer'. Yan Shi proudly presented the king with a life-size, human-shaped figure of his mechanical 'handiwork' made of leather, wood, and artificial organs.[14] There are also accounts of flying automata in the Han Fei Zi and other texts, which attributes the 5th century BC Mohist philosopher Mozi and his contemporary Lu Ban with the invention of artificial wooden birds (ma yuan) that could successfully fly.[17]\n In 1066, the Chinese inventor Su Song built a water clock in the form of a tower which featured mechanical figurines which chimed the hours.[23][24][25] His mechanism had a programmable drum machine with pegs (cams) that bumped into little levers that operated percussion instruments. The drummer could be made to play different rhythms and different drum patterns by moving the pegs to different locations.[25]\nSamarangana Sutradhara, a Sanskrit treatise by Bhoja (11th century), includes a chapter about the construction of mechanical contrivances (automata), including mechanical bees and birds, fountains shaped like humans and animals, and male and female dolls that refilled oil lamps, danced, played instruments, and re-enacted scenes from Hindu mythology.[26][27][28]\n13th century Muslim scientist Ismail al-Jazari created several automated devices. He built automated moving peacocks driven by hydropower.[29] He also invented the earliest known automatic gates, which were driven by hydropower,[30] created automatic doors as part of one of his elaborate water clocks.[31] One of al-Jazari's humanoid automata was a waitress that could serve water, tea or drinks. The drink was stored in a tank with a reservoir from where the drink drips into a bucket and, after seven minutes, into a cup, after which the waitress appears out of an automatic door serving the drink.[32] Al-Jazari invented a hand washing automaton incorporating a flush mechanism now used in modern flush toilets. It features a female humanoid automaton standing by a basin filled with water. When the user pulls the lever, the water drains and the female automaton refills the basin.[19]\n\nMark E. Rosheim summarizes the advances in robotics made by Muslim engineers, especially al-Jazari, as follows:Unlike the Greek designs, these Arab examples reveal an interest, not only in dramatic illusion, but in manipulating the environment for human comfort. Thus, the greatest contribution the Arabs made, besides preserving, disseminating and building on the work of the Greeks, was the concept of practical application. This was the key element that was missing in Greek robotic science.[19]: 9  In the 14th century, the coronation of Richard II of England featured an automata angel.[34]\nIn Renaissance Italy, Leonardo da Vinci (1452–1519) sketched plans for a humanoid robot around 1495. Da Vinci's notebooks, rediscovered in the 1950s, contained detailed drawings of a mechanical knight now known as Leonardo's robot, able to sit up, wave its arms and move its head and jaw.[35] The design was probably based on anatomical research recorded in his Vitruvian Man. It is not known whether he attempted to build it. According to Encyclopædia Britannica, Leonardo da Vinci may have been influenced by the classic automata of al-Jazari.[29]\nIn Japan, complex animal and human automata were built between the 17th to 19th centuries, with many described in the 18th century Karakuri zui (Illustrated Machinery, 1796). One such automaton was the karakuri ningyō, a mechanized puppet.[36] Different variations of the karakuri existed: the Butai karakuri, which were used in theatre, the Zashiki karakuri, which were small and used in homes, and the Dashi karakuri which were used in religious festivals, where the puppets were used to perform reenactments of traditional myths and legends.\nIn France, between 1738 and 1739, Jacques de Vaucanson exhibited several life-sized automatons: a flute player, a pipe player and a duck. The mechanical duck could flap its wings, crane its neck, and swallow food from the exhibitor's hand, and it gave the illusion of digesting its food by excreting matter stored in a hidden compartment.[37] About 30 years later in Switzerland the clockmaker Pierre Jaquet-Droz made several complex mechanical figures that could write and play music. Several of these devices still exist and work.[38]\nRemotely operated vehicles were demonstrated in the late 19th century in the form of several types of remotely controlled torpedoes. The early 1870s saw remotely controlled torpedoes by John Ericsson (pneumatic), John Louis Lay (electric wire guided), and Victor von Scheliha (electric wire guided).[39]\nThe Brennan torpedo, invented by Louis Brennan in 1877, was powered by two contra-rotating propellers that were spun by rapidly pulling out wires from drums wound inside the torpedo. Differential speed on the wires connected to the shore station allowed the torpedo to be guided to its target, making it \"the world's first practical guided missile\".[40] In 1897 the British inventor Ernest Wilson was granted a patent for a torpedo remotely controlled by \"Hertzian\" (radio) waves[41][42] and in 1898 Nikola Tesla publicly demonstrated a wireless-controlled torpedo that he hoped to sell to the US Navy.[43][44]\nIn 1903, the Spanish engineer Leonardo Torres Quevedo demonstrated a radio control system called \"Telekino\" at the Paris Academy of Sciences,[45] which he wanted to use to control an airship of his own design. He obtained some patents in other countries.[46] Unlike the previous mechanisms, which carried out actions of the 'on/off' type, Torres developed a system for controlling any mechanical or electrical device with different states of operation.[47] \nThe transmitter was capable of sending a family of different codewords by means of a binary telegraph signal to the receiver, which was able to set up a different state of operation in the device being used, depending on the codeword. Specifically, it was able to do up to 19 different actions.[48][49]\nArchibald Low, known as the \"father of radio guidance systems\" for his pioneering work on guided rockets and planes during the First World War. In 1917, he demonstrated a remote controlled aircraft to the Royal Flying Corps and in the same year built the first wire-guided rocket.\nIn 1928, one of the first humanoid robots, Eric, was exhibited at the annual exhibition of the Model Engineers Society in London, where it delivered a speech. Invented by W. H. Richards, the robot's frame consisted of an aluminium body of armour with eleven electromagnets and one motor powered by a twelve-volt power source. The robot could move its hands and head and could be controlled through remote control or voice control.[50] Both Eric and his \"brother\" George toured the world.[51]\nWestinghouse Electric Corporation built Televox in 1926; it was a cardboard cutout connected to various devices which users could turn on and off. In 1939, the humanoid robot known as Elektro was debuted at the 1939 New York World's Fair.[52][53] Seven feet tall (2.1 m) and weighing 265 pounds (120.2 kg), it could walk by voice command, speak about 700 words (using a 78-rpm record player), smoke cigarettes, blow up balloons, and move its head and arms. The body consisted of a steel gear, cam and motor skeleton covered by an aluminum skin. In 1928, Japan's first robot, Gakutensoku, was designed and constructed by biologist Makoto Nishimura.\nThe German V-1 flying bomb was equipped with systems for automatic guidance and range control, flying on a predetermined course (which could include a 90-degree turn) and entering a terminal dive after a predetermined distance. It was reported as being a 'robot' in contemporary descriptions [54]\nThe first electronic autonomous robots with complex behaviour were created by William Grey Walter of the Burden Neurological Institute at Bristol, England in 1948 and 1949. He wanted to prove that rich connections between a small number of brain cells could give rise to very complex behaviors – essentially that the secret of how the brain worked lay in how it was wired up. His first robots, named Elmer and Elsie, were constructed between 1948 and 1949 and were often described as tortoises due to their shape and slow rate of movement. The three-wheeled tortoise robots were capable of phototaxis, by which they could find their way to a recharging station when they ran low on battery power.\nWalter stressed the importance of using purely analogue electronics to simulate brain processes at a time when his contemporaries such as Alan Turing and John von Neumann were all turning towards a view of mental processes in terms of digital computation. His work inspired subsequent generations of robotics researchers such as Rodney Brooks, Hans Moravec and Mark Tilden. Modern incarnations of Walter's turtles may be found in the form of BEAM robotics.[55]\nThe first digitally operated and programmable robot was invented by George Devol in 1954 and was ultimately called the Unimate. This ultimately laid the foundations of the modern robotics industry.[56] Devol sold the first Unimate to General Motors in 1960, and it was installed in 1961 in a plant in Trenton, New Jersey to lift hot pieces of metal from a die casting machine and stack them.[57]\nThe first palletizing robot was introduced in 1963 by the Fuji Yusoki Kogyo Company.[58] In 1973, a robot with six electromechanically driven axes was patented[59][60][61] by KUKA robotics in Germany, and the programmable universal manipulation arm was invented by Victor Scheinman in 1976, and the design was sold to Unimation.\nCommercial and industrial robots are now in widespread use performing jobs more cheaply or with greater accuracy and reliability than humans. They are also employed for jobs which are too dirty, dangerous or dull to be suitable for humans. Robots are widely used in manufacturing, assembly and packing, transport, earth and space exploration, surgery, weaponry, laboratory research, and mass production of consumer and industrial goods.[62]\nVarious techniques have emerged to develop the science of robotics and robots. One method is evolutionary robotics, in which a number of differing robots are submitted to tests. Those which perform best are used as a model to create a subsequent \"generation\" of robots. Another method is developmental robotics, which tracks changes and development within a single robot in the areas of problem-solving and other functions. Another new type of robot is just recently introduced which acts both as a smartphone and robot and is named RoboHon.[63]\nAs robots become more advanced, eventually there may be a standard computer operating system designed mainly for robots. Robot Operating System (ROS) is an open-source software set of programs being developed at Stanford University, the Massachusetts Institute of Technology, and the Technical University of Munich, Germany, among others. ROS provides ways to program a robot's navigation and limbs regardless of the specific hardware involved. It also provides high-level commands for items like image recognition and even opening doors. When ROS boots up on a robot's computer, it would obtain data on attributes such as the length and movement of robots' limbs. It would relay this data to higher-level algorithms. Microsoft is also developing a \"Windows for robots\" system with its Robotics Developer Studio, which has been available since 2007.[64]\nJapan hopes to have full-scale commercialization of service robots by 2025. Much technological research in Japan is led by Japanese government agencies, particularly the Trade Ministry.[65]\nMany future applications of robotics seem obvious to people, even though they are well beyond the capabilities of robots available at the time of the prediction.[66][67] As early as 1982 people were confident that someday robots would:[68] 1. Clean parts by removing molding flash 2. Spray paint automobiles with absolutely no human presence 3. Pack things in boxes—for example, orient and nest chocolate candies in candy boxes 4. Make electrical cable harness 5. Load trucks with boxes—a packing problem 6. Handle soft goods, such as garments and shoes 7. Shear sheep 8. Be used asprostheses 9. Cook fast food and work in other service industries 10. Work as a household robot.\nGenerally such predictions are overly optimistic in timescale.\nIn 2008, Caterpillar Inc. developed a dump truck which can drive itself without any human operator.[69] Many analysts believe that self-driving trucks may eventually revolutionize logistics.[70] By 2014, Caterpillar had a self-driving dump truck which is expected to greatly change the process of mining. In 2015, these Caterpillar trucks were actively used in mining operations in Australia by the mining company Rio Tinto Coal Australia.[71][72][73][74] Some analysts believe that within the next few decades, most trucks will be self-driving.[75]\nA literate or 'reading robot' named Marge has intelligence that comes from software. She can read newspapers, find and correct misspelled words, learn about banks like Barclays, and understand that some restaurants are better places to eat than others.[76]\nBaxter is a new robot introduced in 2012 which learns by guidance. A worker could teach Baxter how to perform a task by moving its hands in the desired motion and having Baxter memorize them. Extra dials, buttons, and controls are available on Baxter's arm for more precision and features. Any regular worker could program Baxter and it only takes a matter of minutes, unlike usual industrial robots that take extensive programs and coding to be used. This means Baxter needs no programming to operate. No software engineers are needed. This also means Baxter can be taught to perform multiple, more complicated tasks. Sawyer was added in 2015 for smaller, more precise tasks.[77]\nPrototype cooking robots have been developed and could be programmed for autonomous, dynamic and adjustable preparation of discrete meals.[78][79]\nThe word robot was introduced to the public by the Czech interwar writer Karel Čapek in his play R.U.R. (Rossum's Universal Robots), published in 1920.[6] The play begins in a factory that uses a chemical substitute for protoplasm to manufacture living, simplified people called robots. The play does not focus in detail on the technology behind the creation of these living creatures, but in their appearance they prefigure modern ideas of androids, creatures who can be mistaken for humans. These mass-produced workers are depicted as efficient but emotionless, incapable of original thinking and indifferent to self-preservation. At issue is whether the robots are being exploited and the consequences of human dependence upon commodified labor (especially after a number of specially-formulated robots achieve self-awareness and incite robots all around the world to rise up against the humans).\nKarel Čapek himself did not coin the word. He wrote a short letter in reference to an etymology in the Oxford English Dictionary in which he named his brother, the painter and writer Josef Čapek, as its actual originator.[6]\nIn an article in the Czech journal Lidové noviny in 1933, he explained that he had originally wanted to call the creatures laboři ('workers', from Latin labor). However, he did not like the word, and sought advice from his brother Josef, who suggested roboti. The word robota means literally 'corvée, serf labor', and figuratively 'drudgery, hard work' in Czech and also (more general) 'work, labor' in many Slavic languages (e.g.: Bulgarian, Russian, Serbian, Slovak, Polish, Macedonian, Ukrainian, archaic Czech, as well as robot in Hungarian). Traditionally the robota (Hungarian robot) was the work period a serf (corvée) had to give for his lord, typically six months of the year. The origin of the word is the Old Church Slavonic rabota 'servitude' ('work' in contemporary Bulgarian, Macedonian and Russian), which in turn comes from the Proto-Indo-European root *orbh-. Robot is cognate with the German Arbeit 'work'.[80][81]\nEnglish pronunciation of the word has evolved relatively quickly since its introduction. In the U.S. during the late 1930s to early 1940s it was pronounced /ˈroʊboʊt/.[82][better source needed] By the late 1950s to early 1960s, some were pronouncing it /ˈroʊbət/, while others used /ˈroʊbɒt/[83] By the 1970s, its current pronunciation /ˈroʊbɒt/ had become predominant.\nThe word robotics, used to describe this field of study,[4] was coined by the science fiction writer Isaac Asimov. Asimov created the Three Laws of Robotics which are a recurring theme in his books. These have since been used by many others to define laws used in fiction. (The three laws are pure fiction, and no technology yet created has the ability to understand or follow them, and in fact most robots serve military purposes, which run quite contrary to the first law and often the third law. \"People think about Asimov's laws, but they were set up to point out how a simple ethical system doesn't work. If you read the short stories, every single one is about a failure, and they are totally impractical,\" said Dr. Joanna Bryson of the University of Bath.[84])\nMobile robots[85] have the capability to move around in their environment and are not fixed to one physical location. An example of a mobile robot that is in common use today is the automated guided vehicle or automatic guided vehicle (AGV). An AGV is a mobile robot that follows markers or wires in the floor, or uses vision or lasers.[86] AGVs are discussed later in this article.\nMobile robots are also found in industry, military and security environments.[87] They also appear as consumer products, for entertainment or to perform certain tasks like vacuum cleaning. Mobile robots are the focus of a great deal of current research and almost every major university has one or more labs that focus on mobile robot research.[88]\nMobile robots are usually used in tightly controlled environments such as on assembly lines because they have difficulty responding to unexpected interference. Because of this most humans rarely encounter robots. However domestic robots for cleaning and maintenance are increasingly common in and around homes in developed countries. Robots can also be found in military applications.[89]\nIndustrial robots usually consist of a jointed arm (multi-linked manipulator) and an end effector that is attached to a fixed surface. One of the most common type of end effector is a gripper assembly.\nThe International Organization for Standardization gives a definition of a  manipulating industrial robot in ISO 8373:\n\"an automatically controlled, reprogrammable, multipurpose, manipulator programmable in three or more axes, which may be either fixed in place or mobile for use in industrial automation applications.\"[90]\nThis definition is used by the International Federation of Robotics, the European Robotics Research Network (EURON) and many national standards committees.[91]\nMost commonly industrial robots are fixed robotic arms and manipulators used primarily for production and distribution of goods. The term \"service robot\" is less well-defined. The International Federation of Robotics has proposed a tentative definition, \"A service robot is a robot which operates semi- or fully autonomously to perform services useful to the well-being of humans and equipment, excluding manufacturing operations.\"[92]\nRobots are used as educational assistants to teachers. From the 1980s, robots such as turtles were used in schools and programmed using the Logo language.[93][94]\nThere are robot kits like Lego Mindstorms, BIOLOID, OLLO from ROBOTIS, or BotBrain Educational Robots can help children to learn about mathematics, physics, programming, and electronics. Robotics have also been introduced into the lives of elementary and high school students in the form of robot competitions with the company FIRST (For Inspiration and Recognition of Science and Technology). The organization is the foundation for the FIRST Robotics Competition, FIRST Tech Challenge, FIRST Lego League Challenge and FIRST Lego League Explore competitions.\nThere have also been robots such as the teaching computer, Leachim (1974).[95] Leachim was an early example of speech synthesis using the using the Diphone synthesis method. 2-XL (1976) was a robot shaped game / teaching toy based on branching between audible tracks on an 8-track tape player, both invented by Michael J. Freeman.[96] Later, the 8-track was upgraded to tape cassettes and then to digital.\nModular robots are a new breed of robots that are designed to increase the use of robots by modularizing their architecture.[97] The functionality and effectiveness of a modular robot is easier to increase compared to conventional robots. These robots are composed of a single type of identical, several different identical module types, or similarly shaped modules, which vary in size. Their architectural structure allows hyper-redundancy for modular robots, as they can be designed with more than 8 degrees of freedom (DOF). Creating the programming, inverse kinematics and dynamics for modular robots is more complex than with traditional robots. Modular robots may be composed of L-shaped modules, cubic modules, and U and H-shaped modules. ANAT technology, an early modular robotic technology patented by Robotics Design Inc., allows the creation of modular robots from U- and H-shaped modules that connect in a chain, and are used to form heterogeneous and homogenous modular robot systems. These \"ANAT robots\" can be designed with \"n\" DOF as each module is a complete motorized robotic system that folds relatively to the modules connected before and after it in its chain, and therefore a single module allows one degree of freedom. The more modules that are connected to one another, the more degrees of freedom it will have. L-shaped modules can also be designed in a chain, and must become increasingly smaller as the size of the chain increases, as payloads attached to the end of the chain place a greater strain on modules that are further from the base. ANAT H-shaped modules do not suffer from this problem, as their design allows a modular robot to distribute pressure and impacts evenly amongst other attached modules, and therefore payload-carrying capacity does not decrease as the length of the arm increases. Modular robots can be manually or self-reconfigured to form a different robot, that may perform different applications. Because modular robots of the same architecture type are composed of modules that compose different modular robots, a snake-arm robot can combine with another to form a dual or quadra-arm robot, or can split into several mobile robots, and mobile robots can split into multiple smaller ones, or combine with others into a larger or different one. This allows a single modular robot the ability to be fully specialized in a single task, as well as the capacity to be specialized to perform multiple different tasks.\nModular robotic technology is currently being applied in hybrid transportation,[98] industrial automation,[99] duct cleaning[100] and handling. Many research centres and universities have also studied this technology, and have developed prototypes.\nA collaborative robot or cobot is a robot that can safely and effectively interact with human workers while performing simple industrial tasks. However, end-effectors and other environmental conditions may create hazards, and as such risk assessments should be done before using any industrial motion-control application.[101]\nThe collaborative robots most widely used in industries today are manufactured by Universal Robots in Denmark.[102]\nRethink Robotics—founded by Rodney Brooks, previously with iRobot—introduced Baxter in September 2012; as an industrial robot designed to safely interact with neighboring human workers, and be programmable for performing simple tasks.[103] Baxters stop if they detect a human in the way of their robotic arms and have prominent off switches. Intended for sale to small businesses, they are promoted as the robotic analogue of the personal computer.[104] As of May 2014[update], 190 companies in the US have bought Baxters and they are being used commercially in the UK.[10]\nRoughly half of all the robots in the world are in Asia, 32% in Europe, and 16% in North America, 1% in Australasia and 1% in Africa.[107] 40% of all the robots in the world are in Japan,[108] making Japan the country with the highest number of robots.\nAs robots have become more advanced and sophisticated, experts and academics have increasingly explored the questions of what ethics might govern robots' behavior,[110][111] and whether robots might be able to claim any kind of social, cultural, ethical or legal rights.[112] One scientific team has said that it was possible that a robot brain would exist by 2019.[113] Others predict robot intelligence breakthroughs by 2050.[114] Recent advances have made robotic behavior more sophisticated.[115] The social impact of intelligent robots is subject of a 2010 documentary film called Plug \u0026 Pray.[116]\nVernor Vinge has suggested that a moment may come when computers and robots are smarter than humans. He calls this \"the Singularity\".[117] He suggests that it may be somewhat or possibly very dangerous for humans.[118] This is discussed by a philosophy called Singularitarianism.\nIn 2009, experts attended a conference hosted by the Association for the Advancement of Artificial Intelligence (AAAI) to discuss whether computers and robots might be able to acquire any autonomy, and how much these abilities might pose a threat or hazard. They noted that some robots have acquired various forms of semi-autonomy, including being able to find power sources on their own and being able to independently choose targets to attack with weapons. They also noted that some computer viruses can evade elimination and have achieved \"cockroach intelligence.\" They noted that self-awareness as depicted in science-fiction is probably unlikely, but that there were other potential hazards and pitfalls.[117] Various media sources and scientific groups have noted separate trends in differing areas which might together result in greater robotic functionalities and autonomy, and which pose some inherent concerns.[119][120][121]\nSome experts and academics have questioned the use of robots for military combat, especially when such robots are given some degree of autonomous functions.[122] There are also concerns about technology which might allow some armed robots to be controlled mainly by other robots.[123] The US Navy has funded a report which indicates that, as military robots become more complex, there should be greater attention to implications of their ability to make autonomous decisions.[124][125] One researcher states that autonomous robots might be more humane, as they could make decisions more effectively. However, other experts question this.[126]\nOne robot in particular, the EATR, has generated public concerns[127] over its fuel source, as it can continually refuel itself using organic substances.[128] Although the engine for the EATR is designed to run on biomass and vegetation[129] specifically selected by its sensors, which it can find on battlefields or other local environments, the project has stated that chicken fat can also be used.[130]\nManuel De Landa has noted that \"smart missiles\" and autonomous bombs equipped with artificial perception can be considered robots, as they make some of their decisions autonomously. He believes this represents an important and dangerous trend in which humans are handing over important decisions to machines.[131]\nFor centuries, people have predicted that machines would make workers obsolete and increase unemployment, although the causes of unemployment are usually thought to be due to social policy.[132][133][134]\nA recent example of human replacement involves Taiwanese technology company Foxconn who, in July 2011, announced a three-year plan to replace workers with more robots. At present the company uses ten thousand robots but will increase them to a million robots over a three-year period.[135]\nLawyers have speculated that an increased prevalence of robots in the workplace could lead to the need to improve redundancy laws.[136]\nKevin J. Delaney said \"Robots are taking human jobs. But Bill Gates believes that governments should tax companies' use of them, as a way to at least temporarily slow the spread of automation and to fund other types of employment.\"[137] The robot tax would also help pay a guaranteed living wage to the displaced workers.\nThe World Bank's World Development Report 2019 puts forth evidence showing that while automation displaces workers, technological innovation creates more new industries and jobs on balance.[138]\nAt present, there are two main types of robots, based on their use: general-purpose autonomous robots and dedicated robots.\nRobots can be classified by their specificity of purpose. A robot might be designed to perform one particular task extremely well, or a range of tasks less well. All robots by their nature can be re-programmed to behave differently, but some are limited by their physical form. For example, a factory robot arm can perform jobs such as cutting, welding, gluing, or acting as a fairground ride, while a pick-and-place robot can only populate printed circuit boards.\nGeneral-purpose autonomous robots can perform a variety of functions independently. General-purpose autonomous robots typically can navigate independently in known spaces, handle their own re-charging needs, interface with electronic doors and elevators and perform other basic tasks. Like computers, general-purpose robots can link with networks, software and accessories that increase their usefulness. They may recognize people or objects, talk, provide companionship, monitor environmental quality, respond to alarms, pick up supplies and perform other useful tasks. General-purpose robots may perform a variety of functions simultaneously or they may take on different roles at different times of day. Some such robots try to mimic human beings and may even resemble people in appearance; this type of robot is called a humanoid robot. Humanoid robots are still in a very limited stage, as no humanoid robot can, as of yet, actually navigate around a room that it has never been in.[139] Thus, humanoid robots are really quite limited, despite their intelligent behaviors in their well-known environments.\nOver the last three decades, automobile factories have become dominated by robots. A typical factory contains hundreds of industrial robots working on fully automated production lines, with one robot for every ten human workers. On an automated production line, a vehicle chassis on a conveyor is welded, glued, painted and finally assembled at a sequence of robot stations.\nIndustrial robots are also used extensively for palletizing and packaging of manufactured goods, for example for rapidly taking drink cartons from the end of a conveyor belt and placing them into boxes, or for loading and unloading machining centers.\nMass-produced printed circuit boards (PCBs) are almost exclusively manufactured by pick-and-place robots, typically with SCARA manipulators, which remove tiny electronic components from strips or trays, and place them on to PCBs with great accuracy.[140] Such robots can place hundreds of thousands of components per hour, far out-performing a human in speed, accuracy, and reliability.[141]\nMobile robots, following markers or wires in the floor, or using vision[86] or lasers, are used to transport goods around large facilities, such as warehouses, container ports, or hospitals.[142]\nLimited to tasks that could be accurately defined and had to be performed the same way every time. Very little feedback or intelligence was required, and the robots needed only the most basic exteroceptors (sensors). The limitations of these AGVs are that their paths are not easily altered and they cannot alter their paths if obstacles block them. If one AGV breaks down, it may stop the entire operation.\nDeveloped to deploy triangulation from beacons or bar code grids for scanning on the floor or ceiling. In most factories, triangulation systems tend to require moderate to high maintenance, such as daily cleaning of all beacons or bar codes. Also, if a tall pallet or large vehicle blocks beacons or a bar code is marred, AGVs may become lost. Often such AGVs are designed to be used in human-free environments.\nSuch as SmartLoader,[143] SpeciMinder,[144] ADAM,[145] Tug[146] Eskorta,[147] and MT 400 with Motivity[148] are designed for people-friendly workspaces. They navigate by recognizing natural features. 3D scanners or other means of sensing the environment in two or three dimensions help to eliminate cumulative errors in dead-reckoning calculations of the AGV's current position. Some AGVs can create maps of their environment using scanning lasers with simultaneous localization and mapping (SLAM) and use those maps to navigate in real time with other path planning and obstacle avoidance algorithms. They are able to operate in complex environments and perform non-repetitive and non-sequential tasks such as transporting photomasks in a semiconductor lab, specimens in hospitals and goods in warehouses. For dynamic areas, such as warehouses full of pallets, AGVs require additional strategies using three-dimensional sensors such as time-of-flight or stereovision cameras.\nThere are many jobs that humans would rather leave to robots. The job may be boring, such as domestic cleaning or sports field line marking, or dangerous, such as exploring inside a volcano.[149] Other jobs are physically inaccessible, such as exploring another planet,[150] cleaning the inside of a long pipe, or performing laparoscopic surgery.[151]\nAlmost every unmanned space probe ever launched was a robot.[152][153] Some were launched in the 1960s with very limited abilities, but their ability to fly and land (in the case of Luna 9) is an indication of their status as a robot. This includes the Voyager probes and the Galileo probes, among others.\nTeleoperated robots, or telerobots, are devices remotely operated from a distance by a human operator rather than following a predetermined sequence of movements, but which has semi-autonomous behaviour. They are used when a human cannot be present on site to perform a job because it is dangerous, far away, or inaccessible. The robot may be in another room or another country, or may be on a very different scale to the operator. For instance, a laparoscopic surgery robot allows the surgeon to work inside a human patient on a relatively small scale compared to open surgery, significantly shortening recovery time.[151] They can also be used to avoid exposing workers to the hazardous and tight spaces such as in duct cleaning. When disabling a bomb, the operator sends a small robot to disable it. Several authors have been using a device called the Longpen to sign books remotely.[154] Teleoperated robot aircraft, like the Predator Unmanned Aerial Vehicle, are increasingly being used by the military. These pilotless drones can search terrain and fire on targets.[155][156] Hundreds of robots such as iRobot's Packbot and the Foster-Miller TALON are being used in Iraq and Afghanistan by the U.S. military to defuse roadside bombs or improvised explosive devices (IEDs) in an activity known as explosive ordnance disposal (EOD).[157]\nRobots are used to automate picking fruit on orchards at a cost lower than that of human pickers.\nDomestic robots are simple robots dedicated to a single task work in home use. They are used in simple but often disliked jobs, such as vacuum cleaning, floor washing, and lawn mowing. An example of a domestic robot is a Roomba.\nMilitary robots include the SWORDS robot which is currently used in ground-based combat. It can use a variety of weapons and there is some discussion of giving it some degree of autonomy in battleground situations.[158][159][160]\nUnmanned combat air vehicles (UCAVs), which are an upgraded form of UAVs, can do a wide variety of missions, including combat. UCAVs are being designed such as the BAE Systems Mantis which would have the ability to fly themselves, to pick their own course and target, and to make most decisions on their own.[161] The BAE Taranis is a UCAV built by Great Britain which can fly across continents without a pilot and has new means to avoid detection.[162] Flight trials are expected to begin in 2011.[163]\nThe AAAI has studied this topic in depth[110] and its president has commissioned a study to look at this issue.[164]\nSome have suggested a need to build \"Friendly AI\", meaning that the advances which are already occurring with AI should also include an effort to make AI intrinsically friendly and humane.[165] Several such measures reportedly already exist, with robot-heavy countries such as Japan and South Korea[166] having begun to pass regulations requiring robots to be equipped with safety systems, and possibly sets of 'laws' akin to Asimov's Three Laws of Robotics.[167][168] An official report was issued in 2009 by the Japanese government's Robot Industry Policy Committee.[169] Chinese officials and researchers have issued a report suggesting a set of ethical rules, and a set of new legal guidelines referred to as \"Robot Legal Studies.\"[170] Some concern has been expressed over a possible occurrence of robots telling apparent falsehoods.[171]\nMining robots are designed to solve a number of problems currently facing the mining industry, including skills shortages, improving productivity from declining ore grades, and achieving environmental targets. Due to the hazardous nature of mining, in particular underground mining, the prevalence of autonomous, semi-autonomous, and tele-operated robots has greatly increased in recent times. A number of vehicle manufacturers provide autonomous trains, trucks and loaders that will load material, transport it on the mine site to its destination, and unload without requiring human intervention. One of the world's largest mining corporations, Rio Tinto, has recently expanded its autonomous truck fleet to the world's largest, consisting of 150 autonomous Komatsu trucks, operating in Western Australia.[172] Similarly, BHP has announced the expansion of its autonomous drill fleet to the world's largest, 21 autonomous Atlas Copco drills.[173]\nDrilling, longwall and rockbreaking machines are now also available as autonomous robots.[174] The Atlas Copco Rig Control System can autonomously execute a drilling plan on a drilling rig, moving the rig into position using GPS, set up the drill rig and drill down to specified depths.[175] Similarly, the Transmin Rocklogic system can automatically plan a path to position a rockbreaker at a selected destination.[176] These systems greatly enhance the safety and efficiency of mining operations.\nRobots in healthcare have two main functions. Those which assist an individual, such as a sufferer of a disease like Multiple Sclerosis, and those which aid in the overall systems such as pharmacies and hospitals.\nRobots used in home automation have developed over time from simple basic robotic assistants, such as the Handy 1,[177] through to semi-autonomous robots, such as FRIEND which can assist the elderly and disabled with common tasks.\nThe population is aging in many countries, especially Japan, meaning that there are increasing numbers of elderly people to care for, but relatively fewer young people to care for them.[178][179] Humans make the best carers, but where they are unavailable, robots are gradually being introduced.[180]\nFRIEND is a semi-autonomous robot designed to support disabled and elderly people in their daily life activities, like preparing and serving a meal. FRIEND make it possible for patients who are paraplegic, have muscle diseases or serious paralysis (due to strokes etc.), to perform tasks without help from other people like therapists or nursing staff.\nScript Pro manufactures a robot designed to help pharmacies fill prescriptions that consist of oral solids or medications in pill form.[181][better source needed] The pharmacist or pharmacy technician enters the prescription information into its information system. The system, upon determining whether or not the drug is in the robot, will send the information to the robot for filling. The robot has 3 different size vials to fill determined by the size of the pill. The robot technician, user, or pharmacist determines the needed size of the vial based on the tablet when the robot is stocked. Once the vial is filled it is brought up to a conveyor belt that delivers it to a holder that spins the vial and attaches the patient label. Afterwards it is set on another conveyor that delivers the patient's medication vial to a slot labeled with the patient's name on an LED read out. The pharmacist or technician then checks the contents of the vial to ensure it's the correct drug for the correct patient and then seals the vials and sends it out front to be picked up.\nMcKesson's Robot RX is another healthcare robotics product that helps pharmacies dispense thousands of medications daily with little or no errors.[182] The robot can be ten feet wide and thirty feet long and can hold hundreds of different kinds of medications and thousands of doses. The pharmacy saves many resources like staff members that are otherwise unavailable in a resource scarce industry. It uses an electromechanical head coupled with a pneumatic system to capture each dose and deliver it to either its stocked or dispensed location. The head moves along a single axis while it rotates 180 degrees to pull the medications. During this process it uses barcode technology to verify it's pulling the correct drug. It then delivers the drug to a patient specific bin on a conveyor belt. Once the bin is filled with all of the drugs that a particular patient needs and that the robot stocks, the bin is then released and returned out on the conveyor belt to a technician waiting to load it into a cart for delivery to the floor.\nWhile most robots today are installed in factories or homes, performing labour or life saving jobs, many new types of robot are being developed in laboratories around the world. Much of the research in robotics focuses not on specific industrial tasks, but on investigations into new types of robot, alternative ways to think about or design robots, and new ways to manufacture them. It is expected that these new types of robot will be able to solve real world problems when they are finally realized.[citation needed]\nOne approach to designing robots is to base them on animals. BionicKangaroo was designed and engineered by studying and applying the physiology and methods of locomotion of a kangaroo.\nNanorobotics is the emerging technology field of creating machines or robots whose components are at or close to the microscopic scale of a nanometer (10−9 meters). Also known as \"nanobots\" or \"nanites\", they would be constructed from molecular machines. So far, researchers have mostly produced only parts of these complex systems, such as bearings, sensors, and synthetic molecular motors, but functioning robots have also been made such as the entrants to the Nanobot Robocup contest.[183] Researchers also hope to be able to create entire robots as small as viruses or bacteria, which could perform tasks on a tiny scale. Possible applications include micro surgery (on the level of individual cells), utility fog,[184] manufacturing, weaponry and cleaning.[185] Some people have suggested that if there were nanobots which could reproduce, the earth would turn into \"grey goo\", while others argue that this hypothetical outcome is nonsense.[186][187]\nA few researchers have investigated the possibility of creating robots which can alter their physical form to suit a particular task,[188] like the fictional T-1000. Real robots are nowhere near that sophisticated however, and mostly consist of a small number of cube shaped units, which can move relative to their neighbours. Algorithms have been designed in case any such robots become a reality.[189]\nIn July 2020 scientists reported the development of a mobile robot chemist and demonstrate that it can assist in experimental searches. According to the scientists their strategy was automating the researcher rather than the instruments – freeing up time for the human researchers to think creatively – and could identify photocatalyst mixtures for hydrogen production from water that were six times more active than initial formulations. The modular robot can operate laboratory instruments, work nearly around the clock, and autonomously make decisions on his next actions depending on experimental results.[190][191]\nRobots with silicone bodies and flexible actuators (air muscles, electroactive polymers, and ferrofluids) look and feel different from robots with rigid skeletons, and can have different behaviors.[192] Soft, flexible (and sometimes even squishy) robots are often designed to mimic the biomechanics of animals and other things found in nature, which is leading to new applications in medicine, care giving, search and rescue, food handling and manufacturing, and scientific exploration.[193][194]\nInspired by colonies of insects such as ants and bees, researchers are modeling the behavior of swarms of thousands of tiny robots which together perform a useful task, such as finding something hidden, cleaning, or spying. Each robot is quite simple, but the emergent behavior of the swarm is more complex. The whole set of robots can be considered as one single distributed system, in the same way an ant colony can be considered a superorganism, exhibiting swarm intelligence. The largest swarms so far created include the iRobot swarm, the SRI/MobileRobots CentiBots project[195] and the Open-source Micro-robotic Project swarm, which are being used to research collective behaviors.[196][197] Swarms are also more resistant to failure. Whereas one large robot may fail and ruin a mission, a swarm can continue even if several robots fail. This could make them attractive for space exploration missions, where failure is normally extremely costly.[198]\nRobotics also has application in the design of virtual reality interfaces. Specialized robots are in widespread use in the haptic research community. These robots, called \"haptic interfaces\", allow touch-enabled user interaction with real and virtual environments. Robotic forces allow simulating the mechanical properties of \"virtual\" objects, which users can experience through their sense of touch.[199]\nRobots are used by contemporary artists to create works that include mechanical automation. There are many branches of robotic art, one of which is robotic installation art, a type of installation art that is programmed to respond to viewer interactions, by means of computers, sensors and actuators. The future behavior of such installations can therefore be altered by input from either the artist or the participant, which differentiates these artworks from other types of kinetic art.\nLe Grand Palais in Paris organized an exhibition \"Artists \u0026 Robots\", featuring artworks created by more than forty artists with the help of robots in 2018.[200]\nRobotic characters, androids (artificial men/women) or gynoids (artificial women), and cyborgs (also \"bionic men/women\", or humans with significant mechanical enhancements) have become a staple of science fiction.\nThe first reference in Western literature to mechanical servants appears in Homer's Iliad. In Book XVIII, Hephaestus, god of fire, creates new armor for the hero Achilles, assisted by robots.[201] According to the Rieu translation, \"Golden maidservants hastened to help their master. They looked like real women and could not only speak and use their limbs but were endowed with intelligence and trained in handwork by the immortal gods.\" The words \"robot\" or \"android\" are not used to describe them, but they are nevertheless mechanical devices human in appearance. \"The first use of the word Robot was in Karel Čapek's play R.U.R. (Rossum's Universal Robots) (written in 1920)\". Writer Karel Čapek was born in Czechoslovakia (Czech Republic).\nPossibly the most prolific author of the twentieth century was Isaac Asimov (1920–1992)[202] who published over five-hundred books.[203] Asimov is probably best remembered for his science-fiction stories and especially those about robots, where he placed robots and their interaction with society at the center of many of his works.[204][205] Asimov carefully considered the problem of the ideal set of instructions robots might be given to lower the risk to humans, and arrived at his Three Laws of Robotics: a robot may not injure a human being or, through inaction, allow a human being to come to harm; a robot must obey orders given it by human beings, except where such orders would conflict with the First Law; and a robot must protect its own existence as long as such protection does not conflict with the First or Second Law.[206] These were introduced in his 1942 short story \"Runaround\", although foreshadowed in a few earlier stories. Later, Asimov added the Zeroth Law: \"A robot may not harm humanity, or, by inaction, allow humanity to come to harm\"; the rest of the laws are modified sequentially to acknowledge this.\nAccording to the Oxford English Dictionary, the first passage in Asimov's short story \"Liar!\" (1941) that mentions the First Law is the earliest recorded use of the word robotics. Asimov was not initially aware of this; he assumed the word already existed by analogy with mechanics, hydraulics, and other similar terms denoting branches of applied knowledge.[207]\nRobots are used in a number of competitive events. Robot combat competitions have been popularized by television shows such as Robot Wars and BattleBots, featuring mostly remotely controlled 'robots' that compete against each other directly using various weaponry, there are also amateur robot combat leagues active globally outside of the televised events. Micromouse events, in which autonomous robots compete to solve mazes or other obstacle courses are also held internationally.\nRobot competitions are also often used within educational settings to introduce the concept of robotics to children such as the FIRST Robotics Competition in the US.\nRobots appear in many films. Most of the robots in cinema are fictional. Two of the most famous are R2-D2 and C-3PO from the Star Wars franchise.\nThe concept of humanoid sex robots has drawn public attention and elicited debate regarding their supposed benefits and potential effects on society. Opponents argue that the introduction of such devices would be socially harmful, and demeaning to women and children,[208] while proponents cite their potential therapeutical benefits, particularly in aiding people with dementia or depression.[209]\nFears and concerns about robots have been repeatedly expressed in a wide range of books and films. A common theme is the development of a master race of conscious and highly intelligent robots, motivated to take over or destroy the human race. Frankenstein (1818), often called the first science fiction novel, has become synonymous with the theme of a robot or android advancing beyond its creator.\nOther works with similar themes include The Mechanical Man, The Terminator, Runaway, RoboCop, the Replicators in Stargate, the Cylons in Battlestar Galactica, the Cybermen and Daleks in Doctor Who, The Matrix, Enthiran and I, Robot. Some fictional robots are programmed to kill and destroy; others gain superhuman intelligence and abilities by upgrading their own software and hardware. Examples of popular media where the robot becomes evil are 2001: A Space Odyssey, Red Planet and Enthiran.\nThe 2017 game Horizon Zero Dawn explores themes of robotics in warfare, robot ethics, and the AI control problem, as well as the positive or negative impact such technologies could have on the environment.\nAnother common theme is the reaction, sometimes called the \"uncanny valley\", of unease and even revulsion at the sight of robots that mimic humans too closely.[109]\nMore recently, fictional representations of artificially intelligent robots in films such as A.I. Artificial Intelligence and Ex Machina and the 2016 TV adaptation of Westworld have engaged audience sympathy for the robots themselves."}
{"url":"https://en.wikipedia.org/wiki/Reinforcement_learning","text":"Reinforcement learning (RL) is an interdisciplinary area of machine learning and optimal control concerned with how an intelligent agent ought to take actions in a dynamic environment in order to maximize the cumulative reward. Reinforcement learning is one of three basic machine learning paradigms, alongside supervised learning and unsupervised learning.\nReinforcement learning differs from supervised learning in not needing labelled input/output pairs to be presented, and in not needing sub-optimal actions to be explicitly corrected. Instead the focus is on finding a balance between exploration (of uncharted territory) and exploitation (of current knowledge) with the goal of maximizing the long term reward, whose feedback might be incomplete or delayed.[1]\n\nThe environment is typically stated in the form of a Markov decision process (MDP), because many reinforcement learning algorithms for this context use dynamic programming techniques.[2] The main difference between the classical dynamic programming methods and reinforcement learning algorithms is that the latter do not assume knowledge of an exact mathematical model of the Markov decision process and they target large Markov decision processes where exact methods become infeasible.[3] .mw-parser-output .toclimit-2 .toclevel-1 ul,.mw-parser-output .toclimit-3 .toclevel-2 ul,.mw-parser-output .toclimit-4 .toclevel-3 ul,.mw-parser-output .toclimit-5 .toclevel-4 ul,.mw-parser-output .toclimit-6 .toclevel-5 ul,.mw-parser-output .toclimit-7 .toclevel-6 ul{display:none}Due to its generality, reinforcement learning is studied in many disciplines, such as game theory, control theory, operations research, information theory, simulation-based optimization, multi-agent systems, swarm intelligence, and statistics. In the operations research and control literature, reinforcement learning is called approximate dynamic programming, or neuro-dynamic programming. The problems of interest in reinforcement learning have also been studied in the theory of optimal control, which is concerned mostly with the existence and characterization of optimal solutions, and algorithms for their exact computation, and less with learning or approximation, particularly in the absence of a mathematical model of the environment.\nBasic reinforcement learning is modeled as a Markov decision process:\nThe purpose of reinforcement learning is for the agent to learn an optimal, or nearly-optimal, policy that maximizes the \"reward function\" or other user-provided reinforcement signal that accumulates from the immediate rewards. This is similar to processes that appear to occur in animal psychology. For example, biological brains are hardwired to interpret signals such as pain and hunger as negative reinforcements, and interpret pleasure and food intake as positive reinforcements. In some circumstances, animals can learn to engage in behaviors that optimize these rewards. This suggests that animals are capable of reinforcement learning.[4][5]\nA basic reinforcement learning agent AI interacts with its environment in discrete time steps. At each time t, the agent receives the current state \n  \n    \n      \n        \n          S\n          \n            t\n          \n        \n      \n    \n    {\\displaystyle S_{t}}\n  \n and reward \n  \n    \n      \n        \n          R\n          \n            t\n          \n        \n      \n    \n    {\\displaystyle R_{t}}\n  \n. It then chooses an action \n  \n    \n      \n        \n          A\n          \n            t\n          \n        \n      \n    \n    {\\displaystyle A_{t}}\n  \n from the set of available actions, which is subsequently sent to the environment. The environment moves to a new state \n  \n    \n      \n        \n          S\n          \n            t\n            +\n            1\n          \n        \n      \n    \n    {\\displaystyle S_{t+1}}\n  \n and the reward \n  \n    \n      \n        \n          R\n          \n            t\n            +\n            1\n          \n        \n      \n    \n    {\\displaystyle R_{t+1}}\n  \n associated with the transition \n  \n    \n      \n        (\n        \n          S\n          \n            t\n          \n        \n        ,\n        \n          A\n          \n            t\n          \n        \n        ,\n        \n          S\n          \n            t\n            +\n            1\n          \n        \n        )\n      \n    \n    {\\displaystyle (S_{t},A_{t},S_{t+1})}\n  \n is determined. The goal of a reinforcement learning agent is to learn a policy: \n  \n    \n      \n        π\n        :\n        \n          \n            S\n          \n        \n        ×\n        \n          \n            A\n          \n        \n        →\n        [\n        0\n        ,\n        1\n        ]\n      \n    \n    {\\displaystyle \\pi :{\\mathcal {S}}\\times {\\mathcal {A}}\\rightarrow [0,1]}\n  \n, \n  \n    \n      \n        π\n        (\n        s\n        ,\n        a\n        )\n        =\n        Pr\n        (\n        \n          A\n          \n            t\n          \n        \n        =\n        a\n        ∣\n        \n          S\n          \n            t\n          \n        \n        =\n        s\n        )\n      \n    \n    {\\displaystyle \\pi (s,a)=\\Pr(A_{t}=a\\mid S_{t}=s)}\n  \n that maximizes the expected cumulative reward.\nFormulating the problem as an Markov decision process assumes the agent directly observes the current environmental state; in this case the problem is said to have full observability. If the agent only has access to a subset of states, or if the observed states are corrupted by noise, the agent is said to have partial observability, and formally the problem must be formulated as a Partially observable Markov decision process. In both cases, the set of actions available to the agent can be restricted. For example, the state of an account balance could be restricted to be positive; if the current value of the state is 3 and the state transition attempts to reduce the value by 4, the transition will not be allowed.\nWhen the agent's performance is compared to that of an agent that acts optimally, the difference in performance gives rise to the notion of regret. In order to act near optimally, the agent must reason about the long-term consequences of its actions (i.e., maximize future income), although the immediate reward associated with this might be negative.\nThus, reinforcement learning is particularly well-suited to problems that include a long-term versus short-term reward trade-off. It has been applied successfully to various problems, including energy storage operation,[6] robot control,[7] photovoltaic generators dispatch,[8] backgammon, checkers,[9] Go (AlphaGo), and autonomous driving systems.[10]\nTwo elements make reinforcement learning powerful: the use of samples to optimize performance and the use of function approximation to deal with large environments. Thanks to these two key components, reinforcement learning can be used in large environments in the following situations:\nThe first two of these problems could be considered planning problems (since some form of model is available), while the last one could be considered to be a genuine learning problem. However, reinforcement learning converts both planning problems to machine learning problems.\nThe exploration vs. exploitation trade-off has been most thoroughly studied through the multi-armed bandit problem and for finite state space Markov decision processes in Burnetas and Katehakis (1997).[12]\nReinforcement learning requires clever exploration mechanisms; randomly selecting actions, without reference to an estimated probability distribution, shows poor performance. The case of (small) finite Markov decision processes is relatively well understood. However, due to the lack of algorithms that scale well with the number of states (or scale to problems with infinite state spaces), simple exploration methods are the most practical.\nOne such method is \n  \n    \n      \n        ε\n      \n    \n    {\\displaystyle \\varepsilon }\n  \n-greedy, where \n  \n    \n      \n        0\n        \u003c\n        ε\n        \u003c\n        1\n      \n    \n    {\\displaystyle 0\u003c\\varepsilon \u003c1}\n  \n is a parameter controlling the amount of exploration vs. exploitation.  With probability \n  \n    \n      \n        1\n        −\n        ε\n      \n    \n    {\\displaystyle 1-\\varepsilon }\n  \n, exploitation is chosen, and the agent chooses the action that it believes has the best long-term effect (ties between actions are broken uniformly at random). Alternatively, with probability \n  \n    \n      \n        ε\n      \n    \n    {\\displaystyle \\varepsilon }\n  \n, exploration is chosen, and the action is chosen uniformly at random. \n  \n    \n      \n        ε\n      \n    \n    {\\displaystyle \\varepsilon }\n  \n is usually a fixed parameter but can be adjusted either according to a schedule (making the agent explore progressively less), or adaptively based on heuristics.[13]\nEven if the issue of exploration is disregarded and even if the state was observable (assumed hereafter), the problem remains to use past experience to find out which actions lead to higher cumulative rewards.\nThe agent's action selection is modeled as a map called policy:\nThe policy map gives the probability of taking action \n  \n    \n      \n        a\n      \n    \n    {\\displaystyle a}\n  \n when in state \n  \n    \n      \n        s\n      \n    \n    {\\displaystyle s}\n  \n.[14]: 61  There are also deterministic policies.\nThe state-value function \n  \n    \n      \n        \n          V\n          \n            π\n          \n        \n        (\n        s\n        )\n      \n    \n    {\\displaystyle V_{\\pi }(s)}\n  \n is defined as, expected discounted return starting with state \n  \n    \n      \n        s\n      \n    \n    {\\displaystyle s}\n  \n, i.e. \n  \n    \n      \n        \n          S\n          \n            0\n          \n        \n        =\n        s\n      \n    \n    {\\displaystyle S_{0}=s}\n  \n, and successively following policy \n  \n    \n      \n        π\n      \n    \n    {\\displaystyle \\pi }\n  \n. Hence, roughly speaking, the value function estimates \"how good\" it is to be in a given state.[14]: 60 \nwhere the random variable \n  \n    \n      \n        G\n      \n    \n    {\\displaystyle G}\n  \n denotes the discounted return, and is defined as the sum of future discounted rewards:\nwhere \n  \n    \n      \n        \n          R\n          \n            t\n            +\n            1\n          \n        \n      \n    \n    {\\displaystyle R_{t+1}}\n  \n is the reward for transitioning from state \n  \n    \n      \n        \n          S\n          \n            t\n          \n        \n      \n    \n    {\\displaystyle S_{t}}\n  \n to \n  \n    \n      \n        \n          S\n          \n            t\n            +\n            1\n          \n        \n      \n    \n    {\\displaystyle S_{t+1}}\n  \n, \n  \n    \n      \n        0\n        ≤\n        γ\n        \u003c\n        1\n      \n    \n    {\\displaystyle 0\\leq \\gamma \u003c1}\n  \n is the discount rate. \n  \n    \n      \n        γ\n      \n    \n    {\\displaystyle \\gamma }\n  \n is less than 1, so rewards in the distant future are weighted less than rewards in the immediate future.\nThe algorithm must find a policy with maximum expected discounted return. From the theory of Markov decision processes it is known that, without loss of generality, the search can be restricted to the set of so-called stationary policies. A policy is stationary if the action-distribution returned by it depends only on the last state visited (from the observation agent's history). The search can be further restricted to deterministic stationary policies. A deterministic stationary policy deterministically selects actions based on the current state. Since any such policy can be identified with a mapping from the set of states to the set of actions, these policies can be identified with such mappings with no loss of generality.\nThe brute force approach entails two steps:\nOne problem with this is that the number of policies can be large, or even infinite. Another is that the variance of the returns may be large, which requires many samples to accurately estimate the discounted return of each policy.\nThese problems can be ameliorated if we assume some structure and allow samples generated from one policy to influence the estimates made for others. The two main approaches for achieving this are value function estimation and direct policy search.\nValue function approaches attempt to find a policy that maximizes the discounted return by maintaining a set of estimates of expected discounted returns \n  \n    \n      \n        \n          \n            E\n          \n        \n        ⁡\n        [\n        G\n        ]\n      \n    \n    {\\displaystyle \\operatorname {\\mathbb {E} } [G]}\n  \n for some policy (usually either the \"current\" [on-policy] or the optimal [off-policy] one).\nThese methods rely on the theory of Markov decision processes, where optimality is defined in a sense stronger than the one above: A policy is optimal if it achieves the best-expected discounted return from any initial state (i.e., initial distributions play no role in this definition). Again, an optimal policy can always be found among stationary policies.\nTo define optimality in a formal manner, define the state-value of a policy \n  \n    \n      \n        π\n      \n    \n    {\\displaystyle \\pi }\n  \n by\nwhere \n  \n    \n      \n        G\n      \n    \n    {\\displaystyle G}\n  \n stands for the discounted return associated with following \n  \n    \n      \n        π\n      \n    \n    {\\displaystyle \\pi }\n  \n from the initial state \n  \n    \n      \n        s\n      \n    \n    {\\displaystyle s}\n  \n. Defining \n  \n    \n      \n        \n          V\n          \n            ∗\n          \n        \n        (\n        s\n        )\n      \n    \n    {\\displaystyle V^{*}(s)}\n  \n as the maximum possible state-value of \n  \n    \n      \n        \n          V\n          \n            π\n          \n        \n        (\n        s\n        )\n      \n    \n    {\\displaystyle V^{\\pi }(s)}\n  \n, where \n  \n    \n      \n        π\n      \n    \n    {\\displaystyle \\pi }\n  \n is allowed to change,\nA policy that achieves these optimal state-values in each state is called optimal. Clearly, a policy that is optimal in this strong sense is also optimal in the sense that it maximizes the expected discounted return \n  \n    \n      \n        \n          ρ\n          \n            π\n          \n        \n      \n    \n    {\\displaystyle \\rho ^{\\pi }}\n  \n, since \n  \n    \n      \n        \n          ρ\n          \n            π\n          \n        \n        =\n        \n          \n            E\n          \n        \n        ⁡\n        [\n        \n          V\n          \n            π\n          \n        \n        (\n        s\n        )\n        ]\n      \n    \n    {\\displaystyle \\rho ^{\\pi }=\\operatorname {\\mathbb {E} } [V^{\\pi }(s)]}\n  \n, where \n  \n    \n      \n        s\n      \n    \n    {\\displaystyle s}\n  \n is a state randomly sampled from the distribution \n  \n    \n      \n        μ\n      \n    \n    {\\displaystyle \\mu }\n  \n of initial states (so \n  \n    \n      \n        μ\n        (\n        s\n        )\n        =\n        Pr\n        (\n        \n          S\n          \n            0\n          \n        \n        =\n        s\n        )\n      \n    \n    {\\displaystyle \\mu (s)=\\Pr(S_{0}=s)}\n  \n).\nAlthough state-values suffice to define optimality, it is useful to define action-values. Given a state \n  \n    \n      \n        s\n      \n    \n    {\\displaystyle s}\n  \n, an action \n  \n    \n      \n        a\n      \n    \n    {\\displaystyle a}\n  \n and a policy \n  \n    \n      \n        π\n      \n    \n    {\\displaystyle \\pi }\n  \n, the action-value of the pair \n  \n    \n      \n        (\n        s\n        ,\n        a\n        )\n      \n    \n    {\\displaystyle (s,a)}\n  \n under \n  \n    \n      \n        π\n      \n    \n    {\\displaystyle \\pi }\n  \n is defined by\nwhere \n  \n    \n      \n        G\n      \n    \n    {\\displaystyle G}\n  \n now stands for the random discounted return associated with first taking action \n  \n    \n      \n        a\n      \n    \n    {\\displaystyle a}\n  \n in state \n  \n    \n      \n        s\n      \n    \n    {\\displaystyle s}\n  \n and following \n  \n    \n      \n        π\n      \n    \n    {\\displaystyle \\pi }\n  \n, thereafter.\nThe theory of Markov decision processes states that if \n  \n    \n      \n        \n          π\n          \n            ∗\n          \n        \n      \n    \n    {\\displaystyle \\pi ^{*}}\n  \n is an optimal policy, we act optimally (take the optimal action) by choosing the action from \n  \n    \n      \n        \n          Q\n          \n            \n              π\n              \n                ∗\n              \n            \n          \n        \n        (\n        s\n        ,\n        ⋅\n        )\n      \n    \n    {\\displaystyle Q^{\\pi ^{*}}(s,\\cdot )}\n  \n with the highest action-value at each state, \n  \n    \n      \n        s\n      \n    \n    {\\displaystyle s}\n  \n. The action-value function of such an optimal policy (\n  \n    \n      \n        \n          Q\n          \n            \n              π\n              \n                ∗\n              \n            \n          \n        \n      \n    \n    {\\displaystyle Q^{\\pi ^{*}}}\n  \n) is called the optimal action-value function and is commonly denoted by \n  \n    \n      \n        \n          Q\n          \n            ∗\n          \n        \n      \n    \n    {\\displaystyle Q^{*}}\n  \n. In summary, the knowledge of the optimal action-value function alone suffices to know how to act optimally.\nAssuming full knowledge of the Markov decision process, the two basic approaches to compute the optimal action-value function are value iteration and policy iteration. Both algorithms compute a sequence of functions \n  \n    \n      \n        \n          Q\n          \n            k\n          \n        \n      \n    \n    {\\displaystyle Q_{k}}\n  \n (\n  \n    \n      \n        k\n        =\n        0\n        ,\n        1\n        ,\n        2\n        ,\n        …\n      \n    \n    {\\displaystyle k=0,1,2,\\ldots }\n  \n) that converge to \n  \n    \n      \n        \n          Q\n          \n            ∗\n          \n        \n      \n    \n    {\\displaystyle Q^{*}}\n  \n. Computing these functions involves computing expectations over the whole state-space, which is impractical for all but the smallest (finite) Markov decision processes. In reinforcement learning methods, expectations are approximated by averaging over samples and using function approximation techniques to cope with the need to represent value functions over large state-action spaces.\nMonte Carlo methods can be used in an algorithm that mimics policy iteration. Policy iteration consists of two steps: policy evaluation and policy improvement.\nMonte Carlo is used in the policy evaluation step. In this step, given a stationary, deterministic policy \n  \n    \n      \n        π\n      \n    \n    {\\displaystyle \\pi }\n  \n, the goal is to compute the function values \n  \n    \n      \n        \n          Q\n          \n            π\n          \n        \n        (\n        s\n        ,\n        a\n        )\n      \n    \n    {\\displaystyle Q^{\\pi }(s,a)}\n  \n (or a good approximation to them) for all state-action pairs \n  \n    \n      \n        (\n        s\n        ,\n        a\n        )\n      \n    \n    {\\displaystyle (s,a)}\n  \n. Assume (for simplicity) that the Markov decision process is finite, that sufficient memory is available to accommodate the action-values and that the problem is episodic and after each episode a new one starts from some random initial state. Then, the estimate of the value of a given state-action pair \n  \n    \n      \n        (\n        s\n        ,\n        a\n        )\n      \n    \n    {\\displaystyle (s,a)}\n  \n can be computed by averaging the sampled returns that originated from \n  \n    \n      \n        (\n        s\n        ,\n        a\n        )\n      \n    \n    {\\displaystyle (s,a)}\n  \n over time.  Given sufficient time, this procedure can thus construct a precise estimate \n  \n    \n      \n        Q\n      \n    \n    {\\displaystyle Q}\n  \n of the action-value function \n  \n    \n      \n        \n          Q\n          \n            π\n          \n        \n      \n    \n    {\\displaystyle Q^{\\pi }}\n  \n. This finishes the description of the policy evaluation step.\nIn the policy improvement step, the next policy is obtained by computing a greedy policy with respect to \n  \n    \n      \n        Q\n      \n    \n    {\\displaystyle Q}\n  \n: Given a state \n  \n    \n      \n        s\n      \n    \n    {\\displaystyle s}\n  \n, this new policy returns an action that maximizes \n  \n    \n      \n        Q\n        (\n        s\n        ,\n        ⋅\n        )\n      \n    \n    {\\displaystyle Q(s,\\cdot )}\n  \n. In practice lazy evaluation can defer the computation of the maximizing actions to when they are needed.\nProblems with this procedure include:\nThe first problem is corrected by allowing the procedure to change the policy (at some or all states) before the values settle. This too may be problematic as it might prevent convergence. Most current algorithms do this, giving rise to the class of generalized policy iteration algorithms. Many actor-critic methods belong to this category.\nThe second issue can be corrected by allowing trajectories to contribute to any state-action pair in them. This may also help to some extent with the third problem, although a better solution when returns have high variance is Sutton's temporal difference (TD) methods that are based on the recursive Bellman equation.[15][16] The computation in TD methods can be incremental (when after each transition the memory is changed and the transition is thrown away), or batch (when the transitions are batched and the estimates are computed once based on the batch). Batch methods, such as the least-squares temporal difference method,[17] may use the information in the samples better, while incremental methods are the only choice when batch methods are infeasible due to their high computational or memory complexity. Some methods try to combine the two approaches. Methods based on temporal differences also overcome the fourth issue.\nAnother problem specific to TD comes from their reliance on the recursive Bellman equation. Most TD methods have a so-called \n  \n    \n      \n        λ\n      \n    \n    {\\displaystyle \\lambda }\n  \n parameter \n  \n    \n      \n        (\n        0\n        ≤\n        λ\n        ≤\n        1\n        )\n      \n    \n    {\\displaystyle (0\\leq \\lambda \\leq 1)}\n  \n that can continuously interpolate between Monte Carlo methods that do not rely on the Bellman equations and the basic TD methods that rely entirely on the Bellman equations. This can be effective in palliating this issue.\nIn order to address the fifth issue, function approximation methods are used. Linear function approximation starts with a mapping \n  \n    \n      \n        ϕ\n      \n    \n    {\\displaystyle \\phi }\n  \n that assigns a finite-dimensional vector to each state-action pair. Then, the action values of a state-action pair \n  \n    \n      \n        (\n        s\n        ,\n        a\n        )\n      \n    \n    {\\displaystyle (s,a)}\n  \n are obtained by linearly combining the components of \n  \n    \n      \n        ϕ\n        (\n        s\n        ,\n        a\n        )\n      \n    \n    {\\displaystyle \\phi (s,a)}\n  \n with some weights \n  \n    \n      \n        θ\n      \n    \n    {\\displaystyle \\theta }\n  \n:\nThe algorithms then adjust the weights, instead of adjusting the values associated with the individual state-action pairs. Methods based on ideas from nonparametric statistics (which can be seen to construct their own features) have been explored.\nValue iteration can also be used as a starting point, giving rise to the Q-learning algorithm and its many variants.[18] Including Deep Q-learning methods when a neural network is used to represent Q, with various applications in stochastic search problems.[19]\nThe problem with using action-values is that they may need highly precise estimates of the competing action values that can be hard to obtain when the returns are noisy, though this problem is mitigated to some extent by temporal difference methods. Using the so-called compatible function approximation method compromises generality and efficiency.\nAn alternative method is to search directly in (some subset of) the policy space, in which case the problem becomes a case of stochastic optimization. The two approaches available are gradient-based and gradient-free methods.\nGradient-based methods (policy gradient methods) start with a mapping from a finite-dimensional (parameter) space to the space of policies: given the parameter vector \n  \n    \n      \n        θ\n      \n    \n    {\\displaystyle \\theta }\n  \n, let \n  \n    \n      \n        \n          π\n          \n            θ\n          \n        \n      \n    \n    {\\displaystyle \\pi _{\\theta }}\n  \n denote the policy associated to \n  \n    \n      \n        θ\n      \n    \n    {\\displaystyle \\theta }\n  \n. Defining the performance function by \n  \n    \n      \n        ρ\n        (\n        θ\n        )\n        =\n        \n          ρ\n          \n            \n              π\n              \n                θ\n              \n            \n          \n        \n      \n    \n    {\\displaystyle \\rho (\\theta )=\\rho ^{\\pi _{\\theta }}}\n  \n under mild conditions this function will be differentiable as a function of the parameter vector \n  \n    \n      \n        θ\n      \n    \n    {\\displaystyle \\theta }\n  \n. If the gradient of \n  \n    \n      \n        ρ\n      \n    \n    {\\displaystyle \\rho }\n  \n was known, one could use gradient ascent. Since an analytic expression for the gradient is not available, only a noisy estimate is available. Such an estimate can be constructed in many ways, giving rise to algorithms such as Williams' REINFORCE method[20] (which is known as the likelihood ratio method in the simulation-based optimization literature).[21]\nA large class of methods avoids relying on gradient information. These include simulated annealing, cross-entropy search or methods of evolutionary computation. Many gradient-free methods can achieve (in theory and in the limit) a global optimum.\nPolicy search methods may converge slowly given noisy data. For example, this happens in episodic problems when the trajectories are long and the variance of the returns is large. Value-function based methods that rely on temporal differences might help in this case. In recent years, actor–critic methods have been proposed and performed well on various problems.[22]\nPolicy search methods have been used in the robotics context.[23] Many policy search methods may get stuck in local optima (as they are based on local search).\nFinally, all of the above methods can be combined with algorithms that first learn a model of the Markov Decision Process, the probability of each next state given an action taken from an existing state. For instance, the Dyna algorithm[24] learns a model from experience, and uses that to provide more modelled transitions for a value function, in addition to the real transitions.  Such methods can sometimes be extended to use of non-parametric models, such as when the transitions are simply stored and 'replayed'[25] to the learning algorithm.\nModel-based methods can be more computationally intensive than model-free approaches, and their utility can be limited by the extent to which the Markov Decision Process can be learnt.[26]\nThere are other ways to use models than to update a value function.[27] For instance, in model predictive control the model is used to update the behavior directly.\nBoth the asymptotic and finite-sample behaviors of most algorithms are well understood. Algorithms with provably good online performance (addressing the exploration issue) are known.\nEfficient exploration of Markov decision processes is given in  Burnetas and Katehakis (1997).[12] Finite-time performance bounds have also appeared for many algorithms, but these bounds are expected to be rather loose and thus more work is needed to better understand the relative advantages and limitations.\nFor incremental algorithms, asymptotic convergence issues have been settled[clarification needed]. Temporal-difference-based algorithms converge under a wider set of conditions than was previously possible (for example, when used with arbitrary, smooth function approximation).\nResearch topics include:\nAssociative reinforcement learning tasks combine facets of stochastic learning automata tasks and supervised learning pattern classification tasks. In associative reinforcement learning tasks, the learning system interacts in a closed loop with its environment.[44]\nThis approach extends reinforcement learning by using a deep neural network and without explicitly designing the state space.[45] The work on learning ATARI games by Google DeepMind increased attention to deep reinforcement learning or end-to-end reinforcement learning.[46]\nAdversarial deep reinforcement learning is an active area of research in reinforcement learning focusing on vulnerabilities of learned policies. In this research area some studies initially showed that reinforcement learning policies are susceptible to imperceptible adversarial manipulations.[47][48][49] While some methods have been proposed to overcome these susceptibilities, in the most recent studies it has been shown that these proposed solutions are far from providing an accurate representation of current vulnerabilities of deep reinforcement learning policies.[50]\nBy introducing fuzzy inference in reinforcement learning,[51] approximating the state-action value function with fuzzy rules in continuous space becomes possible. The IF - THEN form of fuzzy rules make this approach suitable for expressing the results in a form close to natural language. Extending FRL with Fuzzy Rule Interpolation [52] allows the use of reduced size sparse fuzzy rule-bases to emphasize cardinal rules (most important state-action values).\nIn inverse reinforcement learning (IRL), no reward function is given. Instead, the reward function is inferred given an observed behavior from an expert. The idea is to mimic observed behavior, which is often optimal or close to optimal.[53] One popular IRL paradigm is named maximum entropy inverse reinforcement learning (MaxEnt IRL). [54] MaxEnt IRL estimates the parameters of a linear model of the reward function by maximizing the entropy of the probability distribution of observed trajectories subject to constraints related to matching expected feature counts. Recently it has been shown that MaxEnt IRL is a particular case of a more general framework named random utility inverse reinforcement learning (RU-IRL). [55] RU-IRL is based on random utility theory and Markov decision processes. While prior IRL approaches assume that the apparent random behavior of an observed agent is due to it following a random policy, RU-IRL assumes that the observed agent follows a deterministic policy but randomness in observed behavior is due to the fact that an observer only has partial access to the features the observed agent uses in decision making. The utility function is modeled as a random variable to account for the ignorance of the observer regarding the features the observed agent actually considers in its utility function.\nSafe reinforcement learning (SRL) can be defined as the process of learning policies that maximize the expectation of the return in problems in which it is important to ensure reasonable system performance and/or respect safety constraints during the learning and/or deployment processes.[56]"}
{"url":"https://en.wikipedia.org/wiki/Robot_Operating_System","text":"Robot Operating System (ROS or ros) is an open-source robotics middleware suite. Although ROS is not an operating system (OS) but a set of software frameworks for robot software  development, it provides services designed for a heterogeneous computer cluster such as hardware abstraction, low-level device control, implementation of commonly used functionality, message-passing between processes, and package management. Running sets of ROS-based processes are represented in a graph architecture where processing takes place in nodes that may receive, post, and multiplex sensor data, control, state, planning, actuator, and other messages. Despite the importance of reactivity and low latency in robot control, ROS is not a real-time operating system (RTOS). However, it is possible to integrate ROS with real-time computing code.[3] The lack of support for real-time systems has been addressed in the creation of ROS 2,[4][5][6] a major revision of the ROS API which will take advantage of modern libraries and technologies for core ROS functions and add support for real-time code and embedded system hardware.\nSoftware in the ROS Ecosystem[7] can be separated into three groups:\nBoth the language-independent tools and the main client libraries (C++, Python, and Lisp) are released under the terms of the BSD license, and as such are open-source software and free for both commercial and research use. The majority of other packages are licensed under a variety of open-source licenses. These other packages implement commonly used functionality and applications such as hardware drivers, robot models, datatypes, planning, perception, simultaneous localization and mapping (SLAM), simulation tools, and other algorithms.\nThe main ROS client libraries are geared toward a Unix-like system, mostly because of their dependence on large sets of open-source software dependencies. For these client libraries, Ubuntu Linux is listed as \"Supported\" while other variants such as Fedora Linux, macOS, and Microsoft Windows are designated \"experimental\" and are supported by the community.[12] The native Java ROS client library, rosjava,[13] however, does not share these limitations and has enabled ROS-based software to be written for the Android OS.[14] rosjava has also enabled ROS to be integrated into an officially supported MATLAB toolbox which can be used on Linux, macOS, and Microsoft Windows.[15] A JavaScript client library, roslibjs[16] has also been developed which enables integration of software into a ROS system via any standards-compliant web browser.\nSometime before 2007, the first pieces of what eventually would become ROS began coalescing at Stanford University.[17][18] Eric Berger and Keenan Wyrobek, PhD students working in Kenneth Salisbury's[19] robotics laboratory at Stanford, were leading the Personal Robotics Program.[20] While working on robots to do manipulation tasks in human environments, the two students noticed that many of their colleagues were held back by the diverse nature of robotics: an excellent software developer might not have the hardware knowledge required, someone developing state of the art path planning might not know how to do the computer vision required. In an attempt to remedy this situation, the two students set out to make a baseline system that would provide a starting place for others in academia to build upon. In the words of Eric Berger, \"something that didn’t suck, in all of those different dimensions\".[17]\nIn their first steps towards this unifying system, the two built the PR1 as a hardware prototype and began to work on software from it, borrowing the best practices from other early open-source robotic software frameworks, particularly switchyard, a system that Morgan Quigley, another Stanford PhD student, had been working on in support of the STanford Artificial Intelligence Robot (STAIR)[21][22][23][24] by the Stanford Artificial Intelligence Laboratory. Early funding of US$50,000 was provided by Joanna Hoffman and Alain Rossmann, which supported the development of the PR1. While seeking funding for further development,[25] Eric Berger and Keenan Wyrobek met Scott Hassan, the founder of Willow Garage, a technology incubator which was working on an autonomous SUV and a solar autonomous boat. Hassan shared Berger and Wyrobek's vision of a \"Linux for robotics\", and invited them to come and work at Willow Garage. Willow Garage was started in January 2007, and the first commit of ROS code was made to SourceForge on 7 November 2007.[26]\nWillow Garage began developing the PR2 robot as a follow-up to the PR1, and ROS as the software to run it. Groups from more than twenty institutions made contributions to ROS, both the core software and the growing number of packages which worked with ROS to form a greater software ecosystem.[27][28] That people outside of Willow were contributing to ROS (especially from Stanford's STAIR project) meant that ROS was a multi-robot platform from the start. While Willow Garage had originally had other projects in progress, they were scrapped in favor of the Personal Robotics Program: focused on producing the PR2 as a research platform for academia and ROS as the open-source robotics stack that would underlie both academic research and tech startups, much like the LAMP stack did for web-based startups.\nIn December 2008, Willow Garage met the first of their three internal milestones: continuous navigation for the PR2 over a period of two days and a distance of pi kilometers.[29] Soon after, an early version of ROS (0.4 Mango Tango)[30] was released, followed by the first RVIZ documentation and the first paper on ROS.[28] In early summer, the second internal milestone: having the PR2 navigate the office, open doors, and plug itself it in, was reached.[31] This was followed in August by the initiation of the ROS.org website.[32] Early tutorials on ROS were posted in December,[33] preparing for the release of ROS 1.0, in January 2010.[34] This was Milestone 3: producing tons of documentation and tutorials for the enormous abilities that Willow Garage's engineers had developed over the preceding 3 years.\nFollowing this, Willow Garage achieved one of its longest held goals: giving away 10 PR2 robots to worthy academic institutions. This had long been a goal of the founders, as they felt that the PR2 could kick-start robotics research around the world. They ended up awarding eleven PR2s to different institutions, including University of Freiburg (Germany), Robert Bosch GmbH, Georgia Institute of Technology, KU Leuven (Belgium), Massachusetts Institute of Technology (MIT), Stanford University, Technical University of Munich (Germany), University of California, Berkeley, University of Pennsylvania, University of Southern California (USC), and University of Tokyo (Japan).[35] This, combined with Willow Garage's highly successful internship program[36] (run from 2008 to 2010 by Melonee Wise), helped to spread the word about ROS throughout the robotics world. The first official ROS distribution release: ROS Box Turtle, was released on 2 March 2010, marking the first time that ROS was officially distributed with a set of versioned packages for public use. These developments led to the first drone running ROS,[37] the first autonomous car running ROS,[38] and the adaption of ROS for Lego Mindstorms.[39] With the PR2 Beta program well underway, the PR2 robot was officially released for commercial purchase on 9 September 2010.[40]\n2011 was a banner year for ROS with the launch of ROS Answers, a Q/A forum for ROS users, on 15 February;[41] the introduction of the highly successful TurtleBot robot kit on 18 April;[42] and the total number of ROS repositories passing 100 on 5 May.[43] Willow Garage began 2012 by creating the Open Source Robotics Foundation (OSRF)[44] in April. The OSRF was immediately awarded a software contract by the Defense Advanced Research Projects Agency (DARPA).[45] Later that year, the first ROSCon was held in St. Paul, Minnesota,[46] the first book on ROS, ROS By Example,[47] was published, and Baxter, the first commercial robot to run ROS, was announced by Rethink Robotics.[48] Soon after passing its fifth anniversary in November, ROS began running on every continent on 3 December 2012.[49]\nIn February 2013, the OSRF became the primary software maintainers for ROS,[50] foreshadowing the announcement in August that Willow Garage would be absorbed by its founders, Suitable Technologies.[51] At this point, ROS had released seven major versions (up to ROS Groovy),[52] and had users all over the globe. This chapter of ROS development would be finalized when Clearpath Robotics took over support responsibilities for the PR2 in early 2014.[53]\nIn the years since OSRF took over primary development of ROS, a new version has been released every year,[52] while interest in ROS continues to grow. ROSCons have occurred every year since 2012, co-located with either ICRA or IROS, two flagship robotics conferences. Meetups of ROS developers have been organized in a variety of countries,[54][55][56] a number of ROS books have been published,[57] and many educational programs initiated.[58][59] On 1 September 2014, NASA announced the first robot to run ROS in space: Robotnaut 2, on the International Space Station.[60] In 2017, the OSRF changed its name to Open Robotics. Tech giants Amazon and Microsoft began to take an interest in ROS during this time, with Microsoft porting core ROS to Windows in September 2018,[61] followed by Amazon Web Services releasing RoboMaker in November 2018.[62]\nPerhaps the most important development of the OSRF/Open Robotics years thus far (not to discount the explosion of robot platforms which began to support ROS or the enormous improvements in each ROS version) was the proposal of ROS 2, a significant API change to ROS which is intended to support real-time programming, a wider variety of computing environments, and more modern technology.[63] ROS 2 was announced at ROSCon 2014,[64] the first commits to the ros2 repository were made in February 2015, followed by alpha releases in August 2015.[65] The first distribution release of ROS 2, Ardent Apalone, was released on 8 December 2017,[65] ushering in a new era of next-generation ROS development.\nROS was designed to be open source, intending that users would be able to choose the configuration of tools and libraries which interacted with the core of ROS so that users could shift their software stacks to fit their robot and application area. As such, there is very little which is core to ROS, beyond the general structure within which programs must exist and communicate. In one sense, ROS is the underlying plumbing behind nodes and message passing. However, in reality, ROS is not only that plumbing, but a rich and mature set of tools, a wide-ranging set of robot-agnostic abilities provided by packages, and a greater ecosystem of additions to ROS.\nROS processes are represented as nodes in a graph structure, connected by edges called topics.[66] ROS nodes can pass messages to one another through topics, make service calls to other nodes, provide a service for other nodes, or set or retrieve shared data from a communal database called the parameter server. A process called the ROS Master[66] makes all of this possible by registering nodes to itself, setting up node-to-node communication for topics, and controlling parameter server updates. Messages and service calls do not pass through the master, rather the master sets up peer-to-peer communication between all node processes after they register themselves with the master. This decentralized architecture lends itself well to robots, which often consist of a subset of networked computer hardware, and may communicate with off-board computers for heavy computing or commands.\nA node represents one process running the ROS graph. Every node has a name, which it registers with the ROS master before it can take any other actions. Multiple nodes with different names can exist under different namespaces, or a node can be defined as anonymous, in which case it will randomly generate an additional identifier to add to its given name. Nodes are at the center of ROS programming, as most ROS client code is in the form of a ROS node which takes actions based on information received from other nodes, sends information to other nodes, or sends and receives requests for actions to and from other nodes.\nTopics are named buses over which nodes send and receive messages.[67] Topic names must be unique within their namespace as well. To send messages to a topic, a node must publish to said topic, while to receive messages it must subscribe. The publish/subscribe model is anonymous: no node knows which nodes are sending or receiving on a topic, only that it is sending/receiving on that topic. The types of messages passed on a topic vary widely and can be user-defined. The content of these messages can be sensor data, motor control commands, state information, actuator commands, or anything else.\nA node may also advertise services.[68] A service represents an action that a node can take which will have a single result. As such, services are often used for actions which have a defined start and end, such as capturing a one-frame image, rather than processing velocity commands to a wheel motor or odometer data from a wheel encoder. Nodes advertise services and call services from one another.\nThe parameter server[68] is a database shared between nodes which allows for communal access to static or semi-static information. Data which does not change frequently and as such will be infrequently accessed, such as the distance between two fixed points in the environment, or the weight of the robot, are good candidates for storage in the parameter server.\nROS's core functionality is augmented by a variety of tools which allow developers to visualize and record data, easily navigate the ROS package structures, and create scripts automating complex configuration and setup processes. The addition of these tools greatly increases the abilities of systems using ROS by simplifying and providing solutions to a number of common robotics development problems. These tools are provided in packages like any other algorithm, but rather than providing implementations of hardware drivers or algorithms for various robotic tasks, these packages provide task and robot-agnostic tools which come with the core of most modern ROS installations.\nrviz[69] (Robot Visualization tool) is a three-dimensional visualizer used to visualize robots, the environments they work in, and sensor data. It is a highly configurable tool, with many different types of visualizations and plugins. Unified Robot Description Format (URDF) is an XML file format for robot model description.\nrosbag[70] is a command line tool used to record and playback ROS message data. rosbag uses a file format called bags,[71] which log ROS messages by listening to topics and recording messages as they come in. Playing messages back from a bag is largely the same as having the original nodes which produced the data in the ROS computation graph, making bags a useful tool for recording data to be used in later development. While rosbag is a command line only tool, rqt_bag[72] provides a GUI interface to rosbag.\ncatkin[73] is the ROS build system, having replaced rosbuild[74] as of ROS Groovy. catkin is based on CMake, and is similarly cross-platform, open-source, and language-independent.\nThe rosbash[75] package provides a suite of tools which augment the functionality of the bash shell. These tools include rosls, roscd, and roscp, which replicate the functionalities of ls, cd, and cp respectively. The ROS versions of these tools allow users to use ros package names in place of the file path where the package is located. The package also adds tab-completion to most ROS utilities, and includes rosed, which edits a given file with the chosen default text editor, as well rosrun, which runs executables in ROS packages. rosbash supports the same functionalities for zsh and tcsh, to a lesser extent.\nroslaunch[76] is a tool used to launch multiple ROS nodes both locally and remotely, as well as setting parameters on the ROS parameter server. roslaunch configuration files, which are written using XML can easily automate a complex startup and configuration process into a single command. roslaunch scripts can include other roslaunch scripts, launch nodes on specific machines, and even restart processes which die during execution.\nROS contains many open-source implementations of common robotics functionality and algorithms. These open-source implementations are organized into packages. Many packages are included as part of ROS distributions, while others may be developed by individuals and distributed through code sharing sites such as github. Some packages of note include:\nROS releases may be incompatible with other releases and are often referred to by code name rather than version number. ROS currently releases a version every year in May, following the release of Ubuntu LTS versions.[92] ROS 2 currently releases a new version every six months (in December and July). These releases are supported for a single year. There are currently two active major versions seeing releases: ROS 1 and ROS 2. Aside to this there is the ROS-Industrial or ROS-I derivate project since at least 2012.\nROS-Industrial[107] is an open-source project (BSD (legacy)/Apache 2.0 (preferred) license) that extends the advanced abilities of ROS to manufacturing automation and robotics. In the industrial environment, there are two different approaches to programming a robot: either through an external proprietary controller, typically implemented using ROS, or via the respective native programming language of the robot. ROS can therefore be seen as the software-based approach to program industrial robots instead of the classic robot controller-based approach.\nThe ROS-Industrial repository includes interfaces for common industrial manipulators, grippers, sensors, and device networks. It also provides software libraries for automatic 2D/3D sensor calibration, process path/motion planning, applications like Scan-N-Plan, developer tools like the Qt Creator ROS Plugin, and training curriculum that is specific to the needs of manufacturers. ROS-I is supported by an international Consortium of industry and research members. The project began as a collaborative endeavor between Yaskawa Motoman Robotics, Southwest Research Institute, and Willow Garage to support the use of ROS for manufacturing automation, with the GitHub repository being founded in January 2012 by Shaun Edwards (SwRI). Currently, the Consortium is divided into three groups; the ROS-Industrial Consortium Americas (led by SwRI and located in San Antonio, Texas), the ROS-Industrial Consortium Europe (led by Fraunhofer IPA and located in Stuttgart, Germany) and the ROS-Industrial Consortium Asia Pacific (led by Advanced Remanufacturing and Technology Centre (ARTC) and Nanyang Technological University (NTU) and located in Singapore).\nThe Consortia supports the global ROS-Industrial community by conducting ROS-I training, providing technical support and setting the future roadmap for ROS-I, as well as conducting precompetitive joint industry projects to develop new ROS-I abilities.[108]"}
{"url":"https://en.wikipedia.org/wiki/Intelligent_agent","text":"In intelligence and artificial intelligence, an intelligent agent (IA) is an agent acting in an intelligent manner; It perceives its environment, takes actions autonomously in order to achieve goals, and may improve its performance with learning or acquiring knowledge. An intelligent agent may be simple or complex: A thermostat or other control system is considered an example of an intelligent agent, as is a human being, as is any system that meets the definition, such as a firm, a state, or a biome.[1]\nLeading AI textbooks define \"artificial intelligence\" as the \"study and design of intelligent agents\", a definition that considers goal-directed behavior to be the essence of intelligence. Goal-directed agents are also described using a term borrowed from economics, \"rational agent\".[1]\nAn agent has an \"objective function\" that encapsulates all the IA's goals. Such an agent is designed to create and execute whatever plan will, upon completion, maximize the expected value of the objective function.[2] For example, a reinforcement learning agent has a \"reward function\" that allows the programmers to shape the IA's desired behavior,[3] and an evolutionary algorithm's behavior is shaped by a \"fitness function\".[4] \nIntelligent agents in artificial intelligence are closely related to agents in economics, and versions of the intelligent agent paradigm are studied in cognitive science, ethics, the philosophy of practical reason, as well as in many interdisciplinary socio-cognitive modeling and computer social simulations. \nIntelligent agents are often described schematically as an abstract functional system similar to a computer program. Abstract descriptions of intelligent agents are called abstract intelligent agents (AIA) to distinguish them from their real-world implementations. An autonomous intelligent agent is designed to function in the absence of human intervention. Intelligent agents are also closely related to software agents (an autonomous computer program that carries out tasks on behalf of users).\nArtificial Intelligence: A Modern Approach[5][6][2] defines an \"agent\" as \n\"Anything that can be viewed as perceiving its environment through sensors and acting upon that environment through actuators\"It defines a \"rational agent\" as:\n\"An agent that acts so as to maximize the expected value of a performance measure based on past experience and knowledge.\"It also defines the field of \"artificial intelligence research\" as:\n\"The study and design of rational agents\"Padgham \u0026 Winikoff (2005) agree that an intelligent agent is situated in an environment and responds in a timely (though not necessarily real-time) manner to changes in the environment. However, intelligent agents must also proactively pursue goals in a flexible and robust way.[a] Optional desiderata include that the agent be rational, and that the agent be capable of belief-desire-intention analysis.[7]\nKaplan and Haenlein define artificial intelligence as \"A system's ability to correctly interpret external data, to learn from such data, and to use those learnings to achieve specific goals and tasks through flexible adaptation.\"[8] This definition is closely related to that of an intelligent agent.\nPhilosophically, this definition of artificial intelligence avoids several lines of criticism. Unlike the Turing test, it does not refer to human intelligence in any way. Thus, there is no need to discuss if it is \"real\" vs \"simulated\" intelligence (i.e., \"synthetic\" vs \"artificial\" intelligence) and does not indicate that such a machine has a mind, consciousness or true understanding (i.e., it does not imply John Searle's \"strong AI hypothesis\"). It also doesn't attempt to draw a sharp dividing line between behaviors that are \"intelligent\" and behaviors that are \"unintelligent\"—programs need only be measured in terms of their objective function.\nMore importantly, it has a number of practical advantages that have helped move AI research forward. It provides a reliable and scientific way to test programs; researchers can directly compare or even combine different approaches to isolated problems, by asking which agent is best at maximizing a given \"goal function\".  It also gives them a common language to communicate with other fields—such as mathematical optimization (which is defined in terms of \"goals\") or economics (which uses the same definition of a \"rational agent\").[9]\nAn agent that is assigned an explicit \"goal function\" is considered more intelligent if it consistently takes actions that successfully maximize its programmed goal function. The goal can be simple (\"1 if the IA wins a game of Go, 0 otherwise\") or complex (\"Perform actions mathematically similar to ones that succeeded in the past\"). The \"goal function\" encapsulates all of the goals the agent is driven to act on; in the case of rational agents, the function also encapsulates the acceptable trade-offs between accomplishing conflicting goals. (Terminology varies; for example, some agents seek to maximize or minimize a \"utility function\", \"objective function\", or \"loss function\".)[6][2]\nGoals can be explicitly defined or induced. If the AI is programmed for \"reinforcement learning\", it has a \"reward function\" that encourages some types of behavior and punishes others. Alternatively, an evolutionary system can induce goals by using a \"fitness function\" to mutate and preferentially replicate high-scoring AI systems, similar to how animals evolved to innately desire certain goals such as finding food.[10] Some AI systems, such as nearest-neighbor, instead of reason by analogy, these systems are not generally given goals, except to the degree that goals are implicit in their training data.[11] Such systems can still be benchmarked if the non-goal system is framed as a system whose \"goal\" is to accomplish its narrow classification task.[12]\nSystems that are not traditionally considered agents, such as knowledge-representation systems, are sometimes subsumed into the paradigm by framing them as agents that have a goal of (for example) answering questions as accurately as possible; the concept of an \"action\" is here extended to encompass the \"act\" of giving an answer to a question. As an additional extension, mimicry-driven systems can be framed as agents who are optimizing a \"goal function\" based on how closely the IA succeeds in mimicking the desired behavior.[6][2] In the generative adversarial networks of the 2010s, an \"encoder\"/\"generator\" component attempts to mimic and improvise human text composition. The generator is attempting to maximize a function encapsulating how well it can fool an antagonistic \"predictor\"/\"discriminator\" component.[13]\nWhile symbolic AI systems often accept an explicit goal function, the paradigm can also be applied to neural networks and to evolutionary computing. Reinforcement learning can generate intelligent agents that appear to act in ways intended to maximize a \"reward function\".[14] Sometimes, rather than setting the reward function to be directly equal to the desired benchmark evaluation function, machine learning programmers will use reward shaping to initially give the machine rewards for incremental progress in learning.[15] Yann LeCun stated in 2018 that \"Most of the learning algorithms that people have come up with essentially consist of minimizing some objective function.\"[16] AlphaZero chess had a simple objective function; each win counted as +1 point, and each loss counted as -1 point. An objective function for a self-driving car would have to be more complicated.[17] Evolutionary computing can evolve intelligent agents that appear to act in ways intended to maximize a \"fitness function\" that influences how many descendants each agent is allowed to leave.[4]\nThe theoretical and uncomputable AIXI design is a maximally intelligent agent in this paradigm;[18] however, in the real world, the IA is constrained by finite time and hardware resources, and scientists compete to produce algorithms that can achieve progressively higher scores on benchmark tests with real-world hardware.[19][relevant?]\nRussell \u0026 Norvig (2003) group agents into five classes based on their degree of perceived intelligence and capability:[20]\nSimple reflex agents act only on the basis of the current percept, ignoring the rest of the percept history. The agent function is based on the condition-action rule: \"if condition, then action\".\nThis agent function only succeeds when the environment is fully observable. Some reflex agents can also contain information on their current state which allows them to disregard conditions whose actuators are already triggered.\nInfinite loops are often unavoidable for simple reflex agents operating in partially observable environments. If the agent can randomize its actions, it may be possible to escape from infinite loops.\nA model-based agent can handle partially observable environments. Its current state is stored inside the agent maintaining some kind of structure that describes the part of the world which cannot be seen. This knowledge about \"how the world works\" is called a model of the world, hence the name \"model-based agent\".\nA model-based reflex agent should maintain some sort of internal model that depends on the percept history and thereby reflects at least some of the unobserved aspects of the current state. Percept history and impact of action on the environment can be determined by using the internal model. It then chooses an action in the same way as reflex agent.\nAn agent may also use models to describe and predict the behaviors of other agents in the environment.[21]\nGoal-based agents further expand on the capabilities of the model-based agents, by using \"goal\" information. Goal information describes situations that are desirable. This provides the agent a way to choose among multiple possibilities, selecting the one which reaches a goal state. Search and planning are the subfields of artificial intelligence devoted to finding action sequences that achieve the agent's goals.\nGoal-based agents only distinguish between goal states and non-goal states. It is also possible to define a measure of how desirable a particular state is. This measure can be obtained through the use of a utility function which maps a state to a measure of the utility of the state. A more general performance measure should allow a comparison of different world states according to how well they satisfied the agent's goals. The term utility can be used to describe how \"happy\" the agent is.\n\nA rational utility-based agent chooses the action that maximizes the expected utility of the action outcomes - that is, what the agent expects to derive, on average, given the probabilities and utilities of each outcome. A utility-based agent has to model and keep track of its environment, tasks that have involved a great deal of research on perception, representation, reasoning, and learning.\nLearning has the advantage that it allows the agents to initially operate in unknown environments and to become more competent than its initial knowledge alone might allow. The most important distinction is between the \"learning element\", which is responsible for making improvements, and the \"performance element\", which is responsible for selecting external actions.\nThe learning element uses feedback from the \"critic\" on how the agent is doing and determines how the performance element, or \"actor\", should be modified to do better in the future.  The performance element is what we have previously considered to be the entire agent: it takes in percepts and decides on actions.\nThe last component of the learning agent is the \"problem generator\". It is responsible for suggesting actions that will lead to new and informative experiences.\nWeiss (2013) defines four classes of agents:\n\nIn 2013, Alexander Wissner-Gross published a theory pertaining to Freedom and Intelligence for intelligent agents.[22][23]\nTo actively perform their functions, Intelligent Agents today are normally gathered in a hierarchical structure containing many “sub-agents”. Intelligent sub-agents process and perform lower-level functions. Taken together, the intelligent agent and sub-agents create a complete system that can accomplish difficult tasks or goals with behaviors and responses that display a form of intelligence.\nGenerally, an agent can be constructed by separating the body into the sensors and actuators, and so that it operates with a complex perception system that takes the description of the world as input for a controller and outputs commands to the actuator. However, a hierarchy of controller layers is often necessary to balance the immediate reaction desired for low-level tasks and the slow reasoning about complex, high-level goals.[24]\nA simple agent program can be defined mathematically as a function f (called the \"agent function\")[25] which maps every possible percepts sequence to a possible action the agent can perform or to a coefficient, feedback element, function or constant that affects eventual actions:\nAgent function is an abstract concept as it could incorporate various principles of decision making like calculation of utility of individual options, deduction over logic rules, fuzzy logic, etc.[26]\nThe program agent, instead, maps every possible percept to an action.[27]\nWe use the term percept to refer to the agent's perceptional inputs at any given instant. In the following figures, an agent is anything that can be viewed as perceiving its environment through sensors and acting upon that environment through actuators.\nHallerbach et al. discussed the application of agent-based approaches for the development and validation of automated driving systems via a digital twin of the vehicle-under-test and microscopic traffic simulation based on independent agents.[28] Waymo has created a multi-agent simulation environment Carcraft to test algorithms for self-driving cars.[29][30] It simulates traffic interactions between human drivers, pedestrians and automated vehicles. People's behavior is imitated by artificial agents based on data of real human behavior. The basic idea of using agent-based modeling to understand self-driving cars was discussed as early as 2003.[31]\n\"Intelligent agent\" is also often used as a vague marketing term, sometimes synonymous with \"virtual personal assistant\".[32] Some 20th-century definitions characterize an agent as a program that aids a user or that acts on behalf of a user.[33] These examples are known as software agents, and sometimes an \"intelligent software agent\" (that is, a software agent with intelligence) is referred to as an \"intelligent agent\".\nAccording to Nikola Kasabov, IA systems should exhibit the following characteristics:[34]"}
{"url":"https://en.wikipedia.org/wiki/Software_agent","text":"In computer science, a software agent is a computer program that acts for a user or another program in a relationship of agency.\nThe term agent is derived from the Latin agere (to do): an agreement to act on one's behalf. Such \"action on behalf of\" implies the authority to decide which, if any, action is appropriate.[1][2] Some agents are colloquially known as bots, from robot. They may be embodied, as when execution is paired with a robot body, or as software such as a chatbot executing on a computer, such as a mobile device, e.g. Siri. Software agents may be autonomous or work together with other agents or people. Software agents interacting with people (e.g. chatbots, human-robot interaction environments) may possess human-like qualities such as natural language understanding and speech, personality or embody humanoid form (see Asimo).\nRelated and derived concepts include intelligent agents (in particular exhibiting some aspects of artificial intelligence, such as reasoning), autonomous agents (capable of modifying the methods of achieving their objectives), distributed agents (being executed on physically distinct computers), multi-agent systems (distributed agents that work together to achieve an objective that could not be accomplished by a single agent acting alone), and mobile agents (agents that can relocate their execution onto different processors).\nThe basic attributes of an autonomous software agent are that agents:\nThe term \"agent\" describes a software abstraction, an idea, or a concept, similar to OOP terms such as methods, functions, and objects.[citation needed] The concept of an agent provides a convenient and powerful way to describe a complex software entity that is capable of acting with a certain degree of autonomy in order to accomplish tasks on behalf of its host. But unlike objects, which are defined in terms of methods and attributes, an agent is defined in terms of its behavior.[3]\nVarious authors have proposed different definitions of agents, these commonly include concepts such as\nAll agents are programs, but not all programs are agents. Contrasting the term with related concepts may help clarify its meaning. Franklin \u0026 Graesser (1997)[4] discuss four key notions that distinguish agents from arbitrary programs: reaction to the environment, autonomy, goal-orientation and persistence.\nSoftware agents may offer various benefits to their end users by automating complex or repetitive tasks.[6] However, there are organizational and cultural impacts of this technology that need to be considered prior to implementing software agents.\nPeople like to perform easy tasks providing the sensation of success unless the repetition of the simple tasking is affecting the overall output. In general implementing software agents to perform administrative requirements provides a substantial increase in work contentment, as administering their own work does never please the worker. The effort freed up serves for a higher degree of engagement in the substantial tasks of individual work. Hence, software agents may provide the basics to implement self-controlled work, relieved from hierarchical controls and interference.[7] Such conditions may be secured by application of software agents for required formal support.\nThe cultural effects of the implementation of software agents include trust affliction, skills erosion, privacy attrition and social detachment. Some users may not feel entirely comfortable fully delegating important tasks to software applications. Those who start relying solely on intelligent agents may lose important skills, for example, relating to information literacy. In order to act on a user's behalf, a software agent needs to have a complete understanding of a user's profile, including his/her personal preferences. This, in turn, may lead to unpredictable privacy issues. When users start relying on their software agents more, especially for communication activities, they may lose contact with other human users and look at the world with the eyes of their agents. These consequences are what agent researchers and users must consider when dealing with intelligent agent technologies.[8]\nThe concept of an agent can be traced back to Hewitt's Actor Model (Hewitt, 1977) - \"A self-contained, interactive and concurrently-executing object, possessing internal state and communication capability.\"[citation needed]\nTo be more academic, software agent systems are a direct evolution of Multi-Agent Systems (MAS). MAS evolved from Distributed Artificial Intelligence (DAI), Distributed Problem Solving (DPS) and Parallel AI (PAI), thus inheriting all characteristics (good and bad) from DAI and AI.\nJohn Sculley's 1987 \"Knowledge Navigator\" video portrayed an image of a relationship between end-users and agents. Being an ideal first, this field experienced a series of unsuccessful top-down implementations, instead of a piece-by-piece, bottom-up approach. The range of agent types is now (from 1990) broad: WWW, search engines, etc.\nBuyer agents[9] travel around a network (e.g. the internet) retrieving information about goods and services. These agents, also known as 'shopping bots', work very efficiently for commodity products such as CDs, books, electronic components, and other one-size-fits-all products. Buyer agents are typically optimized to allow for digital payment services used in e-commerce and traditional businesses.[10]\nUser agents, or personal agents, are intelligent agents that take action on your behalf. In this category belong those intelligent agents that already perform, or will shortly perform, the following tasks:\nMonitoring and surveillance agents are used to observe and report on equipment, usually computer systems. The agents may keep track of company inventory levels, observe competitors' prices and relay them back to the company, watch stock manipulation by insider trading and rumors, etc.\nFor example, NASA's Jet Propulsion Laboratory has an agent that monitors inventory, planning, schedules equipment orders to keep costs down, and manages food storage facilities. These agents usually monitor complex computer networks that can keep track of the configuration of each computer connected to the network.\nA special case of Monitoring-and-Surveillance agents are organizations of agents used to emulate the Human Decision-Making process during tactical operations. The agents monitor the status of assets (ammunition, weapons available, platforms for transport, etc.) and receive Goals (Missions) from higher level agents. The Agents then pursue the Goals with the Assets at hand, minimizing expenditure of the Assets while maximizing Goal Attainment. (See Popplewell, \"Agents and Applicability\")\nThis agent uses information technology to find trends and patterns in an abundance of information from many different sources. The user can sort through this information in order to find whatever information they are seeking.\nA data mining agent operates in a data warehouse discovering information. A 'data warehouse' brings together information from many different sources. \"Data mining\" is the process of looking through the data warehouse to find information that you can use to take action, such as ways to increase sales or keep customers who are considering defecting.\n'Classification' is one of the most common types of data mining, which finds patterns in information and categorizes them into different classes. Data mining agents can also detect major shifts in trends or a key indicator and can detect the presence of new information and alert you to it. For example, the agent may detect a decline in the construction industry for an economy; based on this relayed information construction companies will be able to make intelligent decisions regarding the hiring/firing of employees or the purchase/lease of equipment in order to best suit their firm.\nSome other examples of current intelligent agents include some spam filters, game bots, and server monitoring tools. Search engine indexing bots also qualify as intelligent agents.\nSoftware bots are becoming important in software engineering.[12]\nAgents are also used in software security application to intercept, examine and act on various types of content.  Example include: \nIssues to consider in the development of agent-based systems include \nFor software agents to work together efficiently they must share semantics of their data elements. This can be done by having computer systems publish their metadata.\nThe definition of agent processing can be approached from two interrelated directions:\nAgent systems are used to model real-world systems with concurrency or parallel processing.\nThe agent uses its access methods to go out into local and remote databases to forage for content. These access methods may include setting up news stream delivery to the agent, or retrieval from bulletin boards, or using a spider to walk the Web. The content that is retrieved in this way is probably already partially filtered – by the selection of the newsfeed or the databases that are searched. The agent next may use its detailed searching or language-processing machinery to extract keywords or signatures from the body of the content that has been received or retrieved. This abstracted content (or event) is then passed to the agent's Reasoning or inferencing machinery in order to decide what to do with the new content. This process combines the event content with the rule-based or knowledge content provided by the user. If this process finds a good hit or match in the new content, the agent may use another piece of its machinery to do a more detailed search on the content. Finally, the agent may decide to take an action based on the new content; for example, to notify the user that an important event has occurred. This action is verified by a security function and then given the authority of the user. The agent makes use of a user-access method to deliver that message to the user. If the user confirms that the event is important by acting quickly on the notification, the agent may also employ its learning machinery to increase its weighting for this kind of event.\nBots can act on behalf of their creators to do good as well as bad. There are a few ways which bots can be created to demonstrate that they are designed with the best intention and are not built to do harm. This is first done by having a bot identify itself in the user-agent HTTP header when communicating with a site. The source IP address must also be validated to establish itself as legitimate. Next, the bot must also always respect a site's robots.txt file since it has become the standard across most of the web. And like respecting the robots.txt file, bots should shy away from being too aggressive and respect any crawl delay instructions.[14]"}
{"url":"https://en.wikipedia.org/wiki/Robotic_process_automation","text":"Robotic process automation (RPA) is a form of business process automation that is based on software robots (bots) or artificial intelligence (AI) agents.[1] RPA should not be confused with artificial intelligence as it is based on automotive technology following a predefined workflow.[2]  It is sometimes referred to as software robotics (not to be confused with robot software).\nIn traditional workflow automation tools, a software developer produces a list of actions to automate a task and interface to the back end system using internal application programming interfaces (APIs) or dedicated scripting language. In contrast, RPA systems develop the action list by watching the user perform that task in the application's graphical user interface (GUI), and then perform the automation by repeating those tasks directly in the GUI. This can lower the barrier to the use of automation in products that might not otherwise feature APIs for this purpose.\nRPA tools have strong technical similarities to graphical user interface testing tools. These tools also automate interactions with the GUI, and often do so by repeating a set of demonstration actions performed by a user. RPA tools differ from such systems in that they allow data to be handled in and between multiple applications, for instance, receiving email containing an invoice, extracting the data, and then typing that into a bookkeeping system.\nThe typical benefits of robotic automation include reduced cost; increased speed, accuracy, and consistency; improved quality and scalability of production. Automation can also provide extra security, especially for sensitive data and financial services.\nAs a form of automation, the concept has been around for a long time in the form of screen scraping, which can be traced back to early forms of malware[ambiguous]. However, RPA is much more extensible, consisting of API integration into other enterprise applications, connectors into ITSM systems, terminal services and even some types of AI (e.g. Machine Learning) services such as image recognition.  It is considered to be a significant technological evolution in the sense that new software platforms are emerging which are sufficiently mature, resilient, scalable and reliable to make this approach viable for use in large enterprises[3] (who would otherwise be reluctant due to perceived risks to quality and reputation).\nA principal barrier to the adoption of self-service is often technological: it may not always be feasible or economically viable to retrofit new interfaces onto existing systems. Moreover, organisations may wish to layer a variable and configurable set of process rules on top of the system interfaces which may vary according to market offerings and the type of customer. This only adds to the cost and complexity of the technological implementation. Robotic automation software provides a pragmatic means of deploying new services in this situation, where the robots simply mimic the behaviour of humans to perform the back-end transcription or processing. The relative affordability of this approach arises from the fact that no new IT transformation or investment is required; instead the software robots simply leverage greater use out of existing IT assets.\nThe hosting of RPA services also aligns with the metaphor of a software robot, with each robotic instance having its own virtual workstation, much like a human worker. The robot uses keyboard and mouse controls to take actions and execute automations. Normally all of these actions take place in a virtual environment and not on screen; the robot does not need a physical screen to operate, rather it interprets the screen display electronically. The scalability of modern solutions based on architectures such as these owes much to the advent of virtualization technology, without which the scalability of large deployments would be limited by the available capacity to manage physical hardware and by the associated costs. The implementation of RPA in business enterprises has shown dramatic cost savings when compared to traditional non-RPA solutions.[4]\nThere are however several risks with RPA. Criticism includes risks of stifling innovation and creating a more complex maintenance environment of existing software that now needs to consider the use of graphical user interfaces in a way they weren't intended to be used.[5]\nAccording to Harvard Business Review, most operations groups adopting RPA have promised their employees that automation would not result in layoffs.[6] Instead, workers have been redeployed to do more interesting work. One academic study highlighted that knowledge workers did not feel threatened by automation: they embraced it and viewed the robots as team-mates.[7] The same study highlighted that, rather than resulting in a lower \"headcount\", the technology was deployed in such a way as to achieve more work and greater productivity with the same number of people.\nConversely, however, some analysts proffer that RPA represents a threat to the business process outsourcing (BPO) industry.[8] The thesis behind this notion is that RPA will enable enterprises to \"repatriate\" processes from offshore locations into local data centers, with the benefit of this new technology. The effect, if true, will be to create high-value jobs for skilled process designers in onshore locations (and within the associated supply chain of IT hardware, data center management, etc.) but to decrease the available opportunity to low-skilled workers offshore. On the other hand, this discussion appears to be healthy ground for debate as another academic study was at pains to counter the so-called \"myth\" that RPA will bring back many jobs from offshore.[7]\nAcademic studies[9][10] project that RPA, among other technological trends, is expected to drive a new wave of productivity and efficiency gains in the global labour market. Although not directly attributable to RPA alone, Oxford University conjectures that up to 35% of all jobs might be automated by 2035.[9]\nThere are geographic implications to the trend in robotic automation. In the example above where an offshored process is \"repatriated\" under the control of the client organization (or even displaced by a Business Process Outsourcer) from an offshore location to a data centre, the impact will be a deficit in economic activity to the offshore location and an economic benefit to the originating economy. On this basis, developed economies – with skills and technological infrastructure to develop and support a robotic automation capability – can be expected to achieve a net benefit from the trend.\nIn a TEDx talk[11] hosted by University College London (UCL), entrepreneur David Moss explains that digital labour in the form of RPA is likely to revolutionize the cost model of the services industry by driving the price of products and services down, while simultaneously improving the quality of outcomes and creating increased opportunity for the personalization of services.\nIn a separate TEDx in 2019 talk,[12] Japanese business executive, and former CIO of Barclays bank, Koichi Hasegawa noted that digital robots can be a positive effect on society if we start using a robot with empathy to help every person. He provides a case study of the Japanese insurance companies – Sompo Japan and Aioi – both of whom introduced bots to speed up the process of insurance pay-outs in past massive disaster incidents.\nMeanwhile, Professor Willcocks, author of the LSE paper[10] cited above, speaks of increased job satisfaction and intellectual stimulation, characterising the technology as having the ability to \"take the robot out of the human\",[13] a reference to the notion that robots will take over the mundane and repetitive portions of people's daily workload, leaving them to be used in more interpersonal roles or to concentrate on the remaining, more meaningful, portions of their day.\nIt was also found in a 2021 study observing the effects of robotization in Europe that, the gender pay gap increased at a rate of .18% for every 1% increase in robotization of a given industry.[14]\nUnassisted RPA, or RPAAI,[15][16] is the next generation of RPA related technologies. Technological advancements around artificial intelligence allow a process to be run on a computer without needing input from a user.\nHyperautomation is the application of advanced technologies like RPA, artificial intelligence, machine learning (ML) and process mining to augment workers and automate processes in ways that are significantly more impactful than traditional automation capabilities.[17][18][19] Hyperautomation is the combination of automation tools to deliver work.[20]\nGartner's report notes that this trend was kicked off with robotic process automation (RPA). The report notes that, \"RPA alone is not hyperautomation. Hyperautomation requires a combination of tools to help support replicating pieces of where the human is involved in a task.\"[21]\nBack office clerical processes outsourced by large organisations - particularly those sent offshore - tend to be simple and transactional in nature, requiring little (if any) analysis or subjective judgement. This would seem to make an ideal starting point for organizations beginning to adopt robotic automation for the back office. Whether client organisations choose to take outsourced processes back \"in house\" from their Business Process Outsourcing (BPO) providers, thus representing a threat to the future of the BPO business,[22] or whether the BPOs implement such automations on their clients' behalf may well depend on a number of factors.\nConversely however, a BPO provider may seek to effect some form of client lock-in by means of automation. By removing cost from a business operation, where the BPO provider is considered to be the owner of the intellectual property and physical implementation of a robotic automation solution (perhaps in terms of hardware, ownership of software licences, etc.), the provider can make it very difficult for the client to take a process back \"in house\" or elect a new BPO provider. This effect occurs as the associated cost savings made through automation would - temporarily at least - have to be reintroduced to the business whilst the technical solution is reimplemented in the new operational context.\nThe geographically agnostic nature of software means that new business opportunities may arise for those organisations that have a political or regulatory impediment to offshoring or outsourcing. A robotised automation can be hosted in a data centre in any jurisdiction and this has two major consequences for BPO providers. Firstly, for example, a sovereign government may not be willing or legally able to outsource the processing of tax affairs and security administration.  On this basis, if robots are compared to a human workforce, this creates a genuinely new opportunity for a \"third sourcing\" option, after the choices of onshore vs. offshore. Secondly, and conversely, BPO providers have previously relocated outsourced operations to different political and geographic territories in response to changing wage inflation and new labor arbitrage opportunities elsewhere. By contrast, a data centre solution would seem to offer a fixed and predictable cost base that, if sufficiently low in cost on a robot vs. human basis,  would seem to eliminate any potential need or desire to continually relocate operational bases.\nWhile robotic process automation has many benefits including cost efficiency and consistency in performance, it also has some limitations. Current RPA solutions demand continual technical support to handle system changes, therefore it lacks the ability to autonomously adapt to new conditions. Because of this limitation, the system sometimes needs manual reconfiguration, which in turn has an effect on efficiency.[23]\nRPA is based on automotive technology following a predefined workflow, and artificial intelligence is data-driven and focuses on processing information to make predictions. Therefore there is a distinct difference between how the two systems operate. AI aims to mimic human intelligence, whereas RPA is focused on reproducing tasks that are typically human-directed.[24] Moreover, RPA could also be explained as virtual robots that take over routinized human work, it can identify data by interpreting the underlying tags. RPA, therefore, is based on machine learning, whereas AI utilizes self-learning technologies.[25]"}
{"url":"https://en.wikipedia.org/wiki/Chatbot","text":"A chatbot (originally chatterbot)[1] is a software application or web interface that is designed to mimic human conversation through text or voice interactions.[2][3][4] Modern chatbots are typically online and use generative artificial intelligence systems that are capable of maintaining a conversation with a user in natural language and simulating the way a human would behave as a conversational partner. Such chatbots often use deep learning and natural language processing, but simpler chatbots have existed for decades.\nSince late 2022, the field has gained widespread attention due to the popularity of OpenAI's ChatGPT,[5][6] followed by alternatives such as Microsoft's Copilot and Google's Gemini.[7] Such examples reflect the recent practice of basing such products upon broad foundational large language models, such as GPT-4 or the Gemini language model, that get fine-tuned so as to target specific tasks or applications (i.e., simulating human conversation, in the case of chatbots). Chatbots can also be designed or customized to further target even more specific situations and/or particular subject-matter domains.[8]\nA major area where chatbots have long been used is in customer service and support, with various sorts of virtual assistants.[9] Companies spanning a wide range of industries have begun using the latest generative artificial intelligence technologies to power more advanced developments in such areas.[8]\nAs chatbots work by predicting responses rather than knowing the meaning of their responses, this means they can produce coherent-sounding but inaccurate or fabricated content, referred to as ‘hallucinations’. When humans use and apply chatbot content contaminated with hallucinations, this results in ‘botshit’.[10] Given the increasing adoption and use of chatbots for generating content, there are concerns that this technology will significantly reduce the cost it takes humans to generate, spread and consume bullshit.[11]\nIn 1950, Alan Turing's famous article \"Computing Machinery and Intelligence\" was published,[12] which proposed what is now called the Turing test as a criterion of intelligence. This criterion depends on the ability of a computer program to impersonate a human in a real-time written conversation with a human judge to the extent that the judge is unable to distinguish reliably—on the basis of the conversational content alone—between the program and a real human. The notoriety of Turing's proposed test stimulated great interest in Joseph Weizenbaum's program ELIZA, published in 1966, which seemed to be able to fool users into believing that they were conversing with a real human. However Weizenbaum himself did not claim that ELIZA was genuinely intelligent, and the introduction to his paper presented it more as a debunking exercise:\nIn artificial intelligence, machines are made to behave in wondrous ways, often sufficient to dazzle even the most experienced observer. But once a particular program is unmasked, once its inner workings are explained, its magic crumbles away; it stands revealed as a mere collection of procedures. The observer says to himself \"I could have written that\". With that thought, he moves the program in question from the shelf marked \"intelligent\", to that reserved for curios. The object of this paper is to cause just such a re-evaluation of the program about to be \"explained\". Few programs ever needed it more.[13]ELIZA's key method of operation (copied by chatbot designers ever since) involves the recognition of clue words or phrases in the input, and the output of the corresponding pre-prepared or pre-programmed responses that can move the conversation forward in an apparently meaningful way (e.g. by responding to any input that contains the word 'MOTHER' with 'TELL ME MORE ABOUT YOUR FAMILY').[13] Thus an illusion of understanding is generated, even though the processing involved has been merely superficial. ELIZA showed that such an illusion is surprisingly easy to generate because human judges are so ready to give the benefit of the doubt when conversational responses are capable of being interpreted as \"intelligent\".\nInterface designers have come to appreciate that humans' readiness to interpret computer output as genuinely conversational—even when it is actually based on rather simple pattern-matching—can be exploited for useful purposes. Most people prefer to engage with programs that are human-like, and this gives chatbot-style techniques a potentially useful role in interactive systems that need to elicit information from users, as long as that information is relatively straightforward and falls into predictable categories. Thus, for example, online help systems can usefully employ chatbot techniques to identify the area of help that users require, potentially providing a \"friendlier\" interface than a more formal search or menu system. This sort of usage holds the prospect of moving chatbot technology from Weizenbaum's \"shelf ... reserved for curios\" to that marked \"genuinely useful computational methods\".\nAmong the most notable early chatbots are ELIZA (1966) and PARRY (1972).[14][15][16][17] More recent notable programs include A.L.I.C.E., Jabberwacky and D.U.D.E (Agence Nationale de la Recherche and CNRS 2006). While ELIZA and PARRY were used exclusively to simulate typed conversation, many chatbots now include other functional features, such as games and web searching abilities. In 1984, a book called The Policeman's Beard is Half Constructed was published, allegedly written by the chatbot Racter (though the program as released would not have been capable of doing so).[18]\nFrom 1978[19] to some time after 1983,[20] the CYRUS project led by Janet Kolodner constructed a chatbot simulating Cyrus Vance (57th United States Secretary of State). It used case-based reasoning, and updated its database daily by parsing wire news from United Press International. The program was unable to process the news items subsequent to the surprise resignation of Cyrus Vance in April 1980, and the team constructed another chatbot simulating his successor, Edmund Muskie.[21][20]\nOne pertinent field of AI research is natural-language processing. Usually, weak AI fields employ specialized software or programming languages created specifically for the narrow function required. For example, A.L.I.C.E. uses a markup language called AIML,[3] which is specific to its function as a conversational agent, and has since been adopted by various other developers of, so-called, Alicebots. Nevertheless, A.L.I.C.E. is still purely based on pattern matching techniques without any reasoning capabilities, the same technique ELIZA was using back in 1966. This is not strong AI, which would require sapience and logical reasoning abilities.\nJabberwacky learns new responses and context based on real-time user interactions, rather than being driven from a static database. Some more recent chatbots also combine real-time learning with evolutionary algorithms that optimize their ability to communicate based on each conversation held. Still, there is currently no general purpose conversational artificial intelligence, and some software developers focus on the practical aspect, information retrieval.\nChatbot competitions focus on the Turing test or more specific goals. Two such annual contests are the Loebner Prize and The Chatterbox Challenge (the latter has been offline since 2015, however, materials can still be found from web archives).[22]\nChatbots may use artificial neural networks as a language model. For example, generative pre-trained transformers (GPT), which use the transformer architecture, have become common to build sophisticated chatbots. The \"pre-training\" in its name refers to the initial training process on a large text corpus, which provides a solid foundation for the model to perform well on downstream tasks with limited amounts of task-specific data. An example of a GPT chatbot is ChatGPT.[23] Despite criticism of its accuracy and tendency to “hallucinate”—that is, to confidently output false information and even cite non-existent sources—ChatGPT has gained attention for its detailed responses and historical knowledge. Another example is BioGPT, developed by Microsoft, which focuses on answering biomedical questions.[24][25] In November 2023, Amazon announced a new chatbot, called Q, for people to use at work.[26]\nDBpedia created a chatbot during the GSoC of 2017.[27][28][29] It can communicate through Facebook Messenger (see Master of Code Global article).\nMany companies' chatbots run on messaging apps or simply via SMS. They are used for B2C customer service, sales and marketing.[30]\nIn 2016, Facebook Messenger allowed developers to place chatbots on their platform. There were 30,000 bots created for Messenger in the first six months, rising to 100,000 by September 2017.[31]\nSince September 2017, this has also been as part of a pilot program on WhatsApp. Airlines KLM and Aeroméxico both announced their participation in the testing;[32][33][34][35] both airlines had previously launched customer services on the Facebook Messenger platform. \nThe bots usually appear as one of the user's contacts, but can sometimes act as participants in a group chat.\nMany banks, insurers, media companies, e-commerce companies, airlines, hotel chains, retailers, health care providers, government entities and restaurant chains have used chatbots to answer simple questions, increase customer engagement,[36] for promotion, and to offer additional ways to order from them.[37] Chatbots are also used in market research to collect short survey responses.[38]\nA 2017 study showed 4% of companies used chatbots.[39] According to a 2016 study, 80% of businesses said they intended to have one by 2020.[40]\nPrevious generations of chatbots were present on company websites, e.g. Ask Jenn from Alaska Airlines which debuted in 2008[41] or Expedia's virtual customer service agent which launched in 2011.[41][42] The newer generation of chatbots includes IBM Watson-powered \"Rocky\", introduced in February 2017 by the New York City-based e-commerce company Rare Carat to provide information to prospective diamond buyers.[43][44]\nUsed by marketers to script sequences of messages, very similar to an autoresponder sequence. Such sequences can be triggered by user opt-in or the use of keywords within user interactions. After a trigger occurs a sequence of messages is delivered until the next anticipated user response. Each user response is used in the decision tree to help the chatbot navigate the response sequences to deliver the correct response message.\nOther companies explore ways they can use chatbots internally, for example for Customer Support, Human Resources, or even in Internet-of-Things (IoT) projects. Overstock.com, for one, has reportedly launched a chatbot named Mila to automate certain simple yet time-consuming processes when requesting sick leave.[45] Other large companies such as Lloyds Banking Group, Royal Bank of Scotland, Renault and Citroën are now using automated online assistants instead of call centres with humans to provide a first point of contact. A SaaS chatbot business ecosystem has been steadily growing since the F8 Conference when Facebook’s Mark Zuckerberg unveiled that Messenger would allow chatbots into the app.[46] In large companies, like in hospitals and aviation organizations, IT architects are designing reference architectures for Intelligent Chatbots that are used to unlock and share knowledge and experience in the organization more efficiently, and reduce the errors in answers from expert service desks significantly.[47] These Intelligent Chatbots make use of all kinds of artificial intelligence like image moderation and natural-language understanding (NLU), natural-language generation (NLG), machine learning and deep learning.\nMany high-tech banking organizations are looking to integrate automated AI-based solutions such as chatbots into their customer service in order to provide faster and cheaper assistance to their clients who are becoming increasingly comfortable with technology.  In particular, chatbots can efficiently conduct a dialogue, usually replacing other communication tools such as email, phone, or SMS. In banking, their major application is related to quick customer service answering common requests, as well as transactional support.\nSeveral studies report significant reduction in the cost of customer services, expected to lead to billions of dollars of economic savings in the next ten years.[48] In 2019, Gartner predicted that by 2021, 15% of all customer service interactions globally will be handled completely by AI.[49]  A study by Juniper Research in 2019 estimates retail sales resulting from chatbot-based interactions will reach $112 billion by 2023.[50]\nSince 2016, when Facebook allowed businesses to deliver automated customer support, e-commerce guidance, content, and interactive experiences through chatbots, a large variety of chatbots were developed for the Facebook Messenger platform.[51]\nIn 2016, Russia-based Tochka Bank launched the world's first Facebook bot for a range of financial services, including a possibility of making payments.[52]\nIn July 2016, Barclays Africa also launched a Facebook chatbot, making it the first bank to do so in Africa.[53]\nThe France's third-largest bank by total assets[54] Société Générale launched their chatbot called SoBot in March 2018. While 80% of users of the SoBot expressed their satisfaction after having tested it, Société Générale deputy director Bertrand Cozzarolo stated that it will never replace the expertise provided by a human advisor.\n[55]\nThe advantages of using chatbots for customer interactions in banking include cost reduction, financial advice, and 24/7 support.[56][57]\nChatbots are also appearing in the healthcare industry.[58][59] A study suggested that physicians in the United States believed that chatbots would be most beneficial for scheduling doctor appointments, locating health clinics, or providing medication information.[60]\nWhatsapp has teamed up with the World Health Organization (WHO) to make a chatbot service that answers users' questions on COVID-19.[61]\nIn 2020, The Indian Government launched a chatbot called MyGov Corona Helpdesk,[62] that worked through Whatsapp and helped people access information about the Coronavirus (COVID-19) pandemic.[63][64]\nCertain patient groups are still reluctant to use chatbots. A mixed-methods study showed that people are still hesitant to use chatbots for their healthcare due to poor understanding of the technological complexity, the lack of empathy, and concerns about cyber-security.[65] The analysis showed that while 6% had heard of a health chatbot and 3% had experience of using it, 67% perceived themselves as likely to use one within 12 months. The majority of participants would use a health chatbot for seeking general health information (78%), booking a medical appointment (78%), and looking for local health services (80%). However, a health chatbot was perceived as less suitable for seeking results of medical tests and seeking specialist advice such as sexual health. \nThe analysis of attitudinal variables showed that most participants reported their preference for discussing their health with doctors (73%) and having access to reliable and accurate health information (93%). While 80% were curious about new technologies that could improve their health, 66% reported only seeking a doctor when experiencing a health problem and 65% thought that a chatbot was a good idea. 30% reported dislike about talking to computers, 41% felt it would be strange to discuss health matters with a chatbot and about half were unsure if they could trust the advice given by a chatbot. Therefore, perceived trustworthiness, individual attitudes towards bots, and dislike for talking to computers are the main barriers to health chatbots.\nIn New Zealand, the chatbot SAM – short for Semantic Analysis Machine[66] (made by Nick Gerritsen of Touchtech[67]) – has been developed. It is designed to share its political thoughts, for example on topics such as climate change, healthcare and education, etc. It talks to people through Facebook Messenger.[68][69][70][71]\nIn 2022, the chatbot \"Leader Lars\" or \"Leder Lars\" was nominated for The Synthetic Party to run in the Danish parliamentary election,[72] and was built by the artist collective Computer Lars.[73] Leader Lars differed from earlier virtual politicians by leading a political party and by not pretending to be an objective candidate.[74] This chatbot engaged in critical discussions on politics with users from around the world.[75]\nIn India, the state government has launched a chatbot for its Aaple Sarkar platform,[76] which provides conversational access to information regarding public services managed.[77][78]\nChatbots have been used at different levels of government departments, including local, national and regional contexts. Chatbots are used to provide services like citizenship and immigration, court administrations, financial aid, and migrants’ rights inquiries. For example, EMMA answers more than 500,000 inquiries monthly, regarding services on citizenship and immigration in the US.[79]\nChatbots have also been incorporated into devices not primarily meant for computing, such as toys.[80]\nHello Barbie is an Internet-connected version of the doll that uses a chatbot provided by the company ToyTalk,[81] which previously used the chatbot for a range of smartphone-based characters for children.[82] These characters' behaviors are constrained by a set of rules that in effect emulate a particular character and produce a storyline.[83]\nThe My Friend Cayla doll was marketed as a line of 18-inch (46 cm) dolls which uses speech recognition technology in conjunction with an Android or iOS mobile app to recognize the child's speech and have a conversation. It, like the Hello Barbie doll, attracted controversy due to vulnerabilities with the doll's Bluetooth stack and its use of data collected from the child's speech.\nIBM's Watson computer has been used as the basis for chatbot-based educational toys for companies such as CogniToys[80] intended to interact with children for educational purposes.[84]\nMalicious chatbots are frequently used to fill chat rooms with spam and advertisements, by mimicking human behavior and conversations or to entice people into revealing personal information, such as bank account numbers. They were commonly found on Yahoo! Messenger, Windows Live Messenger, AOL Instant Messenger and other instant messaging protocols. There has also been a published report of a chatbot used in a fake personal ad on a dating service's website.[85]\nTay, an AI chatbot that learns from previous interaction, caused major controversy due to it being targeted by internet trolls on Twitter. The bot was exploited, and after 16 hours began to send extremely offensive Tweets to users. This suggests that although the bot learned effectively from experience, adequate protection was not put in place to prevent misuse.[86]\nIf a text-sending algorithm can pass itself off as a human instead of a chatbot, its message would be more credible. Therefore, human-seeming chatbots with well-crafted online identities could start scattering fake news that seems plausible, for instance making false claims during an election. With enough chatbots, it might be even possible to achieve artificial social proof.[87][88]\nThe creation and implementation of chatbots is still a developing area, heavily related to artificial intelligence and machine learning, so the provided solutions, while possessing obvious advantages, have some important limitations in terms of functionalities and use cases. However, this is changing over time.\nThe most common limitations are listed below:[89]\nChatbots are increasingly present in businesses and often are used to automate tasks that do not require skill-based talents. With customer service taking place via messaging apps as well as phone calls, there are growing numbers of use-cases where chatbot deployment gives organizations a clear return on investment. Call center workers may be particularly at risk from AI-driven chatbots.[91]\nChatbot jobs\nChatbot developers create, debug, and maintain applications that automate customer services or other communication processes. Their duties include reviewing and simplifying code when needed. They may also help companies implement bots in their operations.\nA study by Forrester (June 2017) predicted that 25% of all jobs would be impacted by AI technologies by 2019.[92]\nPrompt Engineering, the task of designing and refining prompts (inputs) leading to desired AI-generated responses has gained significant demand and popularity in recent years, with the advent of sophisticated models, notably OpenAI’s GPT series (which still contain notable flaws and limitations, as previously outlined)."}
{"url":"https://en.wikipedia.org/wiki/Applications_of_artificial_intelligence","text":"Artificial intelligence (AI) has been used in applications throughout industry and academia. Similar to electricity or computers, AI serves as a general-purpose technology that has numerous applications. Its applications span language translation, image recognition, decision-making,[1] credit scoring, e-commerce and various other domains. AI which accommodates such technologies as machines being equipped perceive, understand, act and learning a scientific discipline.[2]\nA recommendation system predicts the rating or preference a user would give to an item.[3][4] Artificial intelligence recommendation systems are designed to offer suggestions based on previous behavior. These systems have been used by companies such as Netflix, Amazon, Instagram and YouTube, where they generate personalized playlists, product suggestions, and video recommendations.[5]\nMachine learning is also used in web feeds such as for determining which posts should show up in social media feeds.[6][7] Various types of social media analysis also make use of machine learning[8][9] and there is research into its use for (semi-)automated tagging/enhancement/correction of online misinformation and related filter bubbles.[10][11][12]\nAI is used to target web advertisements to those most likely to click or engage in them. It is also used to increase time spent on a website by selecting attractive content for the viewer. It can predict or generalize the behavior of customers from their digital footprints.[13] Both AdSense[citation needed] and Facebook[14] use AI for advertising.\nOnline gambling companies use AI to improve customer targeting.[15]\nPersonality computing AI models add psychological targeting to more traditional social demographics or behavioral targeting.[16] AI has been used to customize shopping options and personalize offers.[17]\nIntelligent personal assistants use AI to understand many natural language requests in other ways than rudimentary commands. Common examples are Apple's Siri, Amazon's Alexa, and a more recent AI, ChatGPT by OpenAI.[18]\nBing Chat has used artificial intelligence as part of its search engine.[19]\nMachine learning can be used to fight against spam, scams, and phishing. It can scrutinize the contents of spam and phishing attacks to attempt to identify malicious elements.[20] Some models built via machine learning algorithms have over 90% accuracy in distinguishing between spam and legitimate emails.[21] These models can be refined from new data and evolving spam tactics. Machine learning also analyzes traits such as sender behavior, email header information, and attachment types.[22]\nSpeech translation technology attempts to convert one language's spoken words into another. This potentially reduces language barriers in global commerce and cross-cultural exchange by allowing speakers of various languages to communicate with one another.[23] \nAI has been used to automatically translate spoken language and textual content, in products such as Microsoft Translator, Google Translate and DeepL Translator.[24] Additionally, research and development are in progress to decode and conduct animal communication.[25][26]\nMeaning is conveyed not only by text, but also through usage and context (see semantics and pragmatics). As a result, the two primary categorization approaches for machine translations are statistical and neural machine translations (NMTs). The old method of performing translation was to use a statistical machine translation (SMT) methodology to forecast the best probable output with specific algorithms. However, with NMT, the approach employs dynamic algorithms to achieve better translations based on context.[27]\nAI has been used in facial recognition systems, with a 99% accuracy rate. Some examples are Apple's Face ID and Android's Face Unlock, which are used to secure mobile devices.[28]\nImage labeling has been used by Google to detect products in photos and to allow people to search based on a photo. Image labeling has also been demonstrated to generate speech to describe images to blind people. [29] Facebook's DeepFace identifies human faces in digital images.\nGames have been a major application[relevant?] of AI's capabilities since the 1950s. In the 21st century, AIs have beaten human players in many games, including chess (Deep Blue), Jeopardy! (Watson),[30] Go (AlphaGo),[31][32][33][34][35][36][37] poker (Pluribus[38] and Cepheus),[39] E-sports (StarCraft),[40][41] and general game playing (AlphaZero[42][43][44] and MuZero).[45][46][47][48] AI has replaced hand-coded algorithms in most chess programs.[49] Unlike go or chess, poker is an imperfect-information game, so a program that plays poker has to reason under uncertainty. The general game players work using feedback from the game system, without knowing the rules.\nAI for Good is an ITU initiative supporting institutions employing AI to tackle some of the world's greatest economic and social challenges. For example, the University of Southern California launched the Center for Artificial Intelligence in Society, with the goal of using AI to address problems such as homelessness. At Stanford, researchers use AI to analyze satellite images to identify high poverty areas.[50]\nIn agriculture, AI has helped farmers identify areas that need irrigation, fertilization, pesticide treatments or increasing yield.[51] Agronomists use AI to conduct research and development. AI has been used to predict the ripening time for crops such as tomatoes,[52] monitor soil moisture, operate agricultural robots, conduct predictive analytics,[53][54] classify livestock pig call emotions,[25] automate greenhouses,[55] detect diseases and pests,[56][57] and save water.[58]\nAI helps in achieving precise farming, which calls for the use of algorithims to analyze data retrieved from satellite imagery and on-site field sensors. It allows for optimization of resource usage and helps to make the right decisions regarding the kind of nutrients, water, and pesticides required to maximize yield.[59] \nUsing machine learning models to monitor the health of crops and the soil. The models will be able to detect and predict diseases and pests in crops ahead of time to allow timely interventions.[60] \nThere are automated machinery such as tractors and harvesters, which can operate autonomously with minimal human labor. With the use of AI many duties in the area are possible to be done with precision.[61] \nCyber security companies are adopting neural networks, machine learning, and natural language processing to improve their systems.[62]\nApplications of AI in cyber security include:\nGoogle fraud czar Shuman Ghosemajumder has said that AI will be used to completely automate most cyber security operations over time.[67]\nAI elevates teaching, focusing on significant issues like the knowledge nexus and educational equality. The evolution of AI in education and technology should be used to improve human capabilities in relationships where they do not replace humans. UNESCO recognizes the future of AI in education as an instrument to reach Sustainable Development Goal 4, called \"Inclusive and Equitable Quality Education.” [68]\nThe World Economic Forum also stresses AI's contribution to students' overall improvement and transforming teaching into a more enjoyable process.[68]\nPersonalized Learning\nAI driven tutoring systems, such as Khan Academy, Duo-lingo and Carnegie Learning  are the forefoot of delivering personalized education.[69]\nThese platforms leverage AI algorithms to analyze individual learning patterns, strengths, and weaknesses, enabling the customization of content to suit each student's pace and style of learning.[69]\nAdministrative Efficiency\nIn educational institutions, AI is increasingly used to automate routine tasks like grading and attendance tracking, which allows educators to devote more time to interactive teaching and direct student engagement.[70]\nFurthermore, AI tools are employed to monitor student progress, analyze learning behaviors, and predict academic challenges, facilitating timely and proactive interventions for students who may be at risk of falling behind.[70]\nEthical and Privacy Concerns\nDespite the benefits, the integration of AI in education raises significant ethical and privacy concerns, particularly regarding the handling of sensitive student data.[69] \nIt is imperative that AI systems in education are designed and operated with a strong emphasis on transparency, security, and respect for privacy to maintain trust and uphold the integrity of educational practices.[69]\nFinancial institutions have long used artificial neural network systems to detect charges or claims outside of the norm, flagging these for human investigation. The use of AI in banking began in 1987 when Security Pacific National Bank launched a fraud prevention taskforce to counter the unauthorized use of debit cards.[71] Kasisto and Moneystream use AI.\nBanks use AI to organize operations, for bookkeeping, investing in stocks, and managing properties. AI can react to changes when business is not taking place.[72] AI is used to combat fraud and financial crimes by monitoring behavioral patterns for any abnormal changes or anomalies.[73][74][75]\nThe use of AI in applications such as online trading and decision-making has changed major economic theories.[76] For example, AI-based buying and selling platforms estimate individualized demand and supply curves and thus enable individualized pricing. AI machines reduce information asymmetry in the market and thus make markets more efficient.[77] The application of artificial intelligence in the financial industry can alleviate the financing constraints of non-state-owned enterprises. Especially for smaller and more innovative enterprises.[78]\nAlgorithmic trading involves the use of AI systems to make trading decisions at speeds orders of magnitude greater than any human is capable of, making millions of trades in a day without human intervention. Such high-frequency trading represents a fast-growing sector. Many banks, funds, and proprietary trading firms now have entire portfolios that are AI-managed. Automated trading systems are typically used by large institutional investors but include smaller firms trading with their own AI systems.[79]\nLarge financial institutions use AI to assist with their investment practices. BlackRock's AI engine, Aladdin, is used both within the company and by clients to help with investment decisions. Its functions include the use of natural language processing to analyze text such as news, broker reports, and social media feeds. It then gauges the sentiment on the companies mentioned and assigns a score. Banks such as UBS and Deutsche Bank use SQREEM (Sequential Quantum Reduction and Extraction Model) to mine data to develop consumer profiles and match them with wealth management products.[80]\nOnline lender Upstart uses machine learning for underwriting.[81]\nZestFinance's Zest Automated Machine Learning (ZAML) platform is used for credit underwriting. This platform uses machine learning to analyze data including purchase transactions and how a customer fills out a form to score borrowers. The platform is particularly useful to assign credit scores to those with limited credit histories.[82]\nAI makes continuous auditing possible. Potential benefits include reducing audit risk, increasing the level of assurance, and reducing audit duration.[83][quantify]\nContinuous auditing with AI allows a real-time monitoring and reporting of financial activities and providing businesses with timely insights that can lead to quick decision making.[84] \nAI software, such as LaundroGraph which uses contemporary suboptimal datasets, could be used for anti-money laundering (AML).[85][86] AI can be used to \"develop the AML pipeline into a robust, scalable solution with a reduced false positive rate and high adaptability\".[87] A study about deep learning for AML identified \"key challenges for researchers\" to have \"access to recent real transaction data and scarcity of labelled training data; and data being highly imbalanced\" and suggests future research should bring-out \"explainability, graph deep learning using natural language processing (NLP), unsupervised and reinforcement learning to handle lack of labelled data; and joint research programs between the research community and industry to benefit from domain knowledge and controlled access to data\".[88]\nBanks use machine learning (ML) to upgrade process monitoring and demonstrating the ability of  responding efficiently to evolving techniques.[89]\nThrough ML and other methods, financial organizations can detect laundering operations and run compliance in an automated and very fast mode.[89]\nIn the 1980s, AI started to become prominent in finance as expert systems were commercialized. For example, Dupont created 100 expert systems, which helped them to save almost $10 million per year.[90] One of the first systems was the Pro-trader expert system that predicted the 87-point drop in the Dow Jones Industrial Average in 1986. \"The major junctions of the system were to monitor premiums in the market, determine the optimum investment strategy, execute transactions when appropriate and modify the knowledge base through a learning mechanism.\"[91]\nOne of the first expert systems to help with financial plans was PlanPowerm and Client Profiling System, created by Applied Expert Systems (APEX). It was launched in 1986. It helped create personal financial plans for people.[92]\nIn the 1990s AI was applied to fraud detection. In 1993 FinCEN Artificial Intelligence System (FAIS) launched. It was able to review over 200,000 transactions per week and over two years it helped identify 400 potential cases of money laundering equal to $1 billion.[93] These expert systems were later replaced by machine learning systems.[94]\nAI can enhance entrepreneurial activity and AI is one of the most dynamic areas for start-ups, with significant venture capital flowing into AI.[95]\nAI facial recognition systems are used for mass surveillance, notably in China.[96][97]\nIn 2019, Bengaluru, India deployed AI-managed traffic signals. This system uses cameras to monitor traffic density and adjust signal timing based on the interval needed to clear traffic.[98]\nVarious countries are deploying AI military applications.[99] The main applications enhance command and control, communications, sensors, integration and interoperability.[100] Research is targeting intelligence collection and analysis, logistics, cyber operations, information operations, and semiautonomous and autonomous vehicles.[99] AI technologies enable coordination of sensors and effectors, threat detection and identification, marking of enemy positions, target acquisition, coordination and deconfliction of distributed Joint Fires between networked combat vehicles involving manned and unmanned teams.[100] AI was incorporated into military operations in Iraq and Syria.[99]\nIn 2023, the United States Department of Defense tested generative AI based on large language models to digitize and integrate data across the military.[101]\nIn the 2023 Israel–Hamas war, Israel used two AI systems to generate targets to strike: Habsora (translated: \"the gospel\") was used to compile a list of buildings to target, while \"Lavender\" produced a list of people. \"Lavender\" produced a list of 37,000 people to target.[102][103] The list of buildings to target included Gazan private homes of people that were suspected of affiliation to Hamas operatives. The combination of AI targeting technology with policy shift away from avoiding civilian targets resulted in unprecedented numbers of civilian deaths. IDF officials say the program addresses the previous issue of the air force running out of targets. Using Habsora, officials say that suspected and junior Hamas members homes significantly expand the \"AI target bank.\" An internal source describes the process as a “mass assassination factory”.[104][103]\nIn 2024, the U.S. military trained artificial intelligence to identify airstrike targets during its operations in Iraq and Syria.[105]\nWorldwide annual military spending on robotics rose from US$5.1 billion in 2010 to US$7.5 billion in 2015.[106][107] Military drones capable of autonomous action are in wide use.[108] Many researchers avoid military applications.[100]\nAI in healthcare is often used for classification, to evaluate a CT scan or electrocardiogram or to identify high-risk patients for population health. AI is helping with the high-cost problem of dosing. One study suggested that AI could save $16 billion. In 2016, a study reported that an AI-derived formula derived the proper dose of immunosuppressant drugs to give to transplant patients.[109] Current research has indicated that non-cardiac vascular illnesses are also being treated with artificial intelligence (AI). For certain disorders, AI algorithms can assist with diagnosis, recommended treatments, outcome prediction, and patient progress tracking. As AI technology advances, it is anticipated that it will become more significant in the healthcare industry.[110]\nThe early detection of diseases like cancer is made possible by AI algorithms, which diagnose diseases by analyzing complex sets of medical data. For example, the IBM Watson system might be used to comb through massive data such as medical records and clinical trials to help diagnose a problem.[111] Microsoft's AI project Hanover helps doctors choose cancer treatments from among the more than 800 medicines and vaccines.[112][113] Its goal is to memorize all the relevant papers to predict which (combinations of) drugs will be most effective for each patient. Myeloid leukemia is one target. Another study reported on an AI that was as good as doctors in identifying skin cancers.[114] Another project monitors multiple high-risk patients by asking each patient questions based on data acquired from doctor/patient interactions.[115] In one study done with transfer learning, an AI diagnosed eye conditions similar to an ophthalmologist and recommended treatment referrals.[116]\nAnother study demonstrated surgery with an autonomous robot. The team supervised the robot while it performed soft-tissue surgery, stitching together a pig's bowel judged better than a surgeon.[117]\nArtificial neural networks are used as clinical decision support systems for medical diagnosis,[118] such as in concept processing technology in EMR software.\nOther healthcare tasks thought suitable for an AI that are in development include:\nAI-enabled chatbots decrease the need for humans to perform basic call center tasks.[134]\nMachine learning in sentiment analysis can spot fatigue in order to prevent overwork.[134] Similarly, decision support systems can prevent industrial disasters and make disaster response more efficient.[135] For manual workers in material handling, predictive analytics may be used to reduce musculoskeletal injury.[136] Data collected from wearable sensors can improve workplace health surveillance, risk assessment, and research.[135][how?]\nAI can auto-code workers' compensation claims.[137][138] AI-enabled virtual reality systems can enhance safety training for hazard recognition.[135] AI can more efficiently detect accident near misses, which are important in reducing accident rates, but are often underreported.[139]\nAlphaFold 2 can determine the 3D structure of a (folded) protein in hours rather than the months required by earlier automated approaches and was used to provide the likely structures of all proteins in the human body and essentially all proteins known to science (more than 200 million).[140][141][142][143]\nMachine learning has been used for drug design. It has also been used for predicting molecular properties and exploring large chemical/reaction spaces.[144] Computer-planned syntheses via computational reaction networks, described as a platform that combines \"computational synthesis with AI algorithms to predict molecular properties\",[145] have been used to explore the origins of life on Earth,[146] drug-syntheses and developing routes for recycling 200 industrial waste chemicals into important drugs and agrochemicals (chemical synthesis design).[147] There is research about which types of computer-aided chemistry would benefit from machine learning.[148] It can also be used for \"drug discovery and development, drug repurposing, improving pharmaceutical productivity, and clinical trials\".[149] It has been used for the design of proteins with prespecified functional sites.[150][151]\nIt has been used with databases for the development of a 46-day process to design, synthesize and test a drug which inhibits enzymes of a particular gene, DDR1. DDR1 is involved in cancers and fibrosis which is one reason for the high-quality datasets that enabled these results.[152]\nThere are various types of applications for machine learning in decoding human biology, such as helping to map gene expression patterns to functional activation patterns[153] or identifying functional DNA motifs.[154] It is widely used in genetic research.[155]\nThere also is some use of machine learning in synthetic biology,[156][157] disease biology,[157] nanotechnology (e.g. nanostructured materials and bionanotechnology),[158][159] and materials science.[160][161][162]\nThere are also prototype robot scientists, including robot-embodied ones like the two Robot Scientists, which show a form of \"machine learning\" not commonly associated with the term.[163][164]\nSimilarly, there is research and development of biological \"wetware computers\" that can learn (e.g. for use as biosensors) and/or implantation into an organism's body (e.g. for use to control prosthetics).[165][166][167] Polymer-based artificial neurons operate directly in biological environments and define biohybrid neurons made of artificial and living components.[168][169]\nMoreover, if whole brain emulation is possible via both scanning and replicating the, at least, bio-chemical brain – as premised in the form of digital replication in The Age of Em, possibly using physical neural networks – that may have applications as or more extensive than e.g. valued human activities and may imply that society would face substantial moral choices, societal risks and ethical problems[170][171] such as whether (and how) such are built, sent through space and used compared to potentially competing e.g. potentially more synthetic and/or less human and/or non/less-sentient types of artificial/semi-artificial intelligence.[additional citation(s) needed] An alternative or additive approach to scanning are types of reverse engineering of the brain.[172][173]\nA subcategory of artificial intelligence is embodied,[174][175] some of which are mobile robotic systems that each consist of one or multiple robots that are able to learn in the physical world.\nHowever, biological computers, even if both highly artificial and intelligent, are typically distinguished from synthetic, often silicon-based, computers – they could however be combined or used for the design of either. Moreover, many tasks may be carried out inadequately by artificial intelligence even if its algorithms were transparent, understood, bias-free, apparently effective, and goal-aligned and its trained data sufficiently large and cleansed – such as in cases were the underlying or available metrics, values or data are inappropriate. Computer-aided is a phrase used to describe human activities that make use of computing as tool in more comprehensive activities and systems such as AI for narrow tasks or making use of such without substantially relying on its results (see also: human-in-the-loop).[citation needed] A study described the biological as a limitation of AI with \"as long as the biological system cannot be understood, formalized, and imitated, we will not be able to develop technologies that can mimic it\" and that if it was understood this does not mean there being \"a technological solution to imitate natural intelligence\".[176] Technologies that integrate biology and are often AI-based include biorobotics.\nArtificial intelligence is used in astronomy to analyze increasing amounts of available data[177][178] and applications, mainly for \"classification, regression, clustering, forecasting, generation, discovery, and the development of new scientific insights\" for example for discovering exoplanets, forecasting solar activity, and distinguishing between signals and instrumental effects in gravitational wave astronomy.[179] It could also be used for activities in space such as space exploration, including analysis of data from space missions, real-time science decisions of spacecraft, space debris avoidance,[180] and more autonomous operation.[181][182][183][178]\nIn the search for extraterrestrial intelligence (SETI), machine learning has been used in attempts to identify artificially generated electromagnetic waves in available data[184][185] – such as real-time observations[186] – and other technosignatures, e.g. via anomaly detection.[187] In ufology, the SkyCAM-5 project headed by Prof. Hakan Kayal[188] and the Galileo Project headed by Prof. Avi Loeb use machine learning to detect and classify peculiar types of UFOs.[189][190][191][192][193] The Galileo Project also seeks to detect two further types of potential extraterrestrial technological signatures with the use of AI: 'Oumuamua-like interstellar objects, and non-manmade artificial satellites.[194][195]\nLoeb has speculated that one type of technological equipment the project may detect could be \"AI astronauts\"[196] and in 2021 – in an opinion piece – that AI \"will\" \"supersede natural intelligence\",[197] while Martin Rees stated that there \"may\" be more civilizations than thought with the \"majority of them\" being artificial.[198] In particular, mid/far future or non-human applications of artificial intelligence could include advanced forms of artificial general intelligence that engages in space colonization or more narrow spaceflight-specific types of AI. In contrast, there have been concerns in relation to potential AGI or AI capable of embryo space colonization, or more generally natural intelligence-based space colonization, such as \"safety of encounters with an alien AI\",[199][200] suffering risks (or inverse goals),[201][202] moral license/responsibility in respect to colonization-effects,[203] or AI gone rogue (e.g. as portrayed with fictional David8 and HAL 9000). See also: space law and space ethics. Loeb has described the possibility of \"AI astronauts\" that engage in \"supervised evolution\" (see also: directed evolution, uplift, directed panspermia and space colonization).[204]\nIt can also be used to produce datasets of spectral signatures of molecules that may be involved in the atmospheric production or consumption of particular chemicals – such as phosphine possibly detected on Venus – which could prevent miss assignments and, if accuracy is improved, be used in future detections and identifications of molecules on other planets.[205]\nIn April 2024, the Scientific Advice Mechanism to the European Commission published advice[206] including a comprehensive evidence review of the opportunities and challenges posed by artificial intelligence in scientific research.\nAs benefits, the evidence review[207] highlighted:\nAs challenges:\nMachine learning can help to restore and attribute ancient texts.[208] It can help to index texts for example to enable better and easier searching[209] and classification of fragments.[210]\n\nArtificial intelligence can also be used to investigate genomes to uncover genetic history, such as interbreeding between archaic and modern humans by which for example the past existence of a ghost population, not Neanderthal or Denisovan, was inferred.[211] \nIt can also be used for \"non-invasive and non-destructive access to internal structures of archaeological remains\".[212] A deep learning system was reported to learn intuitive physics from visual data (of virtual 3D environments) based on an unpublished approach inspired by studies of visual cognition in infants.[213][214] Other researchers have developed a machine learning algorithm that could discover sets of basic variables of various physical systems and predict the systems' future dynamics from video recordings of their behavior.[215][216] In the future, it may be possible that such can be used to automate the discovery of physical laws of complex systems.[215]\nAI could be used for materials optimization and discovery such as the discovery of stable materials and the prediction of their crystal structure.[217][218][219]\nIn November 2023, researchers at Google DeepMind and Lawrence Berkeley National Laboratory announced that they had developed an AI system known as GNoME. This system has contributed to materials science by discovering over 2 million new materials within a relatively short timeframe. GNoME employs deep learning techniques to efficiently explore potential material structures, achieving a significant increase in the identification of stable inorganic crystal structures. The system's predictions were validated through autonomous robotic experiments, demonstrating a noteworthy success rate of 71%. The data of newly discovered materials is publicly available through the Materials Project database, offering researchers the opportunity to identify materials with desired properties for various applications. This development has implications for the future of scientific discovery and the integration of AI in material science research, potentially expediting material innovation and reducing costs in product development. The use of AI and deep learning suggests the possibility of minimizing or eliminating manual lab experiments and allowing scientists to focus more on the design and analysis of unique compounds.[220][221][222]\nMachine learning is used in diverse types of reverse engineering. For example, machine learning has been used to reverse engineer a composite material part, enabling unauthorized production of high quality parts,[223] and for quickly understanding the behavior of malware.[224][225][226] It can be used to reverse engineer artificial intelligence models.[227] It can also design components by engaging in a type of reverse engineering of not-yet existent virtual components such as inverse molecular design for particular desired functionality[228] or protein design for prespecified functional sites.[150][151] Biological network reverse engineering could model interactions in a human understandable way, e.g. bas on time series data of gene expression levels.[229]\nAI is a mainstay of law-related professions. Algorithms and machine learning do some tasks previously done by entry-level lawyers.[230] While its use is common, it is not expected to replace most work done by lawyers in the near future.[231]\nThe electronic discovery industry uses machine learning to reduce manual searching.[232]\nCOMPAS is a commercial system used by U.S. courts to assess the likelihood of recidivism.[233]\nOne concern relates to algorithmic bias, AI programs may become biased after processing data that exhibits bias.[234] ProPublica claims that the average COMPAS-assigned recidivism risk level of black defendants is significantly higher than that of white defendants.[233]\nIn 2019, the city of Hangzhou, China established a pilot program artificial intelligence-based Internet Court to adjudicate disputes related to ecommerce and internet-related intellectual property claims.[235]: 124  Parties appear before the court via videoconference and AI evaluates the evidence presented and applies relevant legal standards.[235]: 124 \nAnother application of AI is in human resources. AI can screen resumes and rank candidates based on their qualifications, predict candidate success in given roles, and automate repetitive communication tasks via chatbots.[236]\nAI has simplified the recruiting /job search process for both recruiters and job seekers. According to Raj Mukherjee from Indeed, 65% of job searchers search again within 91 days after hire. An AI-powered engine streamlines the complexity of job hunting by assessing information on job skills, salaries, and user tendencies, matching job seekers to the most relevant positions. Machine intelligence calculates appropriate wages and highlights resume information for recruiters using NLP, which extracts relevant words and phrases from text. Another application is an AI resume builder that compiles a CV in 5 minutes.[237] Chatbots assist website visitors and refine workflows.\nAI underlies avatars (automated online assistants) on web pages.[238] It can reduce operation and training costs.[238] Pypestream automated customer service for its mobile application to streamline communication with customers.[239]\nA Google app analyzes language and converts speech into text. The platform can identify angry customers through their language and respond appropriately.[240] Amazon uses a chatbot for customer service that can perform tasks like checking the status of an order, cancelling orders, offering refunds and connecting the customer with a human representative.[241]\nIn the hospitality industry, AI is used to reduce repetitive tasks, analyze trends, interact with guests, and predict customer needs.[242] AI hotel services come in the form of a chatbot,[243] application, virtual voice assistant and service robots.\nAI applications analyze media content such as movies, TV programs, advertisement videos or user-generated content. The solutions often involve computer vision.\nTypical scenarios include the analysis of images using object recognition or face recognition techniques, or the analysis of video for scene recognizing scenes, objects or faces. AI-based media analysis can facilitate media search, the creation of descriptive keywords for content, content policy monitoring (such as verifying the suitability of content for a particular TV viewing time), speech to text for archival or other purposes, and the detection of logos, products or celebrity faces for ad placement.\nDeep-fakes can be used for comedic purposes but are better known for fake news and hoaxes.\nIn January 2016,[256] the Horizon 2020 program financed the InVID Project[257][258] to help journalists and researchers detect fake documents, made available as browser plugins.[259][260]\nIn June 2016, the visual computing group of the Technical University of Munich and from Stanford University developed Face2Face,[261] a program that animates photographs of faces, mimicking the facial expressions of another person. The technology has been demonstrated animating the faces of people including Barack Obama and Vladimir Putin. Other methods have been demonstrated based on deep neural networks, from which the name deep fake was taken.\nIn September 2018, U.S. Senator Mark Warner proposed to penalize social media companies that allow sharing of deep-fake documents on their platforms.[262]\nIn 2018, Darius Afchar and Vincent Nozick found a way to detect faked content by analyzing the mesoscopic properties of video frames.[263] DARPA gave 68 million dollars to work on deep-fake detection.[263]\nAudio deepfakes[264][265] and AI software capable of detecting deep-fakes and cloning human voices have been developed.[266][267]\nRespeecher is a program that enables one person to speak with the voice of another.\nAI algorithms have been used to detect deepfake videos.[268][269]\nArtificial Intelligence is also starting to be used in video production, with tools and softwares being developed that utilize generative AI in order to create new video, or alter existing video. Some of the major tools that are being used in these processes currently are DALL-E, Mid-journey, and Runway.[270]  Way mark Studios utilized the tools offered by both DALL-E and Mid-journey to create a fully AI generated film called The Frost in the summer of 2023.[270] Way mark Studios is experimenting with using these AI tools to generate advertisements and commercials for companies in mere seconds.[270]  Yves Bergquist, a director of the AI \u0026 Neuroscience in Media Project at USC's Entertainment Technology Center, says post production crews in Hollywood are already using generative AI, and predicts that in the future more companies will embrace this new technology.[271]\nAI has been used to compose music of various genres.\nDavid Cope created an AI called Emily Howell that managed to become well known in the field of algorithmic computer music.[272] The algorithm behind Emily Howell is registered as a US patent.[273]\nIn 2012, AI Iamus created the first complete classical album.[274]\nAIVA (Artificial Intelligence Virtual Artist), composes symphonic music, mainly classical music for film scores.[275] It achieved a world first by becoming the first virtual composer to be recognized by a musical professional association.[276]\nMelomics creates computer-generated music for stress and pain relief.[277]\nAt Sony CSL Research Laboratory, the Flow Machines software creates pop songs by learning music styles from a huge database of songs. It can compose in multiple styles.\nThe Watson Beat uses reinforcement learning and deep belief networks to compose music on a simple seed input melody and a select style. The software was open sourced[278] and musicians such as Taryn Southern[279] collaborated with the project to create music.\nSouth Korean singer Hayeon's debut song, \"Eyes on You\" was composed using AI which was supervised by real composers, including NUVO.[280]\nNarrative Science sells computer-generated news and reports. It summarizes sporting events based on statistical data from the game. It also creates financial reports and real estate analyses.[281] Automated Insights generates personalized recaps and previews for Yahoo Sports Fantasy Football.[282]\nYseop, uses AI to turn structured data into natural language comments and recommendations. Yseop writes financial reports, executive summaries, personalized sales or marketing documents and more in multiple languages, including English, Spanish, French, and German.[283]\nTALESPIN made up stories similar to the fables of Aesop. The program started with a set of characters who wanted to achieve certain goals. The story narrated their attempts to satisfy these goals.[citation needed] Mark Riedl and Vadim Bulitko asserted that the essence of storytelling was experience management, or \"how to balance the need for a coherent story progression with user agency, which is often at odds\".[284]\nWhile AI storytelling focuses on story generation (character and plot), story communication also received attention. In 2002, researchers developed an architectural framework for narrative prose generation. They faithfully reproduced text variety and complexity on stories such as Little Red Riding Hood.[285] In 2016, a Japanese AI co-wrote a short story and almost won a literary prize.[286]\nSouth Korean company Hanteo Global uses a journalism bot to write articles.[287]\nLiterary authors are also exploring uses of AI. An example is David Jhave Johnston's work ReRites (2017-2019), where the poet created a daily rite of editing the poetic output of a neural network to create a series of performances and publications.\nIn 2010, artificial intelligence used baseball statistics to automatically generate news articles. This was launched by The Big Ten Network using a software from Narrative Science.[288]\nAfter being unable to cover every Minor League Baseball game with a large team of people, Associated Press collaborated with Automated Insights in 2016 to create game recaps that were automated by artificial intelligence.[289]\nUOL in Brazil expanded the use of AI in their writing. Rather than just generating news stories, they programmed the AI to include commonly searched words on Google.[289]\nEl Pais, a Spanish news site that covers many things including sports, allows users to make comments on each news article. They use the Perspective API to moderate these comments and if the software deems a comment to contain toxic language, the commenter will be forced to change their comment in order to publish it.[289]\nA local Dutch media group used AI to create automatic coverage of amateur soccer, set to cover 60,000 games in just a single season. NDC partnered with United Robots to create this algorithm and cover what would have never been able to be done before without an extremely large team.[289]\nLede AI has been used in 2023 to take scores from high school football games to generate stories automatically for the local news paper. This was met with a lot of criticism from readers for the very robotic diction that was published. With some descriptions of games being a \"close encounter of the athletic kind,\" readers were not pleased and let the publishing company, Gannett, know on social media. Gannett has since halted their used of Lede AI until they come up with a solution for what they call an experiment.[290]\n Millions of its articles have been edited by bots[294] which however are usually not artificial intelligence software. Many AI platforms use Wikipedia data,[295] mainly for training machine learning applications. There is research and development of various artificial intelligence applications for Wikipedia such as for identifying outdated sentences,[296] detecting covert vandalism[297] or recommending articles and tasks to new editors.\nMachine translation .mw-parser-output div.crossreference{padding-left:0}(see above) has also be used for translating Wikipedia articles and could play a larger role in creating, updating, expanding, and generally improving articles in the future. A content translation tool allows editors of some Wikipedias to more easily translate articles across several select languages.[298][299]\nIn video games, AI is routinely used to generate behavior in non-player characters (NPCs). In addition, AI is used for pathfinding. Some researchers consider NPC AI in games to be a \"solved problem\" for most production tasks.[who?] Games with less typical AI include the AI director of Left 4 Dead (2008) and the neuroevolutionary training of platoons in Supreme Commander 2 (2010).[300][301] AI is also used in Alien Isolation (2014) as a way to control the actions the Alien will perform next.[302]\nKinect, which provides a 3D body–motion interface for the Xbox 360 and the Xbox One, uses algorithms that emerged from AI research.[303][which?]\nAI has been used to produce visual art. The first AI art program, called AARON, was developed by Harold Cohen in 1968[304] with the goal of being able to code the act of drawing. It started by creating simple black and white drawings, and later to paint using special brushes and dyes that were chosen by the program itself without mediation from Cohen.[305]\nAI platforms such as \"DALL-E\",[306] Stable Diffusion,[306] Imagen,[307] and Midjourney[308] have been used for generating visual images from inputs such as text or other images.[309] Some AI tools allow users to input images and output changed versions of that image, such as to display an object or product in different environments. AI image models can also attempt to replicate the specific styles of artists, and can add visual complexity to rough sketches.\nSince their design in 2014, generative adversarial networks (GANs) have been used by AI artists. GAN computer programming, generates technical images through machine learning frameworks that surpass the need for human operators.[304] Examples of GAN programs that generate art include Artbreeder and DeepDream.\nIn addition to the creation of original art, research methods that utilize AI have been generated to quantitatively analyze digital art collections. Although the main goal of the large-scale digitization of artwork in the past few decades was to allow for accessibility and exploration of these collections, the use of AI in analyzing them has brought about new research perspectives.[310]\nTwo computational methods, close reading and distant viewing, are the typical approaches used to analyze digitized art.[311] While distant viewing includes the analysis of large collections, close reading involves one piece of artwork.\nAI has been in use since the early 2000s, most notably by a system designed by Pixar called \"Genesis\".[312] It was designed to learn algorithms and create 3D models for its characters and props. Notable movies that used this technology included Up and The Good Dinosaur.[313] AI has been used less ceremoniously in recent years. In 2023, it was revealed Netflix of Japan was using AI to generate background images for their upcoming show to be met with backlash online.[314]  In recent years, motion capture became an easily accessible form of AI animation. For example, Move AI is a program built to capture any human movement and reanimate it in its animation program using learning AI.[315]\nPower electronics converters are used in renewable energy, energy storage, electric vehicles and high-voltage direct current transmission. These converters are failure-prone, which can interrupt service and require costly maintenance or catastrophic consequences in mission critical applications.[citation needed] AI can guide the design process for reliable power electronics converters, by calculating exact design parameters that ensure the required lifetime.[316]\nMachine learning can be used for energy consumption prediction and scheduling, e.g. to help with renewable energy intermittency management (see also: smart grid and climate change mitigation in the power grid).[317][318][319][320][better source needed]\nMany telecommunications companies make use of heuristic search to manage their workforces. For example, BT Group deployed heuristic search[321] in an application that schedules 20,000 engineers. Machine learning is also used for speech recognition (SR), including of voice-controlled devices, and SR-related transcription, including of videos.[322][323]\nArtificial intelligence has been combined with digital spectrometry by IdeaCuria Inc.,[324][325] enable applications such as at-home water quality monitoring.\nIn the 1990s early AIs controlled Tamagotchis and Giga Pets, the Internet, and the first widely released robot, Furby. Aibo was a domestic robot in the form of a robotic dog with intelligent features and autonomy.\nMattel created an assortment of AI-enabled toys that \"understand\" conversations, give intelligent responses, and learn.[326]\nOil and gas companies have used artificial intelligence tools to automate functions, foresee equipment issues, and increase oil and gas output.[327][328]\nAI in transport is expected to provide safe, efficient, and reliable transportation while minimizing the impact on the environment and communities. The major development challenge is the complexity of transportation systems that involves independent components and parties, with potentially conflicting objectives.[329]\nAI-based fuzzy logic controllers operate gearboxes. For example, the 2006 Audi TT, VW Touareg [citation needed] and VW Caravell feature the DSP transmission. A number of Škoda variants (Škoda Fabia) include a fuzzy logic-based controller. Cars have AI-based driver-assist features such as self-parking and adaptive cruise control.\nThere are also prototypes of autonomous automotive public transport vehicles such as electric mini-buses[330][331][332][333] as well as autonomous rail transport in operation.[334][335][336]\nThere also are prototypes of autonomous delivery vehicles, sometimes including delivery robots.[337][338][339][340][341][342][343]\nTransportation's complexity means that in most cases training an AI in a real-world driving environment is impractical. Simulator-based testing can reduce the risks of on-road training.[344]\nAI underpins self-driving vehicles. Companies involved with AI include Tesla, Waymo, and General Motors. AI-based systems control functions such as braking, lane changing, collision prevention, navigation and mapping.[345]\nAutonomous trucks are in the testing phase. The UK government passed legislation to begin testing of autonomous truck platoons in 2018.[346] A group of autonomous trucks follow closely behind each other. German corporation Daimler is testing its Freightliner Inspiration.[347]\nAutonomous vehicles require accurate maps to be able to navigate between destinations.[348] Some autonomous vehicles do not allow human drivers (they have no steering wheels or pedals).[349]\nAI has been used to optimize traffic management, which reduces wait times, energy use, and emissions by as much as 25 percent.[350]\nSmart traffic lights have been developed at Carnegie Mellon since 2009.  Professor Stephen Smith has started a company since then Surtrac that has installed smart traffic control systems in 22 cities.  It costs about $20,000 per intersection to install. Drive time has been reduced by 25% and traffic jam waiting time has been reduced by 40% at the intersections it has been installed.[351]\nThe Royal Australian Air Force (RAAF) Air Operations Division (AOD) uses AI for expert systems. AIs operate as surrogate operators for combat and training simulators, mission management aids, support systems for tactical decision making, and post processing of the simulator data into symbolic summaries.[352]\nAircraft simulators use AI for training aviators. Flight conditions can be simulated that allow pilots to make mistakes without risking themselves or expensive aircraft. Air combat can also be simulated.\nAI can also be used to operate planes analogously to their control of ground vehicles. Autonomous drones can fly independently or in swarms.[353]\nAOD uses the Interactive Fault Diagnosis and Isolation System, or IFDIS, which is a rule-based expert system using information from TF-30 documents and expert advice from mechanics that work on the TF-30. This system was designed to be used for the development of the TF-30 for the F-111C. The system replaced specialized workers. The system allowed regular workers to communicate with the system and avoid mistakes, miscalculations, or having to speak to one of the specialized workers.\nSpeech recognition allows traffic controllers to give verbal directions to drones.\nArtificial intelligence supported design of aircraft,[354] or AIDA, is used to help designers in the process of creating conceptual designs of aircraft. This program allows the designers to focus more on the design itself and less on the design process. The software also allows the user to focus less on the software tools. The AIDA uses rule-based systems to compute its data. This is a diagram of the arrangement of the AIDA modules. Although simple, the program is proving effective.\nIn 2003 a Dryden Flight Research Center project created software that could enable a damaged aircraft to continue flight until a safe landing can be achieved.[355] The software compensated for damaged components by relying on the remaining undamaged components.[356]\nThe 2016 Intelligent Autopilot System combined apprenticeship learning and behavioral cloning whereby the autopilot observed low-level actions required to maneuver the airplane and high-level strategy used to apply those actions.[357]\nNeural networks are used by situational awareness systems in ships and boats.[358] There also are autonomous boats.\nAutonomous ships that monitor the ocean, AI-driven satellite data analysis, passive acoustics[359] or remote sensing and other applications of environmental monitoring make use of machine learning.[360][361][362][183]\nFor example, \"Global Plastic Watch\" is an AI-based satellite monitoring-platform for analysis/tracking of plastic waste sites to help prevention of plastic pollution – primarily ocean pollution – by helping identify who and where mismanages plastic waste, dumping it into oceans.[363][364]\nMachine learning can be used to spot early-warning signs of disasters and environmental issues, possibly including natural pandemics,[365][366] earthquakes,[367][368][369] landslides,[370] heavy rainfall,[371] long-term water supply vulnerability,[372] tipping-points of ecosystem collapse,[373] cyanobacterial bloom outbreaks,[374] and droughts.[375][376][377]\nAI can be used for real-time code completion, chat, and automated test generation. These tools are typically integrated with editors and IDEs as plugins. They differ in functionality, quality, speed, and approach to privacy.[378]\nCode suggestions could be incorrect, and should be carefully reviewed by software developers before accepted.\nGitHub Copilot is an artificial intelligence model developed by GitHub and OpenAI that is able to autocomplete code in multiple programming languages.[379] Price for individuals: $10/mo or $100/yr, with one free month trial.\nTabnine was created by Jacob Jackson and was originally owned by Tabnine company. In late 2019, Tabnine was acquired by Codota.[380] Tabnine tool is available as plugin to most popular IDEs. It offers multiple pricing options, including limited \"starter\" free version.[381]\nCodiumAI by CodiumAI, small startup in Tel Aviv, offers automated test creation. Currently supports Python, JS, and TS.[382]\nGhostwriter by Replit offers code completion and chat.[383] They have multiple pricing plans, including a free one and a \"Hacker\" plan for $7/month.\nCodeWhisperer by Amazon collects individual users' content, including files open in the IDE. They claim to focus on security both during transmission and when storing.[384] Individual plan is free, professional plan is $19/user/month.\nOther tools: SourceGraph Cody, CodeCompleteFauxPilot, Tabby[378]\nAI can be used to create other AIs. For example, around November 2017, Google's AutoML project to evolve new neural net topologies created NASNet, a system optimized for ImageNet and POCO F1. NASNet's performance exceeded all previously published performance on ImageNet.[385]\nMachine learning has been used for noise-cancelling in quantum technology,[386] including quantum sensors.[387] Moreover, there is substantial research and development of using quantum computers with machine learning algorithms. For example, there is a prototype, photonic, .mw-parser-output .tooltip-dotted{border-bottom:1px dotted;cursor:help}quantum memristive device for neuromorphic (quantum-)computers (NC)/artificial neural networks and NC-using quantum materials with some variety of potential neuromorphic computing-related applications,[388][389] and quantum machine learning is a field with some variety of applications under development. AI could be used for quantum simulators which may have the application of solving physics and chemistry[390][391] problems as well as for quantum annealers for training of neural networks for AI applications.[392] There may also be some usefulness in chemistry, e.g. for drug discovery, and in materials science, e.g. for materials optimization/discovery (with possible relevance to quantum materials manufacturing[218][219]).[393][394][395][better source needed]\nAI researchers have created many tools to solve the most difficult problems in computer science. Many of their inventions have been adopted by mainstream computer science and are no longer considered AI. All of the following were originally developed in AI laboratories:[396]\nAn optical character reader is used in the extraction of data in business documents like invoices and receipts. It can also be used in business contract documents e.g. employment agreements to extract critical data like employment terms, delivery terms, termination clauses, etc.[397]\nArtificial intelligence in architecture describes the use of artificial intelligence in automation, design and planning in the architectural process or in assisting human skills in the field of architecture. Artificial Intelligence is thought to potentially lead to and ensue major changes in Architecture.[398][399][400]  \nAI in architecture has created a way for architects to create things beyond human understanding. AI implementation of machine learning text-to-render technologies, like DALL-E and stable Diffusion, gives power to visualization complex.[401] \nAI allows designers to demonstrate their creativity and even invent new ideas while designing. In future, AI will not replace architects; instead, it will improve the speed of translating ideas sketching.[401]"}
{"url":"https://en.wikipedia.org/wiki/Android_(robot)","text":"An android is a humanoid robot[1] or other artificial being[2][3][4] often made from a flesh-like material.[2] Historically, androids were completely within the domain of science fiction and frequently seen in film and television, but advances in robot technology now allow the design of functional and realistic[5] humanoid robots.[6]\nThe Oxford English Dictionary traces the earliest use (as \"Androides\") to Ephraim Chambers' 1728 Cyclopaedia, in reference to an automaton that St. Albertus Magnus allegedly created.[3][7] By the late 1700s, \"androides\", elaborate mechanical devices resembling humans performing human activities, were displayed in exhibit halls.[8][1]\nThe term \"android\" appears in US patents as early as 1863 in reference to miniature human-like toy automatons.[9] The term android was used in a more modern sense by the French author Auguste Villiers de l'Isle-Adam in his work Tomorrow's Eve (1886), featuring an artificial humanoid robot named Hadaly.[3] The term made an impact into English pulp science fiction starting from Jack Williamson's The Cometeers (1936) and the distinction between mechanical robots and fleshy androids was popularized by Edmond Hamilton's Captain Future stories (1940–1944).[3]\nAlthough Karel Čapek's robots in R.U.R. (Rossum's Universal Robots) (1921)—the play that introduced the word robot to the world—were organic artificial humans, the word \"robot\" has come to primarily refer to mechanical humans, animals, and other beings.[3] The term \"android\" can mean either one of these,[3] while a cyborg (\"cybernetic organism\" or \"bionic man\") would be a creature that is a combination of organic and mechanical parts.\nThe term \"droid\", popularized by George Lucas in the original Star Wars film and now used widely within science fiction, originated as an abridgment of \"android\", but has been used by Lucas and others to mean any robot, including distinctly non-human form machines like R2-D2. The word \"android\" was used in Star Trek: The Original Series episode \"What Are Little Girls Made Of?\" The abbreviation \"andy\", coined as a pejorative by writer Philip K. Dick in his novel Do Androids Dream of Electric Sheep?, has seen some further usage, such as within the TV series Total Recall 2070.[10]\nWhile the term \"android\" is used in reference to human-looking robots in general (not necessarily male-looking humanoid robots), a robot with a female appearance can also be referred to as a gynoid. Besides one can refer to robots without alluding to their sexual appearance by calling them anthrobots (a portmanteau of anthrōpos and robot; see anthrobotics) or anthropoids (short for anthropoid robots; the term humanoids is not appropriate because it is already commonly used to refer to human-like organic species in the context of science fiction, futurism and speculative astrobiology).[11]\nAuthors have used the term android in more diverse ways than robot or cyborg. In some fictional works, the difference between a robot and android is only superficial, with androids being made to look like humans on the outside but with robot-like internal mechanics.[3] In other stories, authors have used the word \"android\" to mean a wholly organic, yet artificial, creation.[3] Other fictional depictions of androids fall somewhere in between.[3]\nEric G. Wilson, who defines an android as a \"synthetic human being\", distinguishes between three types of android, based on their body's composition:\nAlthough human morphology is not necessarily the ideal form for working robots, the fascination in developing robots that can mimic it can be found historically in the assimilation of two concepts: simulacra (devices that exhibit likeness) and automata (devices that have independence).\nSeveral projects aiming to create androids that look, and, to a certain degree, speak or act like a human being have been launched or are underway.\nJapanese robotics have been leading the field since the 1970s.[12] Waseda University initiated the WABOT project in 1967, and in 1972 completed the WABOT-1, the first android, a full-scale humanoid intelligent robot.[13][14] Its limb control system allowed it to walk with the lower limbs, and to grip and transport objects with hands, using tactile sensors. Its vision system allowed it to measure distances and directions to objects using external receptors, artificial eyes and ears. And its conversation system allowed it to communicate with a person in Japanese, with an artificial mouth.[14][15][16]\nIn 1984, WABOT-2 was revealed, and made a number of improvements. It was capable of playing the organ. Wabot-2 had ten fingers and two feet, and was able to read a score of music. It was also able to accompany a person.[17] In 1986, Honda began its humanoid research and development program, to create humanoid robots capable of interacting successfully with humans.[18]\nThe Intelligent Robotics Lab, directed by Hiroshi Ishiguro at Osaka University, and the Kokoro company demonstrated the Actroid at Expo 2005 in Aichi Prefecture, Japan and released the Telenoid R1 in 2010. In 2006, Kokoro developed a new DER 2 android. The height of the human body part of DER2 is 165 cm. There are 47 mobile points. DER2 can not only change its expression but also move its hands and feet and twist its body. The \"air servosystem\" which Kokoro developed originally is used for the actuator. As a result of having an actuator controlled precisely with air pressure via a servosystem, the movement is very fluid and there is very little noise. DER2 realized a slimmer body than that of the former version by using a smaller cylinder. Outwardly DER2 has a more beautiful proportion. Compared to the previous model, DER2 has thinner arms and a wider repertoire of expressions. Once programmed, it is able to choreograph its motions and gestures with its voice.\nThe Intelligent Mechatronics Lab, directed by Hiroshi Kobayashi at the Tokyo University of Science, has developed an android head called Saya, which was exhibited at Robodex 2002 in Yokohama, Japan. There are several other initiatives around the world involving humanoid research and development at this time, which will hopefully introduce a broader spectrum of realized technology in the near future. Now Saya is working at the Science University of Tokyo as a guide.\nThe Waseda University (Japan) and NTT docomo's manufacturers have succeeded in creating a shape-shifting robot WD-2. It is capable of changing its face. At first, the creators decided the positions of the necessary points to express the outline, eyes, nose, and so on of a certain person. The robot expresses its face by moving all points to the decided positions, they say. The first version of the robot was first developed back in 2003. After that, a year later, they made a couple of major improvements to the design. The robot features an elastic mask made from the average head dummy. It uses a driving system with a 3DOF unit. The WD-2 robot can change its facial features by activating specific facial points on a mask, with each point possessing three degrees of freedom. This one has 17 facial points, for a total of 56 degrees of freedom. As for the materials they used, the WD-2's mask is fabricated with a highly elastic material called Septom, with bits of steel wool mixed in for added strength. Other technical features reveal a shaft driven behind the mask at the desired facial point, driven by a DC motor with a simple pulley and a slide screw. Apparently, the researchers can also modify the shape of the mask based on actual human faces. To \"copy\" a face, they need only a 3D scanner to determine the locations of an individual's 17 facial points. After that, they are then driven into position using a laptop and 56 motor control boards. In addition, the researchers also mention that the shifting robot can even display an individual's hair style and skin color if a photo of their face is projected onto the 3D Mask.\nProf Nadia Thalmann, a Nanyang Technological University scientist, directed efforts of the Institute for Media Innovation along with the School of Computer Engineering in the development of a social robot, Nadine. Nadine is powered by software similar to Apple's Siri or Microsoft's Cortana. Nadine may become a personal assistant in offices and homes in future, or she may become a companion for the young and the elderly.\nAssoc Prof Gerald Seet from the School of Mechanical \u0026 Aerospace Engineering and the BeingThere Centre led a three-year R\u0026D development in tele-presence robotics, creating EDGAR. A remote user can control EDGAR with the user's face and expressions displayed on the robot's face in real time. The robot also mimics their upper body movements.\n[19]\nKITECH researched and developed EveR-1, an android interpersonal communications model capable of emulating human emotional expression via facial \"musculature\" and capable of rudimentary conversation, having a vocabulary of around 400 words. She is 160 cm tall and weighs 50 kg, matching the average figure of a Korean woman in her twenties. EveR-1's name derives from the Biblical Eve, plus the letter r for robot. EveR-1's advanced computing processing power enables speech recognition and vocal synthesis, at the same time processing lip synchronization and visual recognition by 90-degree micro-CCD cameras with face recognition technology. An independent microchip inside her artificial brain handles gesture expression, body coordination, and emotion expression. Her whole body is made of highly advanced synthetic jelly silicon and with 60 artificial joints in her face, neck, and lower body; she is able to demonstrate realistic facial expressions and sing while simultaneously dancing. In South Korea, the Ministry of Information and Communication had an ambitious plan to put a robot in every household by 2020.[20] Several robot cities have been planned for the country: the first will be built in 2016 at a cost of 500 billion won (US$440 million), of which 50 billion is direct government investment.[21] The new robot city will feature research and development centers for manufacturers and part suppliers, as well as exhibition halls and a stadium for robot competitions. The country's new Robotics Ethics Charter will establish ground rules and laws for human interaction with robots in the future, setting standards for robotics users and manufacturers, as well as guidelines on ethical standards to be programmed into robots to prevent human abuse of robots and vice versa.[22]\nWalt Disney and a staff of Imagineers created Great Moments with Mr. Lincoln that debuted at the 1964 New York World's Fair.[23]\nDr. William Barry, an Education Futurist and former visiting West Point Professor of Philosophy and Ethical Reasoning at the United States Military Academy, created an AI android character named \"Maria Bot\". This Interface AI android was named after the infamous fictional robot Maria in the 1927 film Metropolis, as a well-behaved distant relative. Maria Bot is the first AI Android Teaching Assistant at the university level.[24][25] Maria Bot has appeared as a keynote speaker as a duo with Barry for a TEDx talk in Everett, Washington in February 2020.[26]\nResembling a human from the shoulders up, Maria Bot is a virtual being android that has complex facial expressions and head movement and engages in conversation about a variety of subjects. She uses AI to process and synthesize information to make her own decisions on how to talk and engage. She collects data through conversations, direct data inputs such as books or articles, and through internet sources.\nMaria Bot was built by an international high-tech company for Barry to help improve education quality and eliminate education poverty. Maria Bot is designed to create new ways for students to engage and discuss ethical issues raised by the increasing presence of robots and artificial intelligence. Barry also uses Maria Bot to demonstrate that programming a robot with life-affirming, ethical framework makes them more likely to help humans to do the same.[27]\nMaria Bot is an ambassador robot for good and ethical AI technology.[28]\nHanson Robotics, Inc., of Texas and KAIST produced an android portrait of Albert Einstein, using Hanson's facial android technology mounted on KAIST's life-size walking bipedal robot body. This Einstein android, also called \"Albert Hubo\", thus represents the first full-body walking android in history.[29] Hanson Robotics, the FedEx Institute of Technology,[30] and the University of Texas at Arlington also developed the android portrait of sci-fi author Philip K. Dick (creator of Do Androids Dream of Electric Sheep?, the basis for the film Blade Runner), with full conversational capabilities that incorporated thousands of pages of the author's works.[31] In 2005, the PKD android won a first-place artificial intelligence award from AAAI.\nAndroids are a staple of science fiction. Isaac Asimov pioneered the fictionalization of the science of robotics and artificial intelligence, notably in his 1950s series I, Robot.[32] One thing common to most fictional androids is that the real-life technological challenges associated with creating thoroughly human-like robots—such as the creation of strong artificial intelligence—are assumed to have been solved.[33] Fictional androids are often depicted as mentally and physically equal or superior to humans—moving, thinking and speaking as fluidly as them.[3][33]\nThe tension between the nonhuman substance and the human appearance—or even human ambitions—of androids is the dramatic impetus behind most of their fictional depictions.[4][33] Some android heroes seek, like Pinocchio, to become human, as in the film Bicentennial Man,[33] or Data in Star Trek: The Next Generation. Others, as in the film Westworld, rebel against abuse by careless humans.[33] Android hunter Deckard in Do Androids Dream of Electric Sheep? and its film adaptation Blade Runner discovers that his targets appear to be, in some ways, more \"human\" than he is.[33] Android stories, therefore, are not essentially stories \"about\" androids; they are stories about the human condition and what it means to be human.[33]\nOne aspect of writing about the meaning of humanity is to use discrimination against androids as a mechanism for exploring racism in society, as in Blade Runner.[34] Perhaps the clearest example of this is John Brunner's 1968 novel Into the Slave Nebula, where the blue-skinned android slaves are explicitly shown to be fully human.[35] More recently, the androids Bishop and Annalee Call in the films Aliens and Alien Resurrection are used as vehicles for exploring how humans deal with the presence of an \"Other\".[36] The 2018 video game Detroit: Become Human also explores how androids are treated as second class citizens in a near future society.\nFemale androids, or \"gynoids\", are often seen in science fiction, and can be viewed as a continuation of the long tradition of men attempting to create the stereotypical \"perfect woman\".[37] Examples include the Greek myth of Pygmalion and the female robot Maria in Fritz Lang's Metropolis. Some gynoids, like Pris in Blade Runner, are designed as sex-objects, with the intent of \"pleasing men's violent sexual desires\",[38] or as submissive, servile companions, such as in The Stepford Wives. Fiction about gynoids has therefore been described as reinforcing \"essentialist ideas of femininity\",[39] although others have suggested that the treatment of androids is a way of exploring racism and misogyny in society.[40]\nThe 2015 Japanese film Sayonara, starring Geminoid F, was promoted as \"the first movie to feature an android performing opposite a human actor\".[41]"}
